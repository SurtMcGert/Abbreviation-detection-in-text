{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# %pip install -q -U ipywidgets transformers tqdm\n",
    "# %pip install -q -U seqeval\n",
    "# %pip install -q -U accelerate\n",
    "# %pip install -q -U transformers[torch]\n",
    "# %pip install -q --upgrade -U torch torchvision torchaudio torchtext\n",
    "# %pip install -q dill==0.3.1.1\n",
    "# %pip install -q numpy==1.14.3\n",
    "# %pip install -q pyarrow==0.3.8\n",
    "# %pip install -q multiprocess==0.70.16\n",
    "# %pip install -q -U datasets==2.6.0\n",
    "# %pip install fsspec==2023.9.2\n",
    "# %pip install spacy\n",
    "# %pip install spacy-en-core-web-sm\n",
    "# %python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from datasets import DatasetDict, Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, pipeline\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "import gc\n",
    "from transformers import Trainer\n",
    "from datasets import DatasetDict, Dataset\n",
    "from transformers.pipelines import PIPELINE_REGISTRY\n",
    "from pipeline import NER_Pipeline\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"torchtext Version: \", torchtext.__version__)\n",
    "print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"surrey-nlp/PLOD-CW\", cache_dir=None, download_mode=\"force_redownload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['B-O', 'B-AC', 'B-LF', 'I-LF']\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "print(f\"train size: {len(train)}\")\n",
    "val = dataset['validation']\n",
    "print(f\"val size: {len(val)}\")\n",
    "test = dataset['test']\n",
    "print(f\"test size: {len(test)}\")\n",
    "\n",
    "def flatten(A):\n",
    "    rt = []\n",
    "    for i in A:\n",
    "        if isinstance(i,list): rt.extend(flatten(i))\n",
    "        else: rt.append(i)\n",
    "    return rt\n",
    "\n",
    "from collections import Counter\n",
    "flat = flatten(train[\"ner_tags\"])\n",
    "print(Counter(flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tags(tag_sequences, possible_tags):\n",
    "    \"\"\"\n",
    "    Decodes a sequence of numerical tags into a list of corresponding textual labels.\n",
    "\n",
    "    Args:\n",
    "        tag_sequence: A list of integers representing numerical tags.\n",
    "        possible_tags: A list of strings representing the possible textual labels.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings representing the decoded textual tags.\n",
    "    \"\"\"\n",
    "\n",
    "    decoded_tags = [[possible_tags[tag] for tag in row] for row in tag_sequences]\n",
    "    return decoded_tags\n",
    "\n",
    "\n",
    "def build_dataset(filtered_set, cw_set, num_of_samples):\n",
    "    \"\"\"\n",
    "    Merges a specified number of rows from a larger list to a smaller list, ensuring no duplicates.\n",
    "\n",
    "    Args:\n",
    "        filtered_set: a split of the filtered dataset\n",
    "        cw_set: a split of the cw dataset\n",
    "        num_of_samples: The number of rows to add from the filtered set.\n",
    "\n",
    "    Returns:\n",
    "        new tokens, pos_tags and ner_tags lists\n",
    "    \"\"\"\n",
    "    # set up the initial lists\n",
    "    tokens = cw_set[\"tokens\"]\n",
    "    pos_tags = cw_set[\"pos_tags\"]\n",
    "    ner_tags = cw_set[\"ner_tags\"]\n",
    "     \n",
    "    # set up the filtered lists\n",
    "    # tokens\n",
    "    filtered_tokens = filtered_set[\"tokens\"]\n",
    "    # pos_tags\n",
    "    filtered_label_list = filtered_set.features[f\"pos_tags\"].feature.names\n",
    "    filtered_pos_tags = decode_tags(filtered_set[\"pos_tags\"], filtered_label_list)\n",
    "    # ner_tags\n",
    "    filtered_label_list = filtered_set.features[f\"ner_tags\"].feature.names\n",
    "    filtered_ner_tags = decode_tags(filtered_set[\"ner_tags\"], filtered_label_list)\n",
    "\n",
    "    # convert the tokens list to sets for efficient duplicate checking\n",
    "    tokens_set = set(tuple(row) for row in tokens)\n",
    "    filtered_tokens_set = set(tuple(row) for row in filtered_tokens)\n",
    "\n",
    "    # find rows to add\n",
    "    rows_to_add = []\n",
    "    for index, row in enumerate(filtered_tokens_set):\n",
    "        if tuple(row) not in tokens_set and len(rows_to_add) < num_of_samples:\n",
    "            rows_to_add.append(index)\n",
    "\n",
    "    # Merge and return the lists\n",
    "    tokens = tokens + [filtered_tokens[i] for i in rows_to_add]\n",
    "    pos_tags = pos_tags + [filtered_pos_tags[i] for i in rows_to_add]\n",
    "    ner_tags = ner_tags + [filtered_ner_tags[i] for i in rows_to_add]\n",
    "\n",
    "    return tokens, pos_tags, ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "def encode_tags(tag_sequences, possible_tags):\n",
    "    \"\"\"\n",
    "    Encodes a sequence of string tags into a list of corresponding integer tags.\n",
    "\n",
    "    Args:\n",
    "        tag_sequences: A 2d list of strings representing numerical tags.\n",
    "        possible_tags: A list of strings representing the possible textual labels.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings representing the decoded textual tags.\n",
    "    \"\"\"\n",
    "\n",
    "    encoded_tags = [[possible_tags.index(tag) for tag in row] for row in tag_sequences]\n",
    "    return encoded_tags\n",
    "def tokenize_and_align_labels(data, tokenizer, task):\n",
    "    tokenized_inputs = tokenizer(data[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    converted_tags = encode_tags(data[f\"{task}_tags\"], label_list)\n",
    "    for i, label in enumerate(converted_tags):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_lists_elementwise(list_A, list_B):\n",
    "  \"\"\"\n",
    "  Combines two 2D lists of strings element-wise into a 2D list of tuples.\n",
    "\n",
    "  Args:\n",
    "      list_A: A 2D list of strings (e.g., [['A', 'A', 'A'], ['A', 'A', 'A']]).\n",
    "      list_B: Another 2D list of strings with the same dimensions as list_A.\n",
    "\n",
    "  Returns:\n",
    "      A 2D list of tuples, where each tuple combines corresponding elements from list_A and list_B.\n",
    "\n",
    "  Raises:\n",
    "      ValueError: If the dimensions of list_A and list_B don't match.\n",
    "  \"\"\"\n",
    "\n",
    "  # Check if dimensions match\n",
    "  if len(list_A) != len(list_B) or len(list_A[0]) != len(list_B[0]):\n",
    "    raise ValueError(\"Dimensions of lists A and B must be equal.\")\n",
    "\n",
    "  # Create the resulting list using list comprehension\n",
    "  return [[(a, b) for a, b in zip(row_a, row_b)] for row_a, row_b in zip(list_A, list_B)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pos_tag(nltk_tag):\n",
    "    \"\"\"\n",
    "    Converts NLTK POS tags to the format expected by the lemmatizer.\n",
    "\n",
    "    Args:\n",
    "        nltk_tag: The POS tag in NLTK format (e.g., VBG, NNS).\n",
    "\n",
    "    Returns:\n",
    "        The corresponding POS tag for the lemmatizer (n, v, a, r, or s) or None if no match.\n",
    "    \"\"\"\n",
    "\n",
    "    tag_map = {\n",
    "        'NUM': '',  # Number (not handled by lemmatizer)\n",
    "        'CCONJ': '',  # Coordinating conjunction (not handled)\n",
    "        'PRON': '',  # Pronoun (not handled)\n",
    "        'NOUN': 'n',   # Noun\n",
    "        'SCONJ': '',  # Subordinating conjunction (not handled)\n",
    "        'SYM': '',   # Symbol (not handled)\n",
    "        'INTJ': '',  # Interjection (not handled)\n",
    "        'ADJ': 'a',    # Adjective\n",
    "        'ADP': '',   # Preposition (not handled)\n",
    "        'PUNCT': '',  # Punctuation (not handled)\n",
    "        'ADV': 'r',    # Adverb\n",
    "        'AUX': 'v',    # Auxiliary verb\n",
    "        'DET': '',   # Determiner (not handled)\n",
    "        'VERB': 'v',   # Verb\n",
    "        'X': '',      # Other (not handled)\n",
    "        'PART': '',   # Particle (not handled)\n",
    "        'PROPN': 'n',   # Proper noun\n",
    "    }\n",
    "    return tag_map.get(nltk_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_list(data, pos_tags):\n",
    "    \"\"\"\n",
    "    Lemmatizes a 2D list of tokens using NLTK.\n",
    "\n",
    "    Args:\n",
    "        data: A 2D list of strings (tokens) to be lemmatized.\n",
    "\n",
    "    Returns:\n",
    "        A 2D list containing the lemmatized tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the WordNet lemmatizer\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    pos_tags = [[convert_pos_tag(tag) for tag in row] for row in pos_tags]\n",
    "\n",
    "\n",
    "    data = combine_lists_elementwise(data, pos_tags)\n",
    "\n",
    "\n",
    "    # Lemmatize with part-of-speech information\n",
    "    lemmatized_data = [[token if pos == '' else lemmatizer.lemmatize(token, pos) for token, pos in row] for row in data]\n",
    "\n",
    "    return lemmatized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(tokens, pos_tags):\n",
    "    # lemmatize the data\n",
    "    data = lemmatize_list(tokens, pos_tags)\n",
    "    # lowercase the data\n",
    "    data = [[string.lower() for string in row] for row in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = pre_process_data(train[\"tokens\"], train[\"pos_tags\"])\n",
    "val_tokens = pre_process_data(val[\"tokens\"], val[\"pos_tags\"])\n",
    "test_tokens = pre_process_data(test[\"tokens\"], test[\"pos_tags\"])\n",
    "original_train_tokens = train[\"tokens\"]\n",
    "original_val_tokens = val[\"tokens\"]\n",
    "print(f\"original train tokens: {original_train_tokens[0]}\\npre-processed train tokens: {train_tokens[0]}\")\n",
    "print(f\"original val tokens: {original_val_tokens[0]}\\npre-processed val tokens: {val_tokens[0]}\")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"tokens\": train_tokens, \"pos_tags\": train[\"pos_tags\"], \"ner_tags\": train[\"ner_tags\"]}),\n",
    "    \"validation\": Dataset.from_dict({\"tokens\": val_tokens, \"pos_tags\": val[\"pos_tags\"], \"ner_tags\": val[\"ner_tags\"]}),\n",
    "    \"test\": Dataset.from_dict({\"tokens\": test_tokens, \"pos_tags\": test[\"pos_tags\"], \"ner_tags\": test[\"ner_tags\"]}),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Extra Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = load_dataset(\"surrey-nlp/PLOD-filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train = filtered_dataset[\"train\"]\n",
    "print(f\"train size: {len(filtered_train)}\")\n",
    "filtered_val = filtered_dataset[\"validation\"]\n",
    "print(f\"val size: {len(filtered_val)}\")\n",
    "filtered_test = filtered_dataset[\"test\"]\n",
    "print(f\"test size: {len(filtered_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium = 10000\n",
    "tokens_medium, pos_tags_medium, ner_tags_medium = build_dataset(filtered_train, train, medium)\n",
    "tokens_medium = pre_process_data(tokens_medium, pos_tags_medium)\n",
    "print(f\"num of medium samples: {len(tokens_medium)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_datasets_dict = {\n",
    "    \"train\": Dataset.from_dict({\"tokens\": tokens_medium, \"pos_tags\": pos_tags_medium, \"ner_tags\": ner_tags_medium})\n",
    "}\n",
    "medium_dataset = DatasetDict(medium_datasets_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True) # use AutoTokenizer because it defaults to fast tokenizers where as using the BERT Tokenizer does not\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    overall_results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    true_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "    \n",
    "    \n",
    "    results = classification_report(true_labels, true_predictions, labels = label_list)\n",
    "    print(results)\n",
    "    return {\n",
    "        \"precision\": overall_results[\"overall_precision\"],\n",
    "        \"recall\": overall_results[\"overall_recall\"],\n",
    "        \"f1\": overall_results[\"overall_f1\"],\n",
    "        \"accuracy\": overall_results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_func(data):\n",
    "    return tokenize_and_align_labels(data, tokenizer, task)\n",
    "tokenized_datasets = dataset.map(intermediate_func, batched=True)\n",
    "medium_tokenized_dataset = medium_dataset.map(intermediate_func, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_checkpoint}-finetuned-GROUP-{task}\",\n",
    "    evaluation_strategy ='steps',\n",
    "    eval_steps = 7000,\n",
    "    logging_steps = 500,\n",
    "    save_total_limit = 1,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.001,\n",
    "    save_steps=0,\n",
    "    metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=medium_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear CUDA memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear CUDA memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"roberta_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(\"SurtMcGert/NLP-group-CW-roberta-ner-tagging\")\n",
    "trainer.model.push_to_hub(\"SurtMcGert/NLP-group-CW-roberta-ner-tagging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register custom pipeline\n",
    "PIPELINE_REGISTRY.register_pipeline(\n",
    "    \"NER_NLP_tagger\",\n",
    "    pipeline_class = NER_Pipeline,\n",
    "    pt_model = AutoModelForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tagger = pipeline(\"NER_NLP_tagger\", model = \"SurtMcGert/NLP-group-CW-roberta-ner-tagging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tagger.requires_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ner_tagger(\"this is a test on our Natural Language Processing (NLP) tagging Artificial Intelligence (AI).\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%streamlit run server.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
