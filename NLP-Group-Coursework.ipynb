{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# %pip install -q -U ipywidgets transformers tqdm\n",
    "# %pip install -q -U seqeval\n",
    "# %pip install -q -U accelerate\n",
    "# %pip install -q -U transformers[torch]\n",
    "# %pip install -q --upgrade -U torch torchvision torchaudio torchtext\n",
    "# %pip install -q dill==0.3.1.1\n",
    "# %pip install -q numpy==1.14.3\n",
    "# %pip install -q pyarrow==0.3.8\n",
    "# %pip install -q multiprocess==0.70.16\n",
    "# %pip install -q -U datasets==2.6.0\n",
    "# %pip install fsspec==2023.9.2\n",
    "# %pip install spacy\n",
    "# %pip install spacy-en-core-web-sm\n",
    "# %python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "import torchtext\n",
    "from datasets import load_dataset, Features, Value\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from datasets import DatasetDict, Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, pipeline, Pipeline\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "import dill\n",
    "import gc\n",
    "from transformers import Trainer\n",
    "import torch.nn as nn\n",
    "from datasets import DatasetDict, Dataset\n",
    "from sklearn.pipeline import Pipeline\n",
    "from transformers.pipelines import PIPELINE_REGISTRY\n",
    "from pipeline import NER_Pipeline\n",
    "import spacy\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.2.0+cu121\n",
      "torchtext Version:  0.17.0+cpu\n",
      "Using GPU.\n"
     ]
    }
   ],
   "source": [
    "SEED = 1234\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"torchtext Version: \", torchtext.__version__)\n",
    "print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12a4ceed09b4537a30f4af084851d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 188k/188k [00:00<00:00, 543kB/s]\n",
      "Downloading data: 100%|██████████| 28.4k/28.4k [00:00<00:00, 108kB/s]\n",
      "Downloading data: 100%|██████████| 28.7k/28.7k [00:00<00:00, 117kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177727f0c741494dacd3d7bd3c2e36c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1072 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f577e5d5e14a0bacecd064f04a9e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519f9135c5e546f2bbeeaaa04d462139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/153 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"surrey-nlp/PLOD-CW\", cache_dir=None, download_mode=\"force_redownload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-O', 'B-AC', 'B-LF', 'I-LF']\n"
     ]
    }
   ],
   "source": [
    "label_list = ['B-O', 'B-AC', 'B-LF', 'I-LF']\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 1072\n",
      "val size: 126\n",
      "test size: 153\n",
      "Counter({'B-O': 32971, 'I-LF': 3231, 'B-AC': 2336, 'B-LF': 1462})\n"
     ]
    }
   ],
   "source": [
    "train = dataset['train']\n",
    "print(f\"train size: {len(train)}\")\n",
    "val = dataset['validation']\n",
    "print(f\"val size: {len(val)}\")\n",
    "test = dataset['test']\n",
    "print(f\"test size: {len(test)}\")\n",
    "\n",
    "def flatten(A):\n",
    "    rt = []\n",
    "    for i in A:\n",
    "        if isinstance(i,list): rt.extend(flatten(i))\n",
    "        else: rt.append(i)\n",
    "    return rt\n",
    "\n",
    "from collections import Counter\n",
    "flat = flatten(train[\"ner_tags\"])\n",
    "print(Counter(flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tags(tag_sequences, possible_tags):\n",
    "    \"\"\"\n",
    "    Decodes a sequence of numerical tags into a list of corresponding textual labels.\n",
    "\n",
    "    Args:\n",
    "        tag_sequence: A list of integers representing numerical tags.\n",
    "        possible_tags: A list of strings representing the possible textual labels.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings representing the decoded textual tags.\n",
    "    \"\"\"\n",
    "\n",
    "    decoded_tags = [[possible_tags[tag] for tag in row] for row in tag_sequences]\n",
    "    return decoded_tags\n",
    "\n",
    "\n",
    "def build_dataset(filtered_set, cw_set, num_of_samples):\n",
    "    \"\"\"\n",
    "    Merges a specified number of rows from a larger list to a smaller list, ensuring no duplicates.\n",
    "\n",
    "    Args:\n",
    "        filtered_set: a split of the filtered dataset\n",
    "        cw_set: a split of the cw dataset\n",
    "        num_of_samples: The number of rows to add from the filtered set.\n",
    "\n",
    "    Returns:\n",
    "        new tokens, pos_tags and ner_tags lists\n",
    "    \"\"\"\n",
    "    # set up the initial lists\n",
    "    tokens = cw_set[\"tokens\"]\n",
    "    pos_tags = cw_set[\"pos_tags\"]\n",
    "    ner_tags = cw_set[\"ner_tags\"]\n",
    "     \n",
    "    # set up the filtered lists\n",
    "    # tokens\n",
    "    filtered_tokens = filtered_set[\"tokens\"]\n",
    "    # pos_tags\n",
    "    filtered_label_list = filtered_set.features[f\"pos_tags\"].feature.names\n",
    "    filtered_pos_tags = decode_tags(filtered_set[\"pos_tags\"], filtered_label_list)\n",
    "    # ner_tags\n",
    "    filtered_label_list = filtered_set.features[f\"ner_tags\"].feature.names\n",
    "    filtered_ner_tags = decode_tags(filtered_set[\"ner_tags\"], filtered_label_list)\n",
    "\n",
    "    # convert the tokens list to sets for efficient duplicate checking\n",
    "    tokens_set = set(tuple(row) for row in tokens)\n",
    "    filtered_tokens_set = set(tuple(row) for row in filtered_tokens)\n",
    "\n",
    "    # find rows to add\n",
    "    rows_to_add = []\n",
    "    for index, row in enumerate(filtered_tokens_set):\n",
    "        if tuple(row) not in tokens_set and len(rows_to_add) < num_of_samples:\n",
    "            rows_to_add.append(index)\n",
    "\n",
    "    # Merge and return the lists\n",
    "    tokens = tokens + [filtered_tokens[i] for i in rows_to_add]\n",
    "    pos_tags = pos_tags + [filtered_pos_tags[i] for i in rows_to_add]\n",
    "    ner_tags = ner_tags + [filtered_ner_tags[i] for i in rows_to_add]\n",
    "\n",
    "    return tokens, pos_tags, ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "def encode_tags(tag_sequences, possible_tags):\n",
    "    \"\"\"\n",
    "    Encodes a sequence of string tags into a list of corresponding integer tags.\n",
    "\n",
    "    Args:\n",
    "        tag_sequences: A 2d list of strings representing numerical tags.\n",
    "        possible_tags: A list of strings representing the possible textual labels.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings representing the decoded textual tags.\n",
    "    \"\"\"\n",
    "\n",
    "    encoded_tags = [[possible_tags.index(tag) for tag in row] for row in tag_sequences]\n",
    "    return encoded_tags\n",
    "def tokenize_and_align_labels(data, tokenizer, task):\n",
    "    tokenized_inputs = tokenizer(data[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    converted_tags = encode_tags(data[f\"{task}_tags\"], label_list)\n",
    "    for i, label in enumerate(converted_tags):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_lists_elementwise(list_A, list_B):\n",
    "  \"\"\"\n",
    "  Combines two 2D lists of strings element-wise into a 2D list of tuples.\n",
    "\n",
    "  Args:\n",
    "      list_A: A 2D list of strings (e.g., [['A', 'A', 'A'], ['A', 'A', 'A']]).\n",
    "      list_B: Another 2D list of strings with the same dimensions as list_A.\n",
    "\n",
    "  Returns:\n",
    "      A 2D list of tuples, where each tuple combines corresponding elements from list_A and list_B.\n",
    "\n",
    "  Raises:\n",
    "      ValueError: If the dimensions of list_A and list_B don't match.\n",
    "  \"\"\"\n",
    "\n",
    "  # Check if dimensions match\n",
    "  if len(list_A) != len(list_B) or len(list_A[0]) != len(list_B[0]):\n",
    "    raise ValueError(\"Dimensions of lists A and B must be equal.\")\n",
    "\n",
    "  # Create the resulting list using list comprehension\n",
    "  return [[(a, b) for a, b in zip(row_a, row_b)] for row_a, row_b in zip(list_A, list_B)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pos_tag(nltk_tag):\n",
    "    \"\"\"\n",
    "    Converts NLTK POS tags to the format expected by the lemmatizer.\n",
    "\n",
    "    Args:\n",
    "        nltk_tag: The POS tag in NLTK format (e.g., VBG, NNS).\n",
    "\n",
    "    Returns:\n",
    "        The corresponding POS tag for the lemmatizer (n, v, a, r, or s) or None if no match.\n",
    "    \"\"\"\n",
    "\n",
    "    tag_map = {\n",
    "        'NUM': '',  # Number (not handled by lemmatizer)\n",
    "        'CCONJ': '',  # Coordinating conjunction (not handled)\n",
    "        'PRON': '',  # Pronoun (not handled)\n",
    "        'NOUN': 'n',   # Noun\n",
    "        'SCONJ': '',  # Subordinating conjunction (not handled)\n",
    "        'SYM': '',   # Symbol (not handled)\n",
    "        'INTJ': '',  # Interjection (not handled)\n",
    "        'ADJ': 'a',    # Adjective\n",
    "        'ADP': '',   # Preposition (not handled)\n",
    "        'PUNCT': '',  # Punctuation (not handled)\n",
    "        'ADV': 'r',    # Adverb\n",
    "        'AUX': 'v',    # Auxiliary verb\n",
    "        'DET': '',   # Determiner (not handled)\n",
    "        'VERB': 'v',   # Verb\n",
    "        'X': '',      # Other (not handled)\n",
    "        'PART': '',   # Particle (not handled)\n",
    "        'PROPN': 'n',   # Proper noun\n",
    "    }\n",
    "    return tag_map.get(nltk_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_list(data, pos_tags):\n",
    "    \"\"\"\n",
    "    Lemmatizes a 2D list of tokens using NLTK.\n",
    "\n",
    "    Args:\n",
    "        data: A 2D list of strings (tokens) to be lemmatized.\n",
    "\n",
    "    Returns:\n",
    "        A 2D list containing the lemmatized tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the WordNet lemmatizer\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    pos_tags = [[convert_pos_tag(tag) for tag in row] for row in pos_tags]\n",
    "\n",
    "\n",
    "    data = combine_lists_elementwise(data, pos_tags)\n",
    "\n",
    "\n",
    "    # Lemmatize with part-of-speech information\n",
    "    lemmatized_data = [[token if pos == '' else lemmatizer.lemmatize(token, pos) for token, pos in row] for row in data]\n",
    "\n",
    "    return lemmatized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(tokens, pos_tags):\n",
    "    # lemmatize the data\n",
    "    data = lemmatize_list(tokens, pos_tags)\n",
    "    # lowercase the data\n",
    "    data = [[string.lower() for string in row] for row in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train tokens: ['For', 'this', 'purpose', 'the', 'Gothenburg', 'Young', 'Persons', 'Empowerment', 'Scale', '(', 'GYPES', ')', 'was', 'developed', '.']\n",
      "pre-processed train tokens: ['for', 'this', 'purpose', 'the', 'gothenburg', 'young', 'persons', 'empowerment', 'scale', '(', 'gypes', ')', 'be', 'develop', '.']\n",
      "original val tokens: ['=', 'Manual', 'Ability', 'Classification', 'System', ';', 'QUEST', '=', 'Quest', '-', 'Quality', 'of', 'upper', 'extremity', 'skills', 'test', ';', 'Cont', '=', 'control', ';', 'M', '=', 'male', ',', 'F', '=', 'female', ',', 'V', '=', 'verbal', ',', 'nonV', '=', 'non', '-', 'Verbal', ',', '|Quad', '=', 'quadriplegia', ',', 'Di', '=', 'Diplegia', ',', 'Hemi', '=', 'hemiplegia', '.']\n",
      "pre-processed val tokens: ['=', 'manual', 'ability', 'classification', 'system', ';', 'quest', '=', 'quest', '-', 'quality', 'of', 'upper', 'extremity', 'skill', 'test', ';', 'cont', '=', 'control', ';', 'm', '=', 'male', ',', 'f', '=', 'female', ',', 'v', '=', 'verbal', ',', 'nonv', '=', 'non', '-', 'verbal', ',', '|quad', '=', 'quadriplegia', ',', 'di', '=', 'diplegia', ',', 'hemi', '=', 'hemiplegia', '.']\n"
     ]
    }
   ],
   "source": [
    "train_tokens = pre_process_data(train[\"tokens\"], train[\"pos_tags\"])\n",
    "val_tokens = pre_process_data(val[\"tokens\"], val[\"pos_tags\"])\n",
    "test_tokens = pre_process_data(test[\"tokens\"], test[\"pos_tags\"])\n",
    "original_train_tokens = train[\"tokens\"]\n",
    "original_val_tokens = val[\"tokens\"]\n",
    "print(f\"original train tokens: {original_train_tokens[0]}\\npre-processed train tokens: {train_tokens[0]}\")\n",
    "print(f\"original val tokens: {original_val_tokens[0]}\\npre-processed val tokens: {val_tokens[0]}\")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"tokens\": train_tokens, \"pos_tags\": train[\"pos_tags\"], \"ner_tags\": train[\"ner_tags\"]}),\n",
    "    \"validation\": Dataset.from_dict({\"tokens\": val_tokens, \"pos_tags\": val[\"pos_tags\"], \"ner_tags\": val[\"ner_tags\"]}),\n",
    "    \"test\": Dataset.from_dict({\"tokens\": test_tokens, \"pos_tags\": test[\"pos_tags\"], \"ner_tags\": test[\"ner_tags\"]}),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Extra Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = load_dataset(\"surrey-nlp/PLOD-filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 112652\n",
      "val size: 24140\n",
      "test size: 24140\n"
     ]
    }
   ],
   "source": [
    "filtered_train = filtered_dataset[\"train\"]\n",
    "print(f\"train size: {len(filtered_train)}\")\n",
    "filtered_val = filtered_dataset[\"validation\"]\n",
    "print(f\"val size: {len(filtered_val)}\")\n",
    "filtered_test = filtered_dataset[\"test\"]\n",
    "print(f\"test size: {len(filtered_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of medium samples: 11072\n"
     ]
    }
   ],
   "source": [
    "medium = 10000\n",
    "tokens_medium, pos_tags_medium, ner_tags_medium = build_dataset(filtered_train, train, medium)\n",
    "tokens_medium = pre_process_data(tokens_medium, pos_tags_medium)\n",
    "print(f\"num of medium samples: {len(tokens_medium)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_datasets_dict = {\n",
    "    \"train\": Dataset.from_dict({\"tokens\": tokens_medium, \"pos_tags\": pos_tags_medium, \"ner_tags\": ner_tags_medium})\n",
    "}\n",
    "medium_dataset = DatasetDict(medium_datasets_dict)\n",
    "# medium_tokenized_dataset = medium_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model_saves/BERT_ex4_six_save\\\\tokenizer_config.json',\n",
       " 'model_saves/BERT_ex4_six_save\\\\special_tokens_map.json',\n",
       " 'model_saves/BERT_ex4_six_save\\\\vocab.txt',\n",
       " 'model_saves/BERT_ex4_six_save\\\\added_tokens.json',\n",
       " 'model_saves/BERT_ex4_six_save\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_checkpoint = \"bert-base-uncased\"\n",
    "# #model_checkpoint = \"xlnet-base-cased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True) # use AutoTokenizer because it defaults to fast tokenizers where as using the BERT Tokenizer does not\n",
    "# assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "# tokenizer.save_pretrained(\"model_saves/BERT_ex4_six_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"model_saves/BERT_ex4_six_save\"\n",
    "# model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775f06ec389b4cb8aff5bcf7400560e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\harry\\.cache\\huggingface\\hub\\models--xlnet-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bdc948cb5404825820575d34082aff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd59e202195c488482757aa3913df31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826ae207113b4bc9bd430c07676dc84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForTokenClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"xlnet-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True) # use AutoTokenizer because it defaults to fast tokenizers where as using the BERT Tokenizer does not\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harry\\AppData\\Local\\Temp\\ipykernel_989304\\4079666221.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n",
      "C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    overall_results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    true_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "    \n",
    "    \n",
    "    results = classification_report(true_labels, true_predictions, labels = label_list)\n",
    "    print(results)\n",
    "    return {\n",
    "        \"precision\": overall_results[\"overall_precision\"],\n",
    "        \"recall\": overall_results[\"overall_recall\"],\n",
    "        \"f1\": overall_results[\"overall_f1\"],\n",
    "        \"accuracy\": overall_results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4e439ace6348288fdfd58d399f21e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1072 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d104148056384e10b1b6132a3c44d0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a50fe30f26a4f9590d9c14ce9bff751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/153 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1455b0c7e76648d69e611660376dcda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11072 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def intermediate_func(data):\n",
    "    return tokenize_and_align_labels(data, tokenizer, task)\n",
    "tokenized_datasets = dataset.map(intermediate_func, batched=True)\n",
    "medium_tokenized_dataset = medium_dataset.map(intermediate_func, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_checkpoint}-finetuned-GROUP-{task}\",\n",
    "    evaluation_strategy ='steps',\n",
    "    eval_steps = 50,\n",
    "    logging_steps = 50,\n",
    "    save_total_limit = 1,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.001,\n",
    "    save_steps=0,\n",
    "    metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=medium_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear CUDA memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear CUDA memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"roberta_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(\"SurtMcGert/NLP-group-CW-roberta-ner-tagging\")\n",
    "trainer.model.push_to_hub(\"SurtMcGert/NLP-group-CW-roberta-ner-tagging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register custom pipeline\n",
    "PIPELINE_REGISTRY.register_pipeline(\n",
    "    \"NER_NLP_tagger\",\n",
    "    pipeline_class = NER_Pipeline,\n",
    "    pt_model = AutoModelForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tagger = pipeline(\"NER_NLP_tagger\", model = \"SurtMcGert/NLP-group-CW-xlnet-ner-tagging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 'B-O'), ('is', 'B-O'), ('a', 'B-O'), ('test', 'B-O'), ('on', 'B-O'), ('our', 'B-O'), ('NLP', 'I-LF'), ('tagging', 'I-LF'), ('AI', 'B-AC'), ('.', '')]\n"
     ]
    }
   ],
   "source": [
    "output = ner_tagger(\"this is a test on our NLP tagging AI.\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python server.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
