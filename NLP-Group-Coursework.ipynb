{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm@ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl#sha256=86cc141f63942d4b2c5fcee06630fd6f904788d2f0ab005cce45aadb8fb73889 (from -r requirements.txt (line 26))\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.2/12.8 MB 6.9 MB/s eta 0:00:02\n",
      "     - -------------------------------------- 0.5/12.8 MB 6.6 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.8/12.8 MB 6.3 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 1.1/12.8 MB 6.2 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 6.2 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.6/12.8 MB 6.2 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 1.9/12.8 MB 6.1 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.2/12.8 MB 6.1 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.5/12.8 MB 6.1 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.7/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.0/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.8/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.1/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.4/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.6/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 4.9/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 6.0/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.3/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.5/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.4/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.9/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.2/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.4/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.0/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.3/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.8/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.1/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.3/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.6/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.9/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.1/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.4/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.7/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 5.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: aiohttp==3.9.5 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 1)) (3.9.5)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: altair==5.3.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 3)) (5.3.0)\n",
      "Requirement already satisfied: annotated-types==0.6.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 4)) (0.6.0)\n",
      "Requirement already satisfied: asttokens==2.4.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 5)) (2.4.1)\n",
      "Requirement already satisfied: async-timeout==4.0.3 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 6)) (4.0.3)\n",
      "Requirement already satisfied: attrs==23.2.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 7)) (23.2.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.3 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 8)) (4.12.3)\n",
      "Requirement already satisfied: blinker==1.8.2 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 9)) (1.8.2)\n",
      "Requirement already satisfied: blis==0.7.11 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 10)) (0.7.11)\n",
      "Requirement already satisfied: cachetools==5.3.3 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 11)) (5.3.3)\n",
      "Requirement already satisfied: catalogue==2.0.10 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 12)) (2.0.10)\n",
      "Requirement already satisfied: certifi==2024.2.2 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 13)) (2024.2.2)\n",
      "Requirement already satisfied: cffi==1.16.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 14)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer==3.3.2 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 15)) (3.3.2)\n",
      "Requirement already satisfied: click==8.1.7 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 16)) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib==0.16.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 17)) (0.16.0)\n",
      "Requirement already satisfied: colorama==0.4.6 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 18)) (0.4.6)\n",
      "Requirement already satisfied: comm==0.2.2 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 19)) (0.2.2)\n",
      "Requirement already satisfied: confection==0.1.4 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 20)) (0.1.4)\n",
      "Requirement already satisfied: cymem==2.0.8 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 21)) (2.0.8)\n",
      "Requirement already satisfied: datasets==2.19.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 22)) (2.19.1)\n",
      "Requirement already satisfied: debugpy==1.8.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 23)) (1.8.1)\n",
      "Requirement already satisfied: decorator==5.1.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 24)) (5.1.1)\n",
      "Requirement already satisfied: dill==0.3.8 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 25)) (0.3.8)\n",
      "Requirement already satisfied: exceptiongroup==1.2.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 27)) (1.2.1)\n",
      "Requirement already satisfied: executing==2.0.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 28)) (2.0.1)\n",
      "Requirement already satisfied: Faker==25.2.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 29)) (25.2.0)\n",
      "Requirement already satisfied: filelock==3.14.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 30)) (3.14.0)\n",
      "Requirement already satisfied: Flask==3.0.3 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 31)) (3.0.3)\n",
      "Requirement already satisfied: frozenlist==1.4.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 32)) (1.4.1)\n",
      "Requirement already satisfied: fsspec==2024.3.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 33)) (2024.3.1)\n",
      "Requirement already satisfied: gitdb==4.0.11 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 34)) (4.0.11)\n",
      "Requirement already satisfied: GitPython==3.1.43 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 35)) (3.1.43)\n",
      "Requirement already satisfied: h11==0.14.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 36)) (0.14.0)\n",
      "Requirement already satisfied: htbuilder==0.6.2 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 37)) (0.6.2)\n",
      "Requirement already satisfied: huggingface-hub==0.23.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 38)) (0.23.0)\n",
      "Requirement already satisfied: idna==3.7 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 39)) (3.7)\n",
      "Requirement already satisfied: iniconfig==2.0.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 40)) (2.0.0)\n",
      "Requirement already satisfied: intel-openmp==2021.4.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 41)) (2021.4.0)\n",
      "Requirement already satisfied: ipykernel==6.29.4 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 42)) (6.29.4)\n",
      "Requirement already satisfied: ipython==8.24.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 43)) (8.24.0)\n",
      "Requirement already satisfied: itsdangerous==2.2.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 44)) (2.2.0)\n",
      "Requirement already satisfied: jedi==0.19.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 45)) (0.19.1)\n",
      "Requirement already satisfied: Jinja2==3.1.4 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 46)) (3.1.4)\n",
      "Requirement already satisfied: joblib==1.4.2 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 47)) (1.4.2)\n",
      "Requirement already satisfied: jsonschema==4.22.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 48)) (4.22.0)\n",
      "Requirement already satisfied: jsonschema-specifications==2023.12.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 49)) (2023.12.1)\n",
      "Requirement already satisfied: jupyter_client==8.6.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 50)) (8.6.1)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 51)) (5.7.2)\n",
      "Requirement already satisfied: langcodes==3.4.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 52)) (3.4.0)\n",
      "Requirement already satisfied: language_data==1.2.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 53)) (1.2.0)\n",
      "Requirement already satisfied: marisa-trie==1.1.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 54)) (1.1.1)\n",
      "Requirement already satisfied: markdown-it-py==3.0.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 55)) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe==2.1.5 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 56)) (2.1.5)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 57)) (0.1.7)\n",
      "Requirement already satisfied: mdurl==0.1.2 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 58)) (0.1.2)\n",
      "Requirement already satisfied: mkl==2021.4.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 59)) (2021.4.0)\n",
      "Requirement already satisfied: more-itertools==10.2.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 60)) (10.2.0)\n",
      "Requirement already satisfied: mpmath==1.3.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 61)) (1.3.0)\n",
      "Requirement already satisfied: multidict==6.0.5 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 62)) (6.0.5)\n",
      "Requirement already satisfied: multiprocess==0.70.16 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 63)) (0.70.16)\n",
      "Requirement already satisfied: murmurhash==1.0.10 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 64)) (1.0.10)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 65)) (1.6.0)\n",
      "Requirement already satisfied: networkx==3.3 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 66)) (3.3)\n",
      "Requirement already satisfied: nltk==3.8.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 67)) (3.8.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 68)) (1.26.4)\n",
      "Requirement already satisfied: outcome==1.3.0.post0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 69)) (1.3.0.post0)\n",
      "Requirement already satisfied: packaging==24.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 70)) (24.0)\n",
      "Requirement already satisfied: pandas==2.2.2 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 71)) (2.2.2)\n",
      "Requirement already satisfied: parso==0.8.4 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 72)) (0.8.4)\n",
      "Requirement already satisfied: pillow==10.3.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 73)) (10.3.0)\n",
      "Requirement already satisfied: platformdirs==4.2.2 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 74)) (4.2.2)\n",
      "Requirement already satisfied: pluggy==1.5.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 75)) (1.5.0)\n",
      "Requirement already satisfied: preshed==3.0.9 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 76)) (3.0.9)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.43 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 77)) (3.0.43)\n",
      "Requirement already satisfied: protobuf==4.25.3 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 78)) (4.25.3)\n",
      "Requirement already satisfied: psutil==5.9.8 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 79)) (5.9.8)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 80)) (0.2.2)\n",
      "Requirement already satisfied: pyarrow==16.0.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 81)) (16.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix==0.6 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 82)) (0.6)\n",
      "Requirement already satisfied: pycparser==2.22 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 83)) (2.22)\n",
      "Requirement already satisfied: pydantic==2.7.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 84)) (2.7.1)\n",
      "Requirement already satisfied: pydantic_core==2.18.2 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 85)) (2.18.2)\n",
      "Requirement already satisfied: pydeck==0.9.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 86)) (0.9.0)\n",
      "Requirement already satisfied: Pygments==2.18.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 87)) (2.18.0)\n",
      "Requirement already satisfied: PySocks==1.7.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 88)) (1.7.1)\n",
      "Requirement already satisfied: pytest==8.2.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 89)) (8.2.0)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 90)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz==2024.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 91)) (2024.1)\n",
      "Requirement already satisfied: pywin32==306 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 92)) (306)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 93)) (6.0.1)\n",
      "Requirement already satisfied: pyzmq==26.0.3 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 94)) (26.0.3)\n",
      "Requirement already satisfied: referencing==0.35.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 95)) (0.35.1)\n",
      "Requirement already satisfied: regex==2024.4.28 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 96)) (2024.4.28)\n",
      "Requirement already satisfied: requests==2.31.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 97)) (2.31.0)\n",
      "Requirement already satisfied: rich==13.7.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 98)) (13.7.1)\n",
      "Requirement already satisfied: rpds-py==0.18.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 99)) (0.18.1)\n",
      "Requirement already satisfied: safetensors==0.4.3 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 100)) (0.4.3)\n",
      "Requirement already satisfied: scikit-learn==1.5.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 101)) (1.5.0)\n",
      "Requirement already satisfied: scipy==1.13.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 102)) (1.13.1)\n",
      "Requirement already satisfied: selenium==4.21.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 103)) (4.21.0)\n",
      "Requirement already satisfied: six==1.16.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 104)) (1.16.0)\n",
      "Requirement already satisfied: smart-open==6.4.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 105)) (6.4.0)\n",
      "Requirement already satisfied: smmap==5.0.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 106)) (5.0.1)\n",
      "Requirement already satisfied: sniffio==1.3.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 107)) (1.3.1)\n",
      "Requirement already satisfied: sortedcontainers==2.4.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 108)) (2.4.0)\n",
      "Requirement already satisfied: soupsieve==2.5 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 109)) (2.5)\n",
      "Requirement already satisfied: spacy==3.7.4 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 110)) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy==3.0.12 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 111)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers==1.0.5 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 112)) (1.0.5)\n",
      "Requirement already satisfied: srsly==2.4.8 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 113)) (2.4.8)\n",
      "Requirement already satisfied: st-annotated-text==4.0.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 114)) (4.0.1)\n",
      "Requirement already satisfied: stack-data==0.6.3 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 115)) (0.6.3)\n",
      "Requirement already satisfied: streamlit==1.34.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 116)) (1.34.0)\n",
      "Requirement already satisfied: sympy==1.12 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 117)) (1.12)\n",
      "Requirement already satisfied: tbb==2021.12.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 118)) (2021.12.0)\n",
      "Requirement already satisfied: tenacity==8.3.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 119)) (8.3.0)\n",
      "Requirement already satisfied: thinc==8.2.3 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 120)) (8.2.3)\n",
      "Requirement already satisfied: threadpoolctl==3.5.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 121)) (3.5.0)\n",
      "Requirement already satisfied: tokenizers==0.19.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 122)) (0.19.1)\n",
      "Requirement already satisfied: toml==0.10.2 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 123)) (0.10.2)\n",
      "Requirement already satisfied: tomli==2.0.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 124)) (2.0.1)\n",
      "Requirement already satisfied: toolz==0.12.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 125)) (0.12.1)\n",
      "Requirement already satisfied: torch==2.3.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 126)) (2.3.0)\n",
      "Requirement already satisfied: torchtext==0.18.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 127)) (0.18.0)\n",
      "Requirement already satisfied: torchvision==0.18.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 128)) (0.18.0)\n",
      "Requirement already satisfied: tornado==6.4 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 129)) (6.4)\n",
      "Requirement already satisfied: tqdm==4.66.4 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 130)) (4.66.4)\n",
      "Requirement already satisfied: traitlets==5.14.3 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 131)) (5.14.3)\n",
      "Requirement already satisfied: transformers==4.40.2 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 132)) (4.40.2)\n",
      "Requirement already satisfied: trio==0.25.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 133)) (0.25.1)\n",
      "Requirement already satisfied: trio-websocket==0.11.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 134)) (0.11.1)\n",
      "Requirement already satisfied: typer==0.9.4 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 135)) (0.9.4)\n",
      "Requirement already satisfied: typing_extensions==4.11.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 136)) (4.11.0)\n",
      "Requirement already satisfied: tzdata==2024.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 137)) (2024.1)\n",
      "Requirement already satisfied: urllib3==2.2.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 138)) (2.2.1)\n",
      "Requirement already satisfied: wasabi==1.1.2 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 139)) (1.1.2)\n",
      "Requirement already satisfied: watchdog==4.0.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 140)) (4.0.0)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 141)) (0.2.13)\n",
      "Requirement already satisfied: weasel==0.3.4 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 142)) (0.3.4)\n",
      "Requirement already satisfied: Werkzeug==3.0.3 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 143)) (3.0.3)\n",
      "Requirement already satisfied: wikipedia==1.4.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 144)) (1.4.0)\n",
      "Requirement already satisfied: Wikipedia-API==0.6.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 145)) (0.6.0)\n",
      "Requirement already satisfied: wsproto==1.2.0 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 146)) (1.2.0)\n",
      "Requirement already satisfied: xxhash==3.4.1 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 147)) (3.4.1)\n",
      "Requirement already satisfied: yarl==1.9.4 in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from -r requirements.txt (line 148)) (1.9.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\harry\\envs\\nlp-group-cw\\lib\\site-packages (from marisa-trie==1.1.1->-r requirements.txt (line 54)) (69.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from datasets import DatasetDict, Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, pipeline\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "import gc\n",
    "from transformers import Trainer\n",
    "from datasets import DatasetDict, Dataset\n",
    "from transformers.pipelines import PIPELINE_REGISTRY\n",
    "from pipeline import NER_Pipeline\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"torchtext Version: \", torchtext.__version__)\n",
    "print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"surrey-nlp/PLOD-CW\", cache_dir=None, download_mode=\"force_redownload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['B-O', 'B-AC', 'B-LF', 'I-LF']\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "print(f\"train size: {len(train)}\")\n",
    "val = dataset['validation']\n",
    "print(f\"val size: {len(val)}\")\n",
    "test = dataset['test']\n",
    "print(f\"test size: {len(test)}\")\n",
    "\n",
    "def flatten(A):\n",
    "    rt = []\n",
    "    for i in A:\n",
    "        if isinstance(i,list): rt.extend(flatten(i))\n",
    "        else: rt.append(i)\n",
    "    return rt\n",
    "\n",
    "from collections import Counter\n",
    "flat = flatten(train[\"ner_tags\"])\n",
    "print(Counter(flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tags(tag_sequences, possible_tags):\n",
    "    \"\"\"\n",
    "    Decodes a sequence of numerical tags into a list of corresponding textual labels.\n",
    "\n",
    "    Args:\n",
    "        tag_sequence: A list of integers representing numerical tags.\n",
    "        possible_tags: A list of strings representing the possible textual labels.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings representing the decoded textual tags.\n",
    "    \"\"\"\n",
    "\n",
    "    decoded_tags = [[possible_tags[tag] for tag in row] for row in tag_sequences]\n",
    "    return decoded_tags\n",
    "\n",
    "\n",
    "def build_dataset(filtered_set, cw_set, num_of_samples):\n",
    "    \"\"\"\n",
    "    Merges a specified number of rows from a larger list to a smaller list, ensuring no duplicates.\n",
    "\n",
    "    Args:\n",
    "        filtered_set: a split of the filtered dataset\n",
    "        cw_set: a split of the cw dataset\n",
    "        num_of_samples: The number of rows to add from the filtered set.\n",
    "\n",
    "    Returns:\n",
    "        new tokens, pos_tags and ner_tags lists\n",
    "    \"\"\"\n",
    "    # set up the initial lists\n",
    "    tokens = cw_set[\"tokens\"]\n",
    "    pos_tags = cw_set[\"pos_tags\"]\n",
    "    ner_tags = cw_set[\"ner_tags\"]\n",
    "     \n",
    "    # set up the filtered lists\n",
    "    # tokens\n",
    "    filtered_tokens = filtered_set[\"tokens\"]\n",
    "    # pos_tags\n",
    "    filtered_label_list = filtered_set.features[f\"pos_tags\"].feature.names\n",
    "    filtered_pos_tags = decode_tags(filtered_set[\"pos_tags\"], filtered_label_list)\n",
    "    # ner_tags\n",
    "    filtered_label_list = filtered_set.features[f\"ner_tags\"].feature.names\n",
    "    filtered_ner_tags = decode_tags(filtered_set[\"ner_tags\"], filtered_label_list)\n",
    "\n",
    "    # convert the tokens list to sets for efficient duplicate checking\n",
    "    tokens_set = set(tuple(row) for row in tokens)\n",
    "    filtered_tokens_set = set(tuple(row) for row in filtered_tokens)\n",
    "\n",
    "    # find rows to add\n",
    "    rows_to_add = []\n",
    "    for index, row in enumerate(filtered_tokens_set):\n",
    "        if tuple(row) not in tokens_set and len(rows_to_add) < num_of_samples:\n",
    "            rows_to_add.append(index)\n",
    "\n",
    "    # Merge and return the lists\n",
    "    tokens = tokens + [filtered_tokens[i] for i in rows_to_add]\n",
    "    pos_tags = pos_tags + [filtered_pos_tags[i] for i in rows_to_add]\n",
    "    ner_tags = ner_tags + [filtered_ner_tags[i] for i in rows_to_add]\n",
    "\n",
    "    return tokens, pos_tags, ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "def encode_tags(tag_sequences, possible_tags):\n",
    "    \"\"\"\n",
    "    Encodes a sequence of string tags into a list of corresponding integer tags.\n",
    "\n",
    "    Args:\n",
    "        tag_sequences: A 2d list of strings representing numerical tags.\n",
    "        possible_tags: A list of strings representing the possible textual labels.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings representing the decoded textual tags.\n",
    "    \"\"\"\n",
    "\n",
    "    encoded_tags = [[possible_tags.index(tag) for tag in row] for row in tag_sequences]\n",
    "    return encoded_tags\n",
    "def tokenize_and_align_labels(data, tokenizer, task):\n",
    "    tokenized_inputs = tokenizer(data[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    converted_tags = encode_tags(data[f\"{task}_tags\"], label_list)\n",
    "    for i, label in enumerate(converted_tags):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_lists_elementwise(list_A, list_B):\n",
    "  \"\"\"\n",
    "  Combines two 2D lists of strings element-wise into a 2D list of tuples.\n",
    "\n",
    "  Args:\n",
    "      list_A: A 2D list of strings (e.g., [['A', 'A', 'A'], ['A', 'A', 'A']]).\n",
    "      list_B: Another 2D list of strings with the same dimensions as list_A.\n",
    "\n",
    "  Returns:\n",
    "      A 2D list of tuples, where each tuple combines corresponding elements from list_A and list_B.\n",
    "\n",
    "  Raises:\n",
    "      ValueError: If the dimensions of list_A and list_B don't match.\n",
    "  \"\"\"\n",
    "\n",
    "  # Check if dimensions match\n",
    "  if len(list_A) != len(list_B) or len(list_A[0]) != len(list_B[0]):\n",
    "    raise ValueError(\"Dimensions of lists A and B must be equal.\")\n",
    "\n",
    "  # Create the resulting list using list comprehension\n",
    "  return [[(a, b) for a, b in zip(row_a, row_b)] for row_a, row_b in zip(list_A, list_B)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pos_tag(nltk_tag):\n",
    "    \"\"\"\n",
    "    Converts NLTK POS tags to the format expected by the lemmatizer.\n",
    "\n",
    "    Args:\n",
    "        nltk_tag: The POS tag in NLTK format (e.g., VBG, NNS).\n",
    "\n",
    "    Returns:\n",
    "        The corresponding POS tag for the lemmatizer (n, v, a, r, or s) or None if no match.\n",
    "    \"\"\"\n",
    "\n",
    "    tag_map = {\n",
    "        'NUM': '',  # Number (not handled by lemmatizer)\n",
    "        'CCONJ': '',  # Coordinating conjunction (not handled)\n",
    "        'PRON': '',  # Pronoun (not handled)\n",
    "        'NOUN': 'n',   # Noun\n",
    "        'SCONJ': '',  # Subordinating conjunction (not handled)\n",
    "        'SYM': '',   # Symbol (not handled)\n",
    "        'INTJ': '',  # Interjection (not handled)\n",
    "        'ADJ': 'a',    # Adjective\n",
    "        'ADP': '',   # Preposition (not handled)\n",
    "        'PUNCT': '',  # Punctuation (not handled)\n",
    "        'ADV': 'r',    # Adverb\n",
    "        'AUX': 'v',    # Auxiliary verb\n",
    "        'DET': '',   # Determiner (not handled)\n",
    "        'VERB': 'v',   # Verb\n",
    "        'X': '',      # Other (not handled)\n",
    "        'PART': '',   # Particle (not handled)\n",
    "        'PROPN': 'n',   # Proper noun\n",
    "    }\n",
    "    return tag_map.get(nltk_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_list(data, pos_tags):\n",
    "    \"\"\"\n",
    "    Lemmatizes a 2D list of tokens using NLTK.\n",
    "\n",
    "    Args:\n",
    "        data: A 2D list of strings (tokens) to be lemmatized.\n",
    "\n",
    "    Returns:\n",
    "        A 2D list containing the lemmatized tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the WordNet lemmatizer\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    pos_tags = [[convert_pos_tag(tag) for tag in row] for row in pos_tags]\n",
    "\n",
    "\n",
    "    data = combine_lists_elementwise(data, pos_tags)\n",
    "\n",
    "\n",
    "    # Lemmatize with part-of-speech information\n",
    "    lemmatized_data = [[token if pos == '' else lemmatizer.lemmatize(token, pos) for token, pos in row] for row in data]\n",
    "\n",
    "    return lemmatized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(tokens, pos_tags):\n",
    "    # lemmatize the data\n",
    "    data = lemmatize_list(tokens, pos_tags)\n",
    "    # lowercase the data\n",
    "    data = [[string.lower() for string in row] for row in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = pre_process_data(train[\"tokens\"], train[\"pos_tags\"])\n",
    "val_tokens = pre_process_data(val[\"tokens\"], val[\"pos_tags\"])\n",
    "test_tokens = pre_process_data(test[\"tokens\"], test[\"pos_tags\"])\n",
    "original_train_tokens = train[\"tokens\"]\n",
    "original_val_tokens = val[\"tokens\"]\n",
    "print(f\"original train tokens: {original_train_tokens[0]}\\npre-processed train tokens: {train_tokens[0]}\")\n",
    "print(f\"original val tokens: {original_val_tokens[0]}\\npre-processed val tokens: {val_tokens[0]}\")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"tokens\": train_tokens, \"pos_tags\": train[\"pos_tags\"], \"ner_tags\": train[\"ner_tags\"]}),\n",
    "    \"validation\": Dataset.from_dict({\"tokens\": val_tokens, \"pos_tags\": val[\"pos_tags\"], \"ner_tags\": val[\"ner_tags\"]}),\n",
    "    \"test\": Dataset.from_dict({\"tokens\": test_tokens, \"pos_tags\": test[\"pos_tags\"], \"ner_tags\": test[\"ner_tags\"]}),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Extra Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = load_dataset(\"surrey-nlp/PLOD-filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train = filtered_dataset[\"train\"]\n",
    "print(f\"train size: {len(filtered_train)}\")\n",
    "filtered_val = filtered_dataset[\"validation\"]\n",
    "print(f\"val size: {len(filtered_val)}\")\n",
    "filtered_test = filtered_dataset[\"test\"]\n",
    "print(f\"test size: {len(filtered_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium = 10000\n",
    "tokens_medium, pos_tags_medium, ner_tags_medium = build_dataset(filtered_train, train, medium)\n",
    "tokens_medium = pre_process_data(tokens_medium, pos_tags_medium)\n",
    "print(f\"num of medium samples: {len(tokens_medium)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_datasets_dict = {\n",
    "    \"train\": Dataset.from_dict({\"tokens\": tokens_medium, \"pos_tags\": pos_tags_medium, \"ner_tags\": ner_tags_medium})\n",
    "}\n",
    "medium_dataset = DatasetDict(medium_datasets_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True) # use AutoTokenizer because it defaults to fast tokenizers where as using the BERT Tokenizer does not\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    overall_results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    true_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "    \n",
    "    \n",
    "    results = classification_report(true_labels, true_predictions, labels = label_list)\n",
    "    print(results)\n",
    "    return {\n",
    "        \"precision\": overall_results[\"overall_precision\"],\n",
    "        \"recall\": overall_results[\"overall_recall\"],\n",
    "        \"f1\": overall_results[\"overall_f1\"],\n",
    "        \"accuracy\": overall_results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_func(data):\n",
    "    return tokenize_and_align_labels(data, tokenizer, task)\n",
    "tokenized_datasets = dataset.map(intermediate_func, batched=True)\n",
    "medium_tokenized_dataset = medium_dataset.map(intermediate_func, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_checkpoint}-finetuned-GROUP-{task}\",\n",
    "    evaluation_strategy ='steps',\n",
    "    eval_steps = 7000,\n",
    "    logging_steps = 500,\n",
    "    save_total_limit = 1,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.001,\n",
    "    save_steps=0,\n",
    "    metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=medium_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear CUDA memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear CUDA memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"roberta_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(\"SurtMcGert/NLP-group-CW-roberta-ner-tagging\")\n",
    "trainer.model.push_to_hub(\"SurtMcGert/NLP-group-CW-roberta-ner-tagging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register custom pipeline\n",
    "PIPELINE_REGISTRY.register_pipeline(\n",
    "    \"NER_NLP_tagger\",\n",
    "    pipeline_class = NER_Pipeline,\n",
    "    pt_model = AutoModelForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tagger = pipeline(\"NER_NLP_tagger\", model = \"SurtMcGert/NLP-group-CW-roberta-ner-tagging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tagger.requires_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ner_tagger(\"this is a test on our Natural Language Processing (NLP) tagging Artificial Intelligence (AI).\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers.pipelines import PIPELINE_REGISTRY\n",
    "from pipeline import NER_Pipeline\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from faker import Faker\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(expected, predicted):\n",
    "    if (expected == predicted):\n",
    "        print(\"============ Test passed ============\")\n",
    "    else:\n",
    "        print(f\"============ Test failed ============\\nPredicted:\\n{predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up pipeline for unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register custom pipeline\n",
    "PIPELINE_REGISTRY.register_pipeline(\n",
    "    \"NER_NLP_tagger\",\n",
    "    pipeline_class=NER_Pipeline,\n",
    "    pt_model=transformers.AutoModelForTokenClassification\n",
    ")\n",
    "\n",
    "# Load NER tagger pipeline\n",
    "ner_tagger = transformers.pipeline(\n",
    "    \"NER_NLP_tagger\", model=\"SurtMcGert/NLP-group-CW-roberta-ner-tagging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to see if the `get_acronym_description()` function works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"Federal Bureau of Investigation (FBI)\"\n",
    "\n",
    "expected = \"The Federal Bureau of Investigation (FBI) is the domestic intelligence and security service of the United States and its principal federal law enforcement agency\"\n",
    "\n",
    "predicted = get_acronym_description(input)\n",
    "\n",
    "if (predicted == \"\"):\n",
    "    print(True)\n",
    "else:\n",
    "    compare(expected, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to see if the `get_acronym_and_long_forms()` function works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [(\"This\", \"B-O\"), (\"is\", \"B-O\"), (\"a\", \"B-O\"), (\"test\", \"B-O\"), (\"Federal\", \"B-LF\"), (\"Bureau\", \"I-LF\"), (\"of\", \"I-LF\"), (\"Investigation\", \"I-LF\"), (\"FBI\", \"B-AC\")]\n",
    "\n",
    "expected = {\n",
    "    \"Acronyms\": [\"FBI\"],\n",
    "    \"Long Forms\": [\"Federal Bureau of Investigation\"]\n",
    "}\n",
    "\n",
    "predicted = get_acronyms_and_long_forms(input)\n",
    "\n",
    "compare(expected, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the front-end functionality. This test will create a list of a mix of randomly-generated and pre-defined texts, and feed it through an actual browser using the Selenium library. The web server must be running in order for this test to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of input texts\n",
    "test_input = [\n",
    "    \"This is a test\",\n",
    "    \"1 2 3 4 5 6 7 8 9 10\",\n",
    "    \"! @ # $ % ^ & * ( ) - _ + = { } [ ] | \\ : ; ' < > , . / ?\",\n",
    "    \"<unk> <UNK> <pad> <PAD> <cls> <CLS>\"\n",
    "    \"Wenn es aus irgendeinem Grund keinem der Teammitglieder gelingt, den individuellen Teil der Prfung abzuschlieen und Sie haben kein Modell, das Sie einsetzen knnen, knnen Sie sich entweder an den Dozenten wenden, um ein vorgefertigtes Modell zu erhalten Modell zu bekommen, oder Sie knnen das vorgefertigte Modell verwenden, das Sie online gefunden haben. In jedem Fall sollte dies sollte dies entsprechend im Notebook dokumentiert werden\",\n",
    "    \"Born in Singapore during British colonial rule, Lee is the eldest son of Singapore's first prime minister, Lee Kuan Yew. He graduated from Trinity College, Cambridge, in 1974 with a bachelor of arts in mathematics with first class honours and a Diploma in Computer Science with distinction (equivalent to a first-class master's in computer science). He served in the Singapore Armed Forces (SAF) between 1971 and 1984, and attained the rank Brigadier-General, completing a Master of Public Administration degree at Harvard Kennedy School in 1980. Lee resigned from the SAF in 1984 to enter politics and was elected the MP for Teck Ghee SMC. Since its dissolution in 1991, he has represented the Teck Ghee ward of Ang Mo Kio GRC. \",\n",
    "    \"However, studies have shown that even vacuum cleaners featuring HEPA (High Efficiency Particulate Air) filters tend to release a large amount of allergens back into the air in the exhaust. In general, more recent and more expensive models do perform better than older and less expensive ones.\",\n",
    "    \"Ragin has made many contributions to sociology. He is a proponent of using fuzzy sets to bridge the divide between quantitative and qualitative methods. His main interests are methodology, political sociology, and comparative-historical research, with a special focus on such topics as the welfare state, ethnic political mobilization, and international political economy. He is also the author of more than 100 articles in research journals and edited books, and he has developed software packages for set theory analyses of social data, Qualitative Comparative Analysis (QCA) and fuzzy set Qualitative Comparative Analysis (fsQCA). \",\n",
    "]\n",
    "\n",
    "# Add additional randomly-generated texts, each one longer than the last\n",
    "fake = Faker()\n",
    "for i in range(4):\n",
    "    test_input.append(fake.text(max_nb_chars=((i + 1) * 1000)))\n",
    "\n",
    "# Create a list of the page elements we want to find on the results page\n",
    "xpaths = [\n",
    "    \"/html/body/div[1]/div[1]/div[1]/div/div/div/section/div[1]/div/div/div/div[6]/div[2]/div/div/div/div/div/div/table\",\n",
    "    \"/html/body/div[1]/div[1]/div[1]/div/div/div/section/div[1]/div/div/div/div[4]/div/div/p/span\",\n",
    "    \"/html/body/div[1]/div[1]/div[1]/div/div/div/section/div[1]/div/div/div/div[6]/div[2]/div/div/div/div/div/div/table\"\n",
    "]\n",
    "\n",
    "# Create web browser\n",
    "browser = webdriver.Firefox()\n",
    "\n",
    "# Go to website\n",
    "browser.get(\"http://localhost:8501\")\n",
    "\n",
    "# Wait for website to load\n",
    "time.sleep(5)\n",
    "\n",
    "test_pass = True\n",
    "for i, input_string in enumerate(test_input):\n",
    "    # Find the input text box\n",
    "    element = WebDriverWait(browser, 20).until(EC.element_to_be_clickable((By.XPATH, \"//*[@id=\\\"text_input_1\\\"]\")))\n",
    "    # Since this test runs repeatedly, clear the text box of the last test input\n",
    "    element.clear()\n",
    "    # Input the sample text\n",
    "    element.send_keys(input_string)\n",
    "    # Press the Enter key\n",
    "    element.send_keys(Keys.RETURN)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Look to see if the elements are generated correctly\n",
    "    for xpath in xpaths:\n",
    "        try:\n",
    "            element = WebDriverWait(browser, 20).until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "        except:\n",
    "            # Fail the test if it cannot find one\n",
    "            test_pass = False\n",
    "            print(f\"Test failed - could not find element with XPATH {xpath}\")\n",
    "\n",
    "    # Pause just so we can see the results page\n",
    "    time.sleep(7)\n",
    "\n",
    "# Close the browser\n",
    "browser.quit()\n",
    "\n",
    "if (test_pass == True):\n",
    "    print(\"Test passed\")\n",
    "else:\n",
    "    print(\"Test failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
