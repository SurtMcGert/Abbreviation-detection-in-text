{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Preparation"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:24:58.265034Z","iopub.status.busy":"2024-03-25T14:24:58.264692Z","iopub.status.idle":"2024-03-25T14:25:52.446044Z","shell.execute_reply":"2024-03-25T14:25:52.444942Z","shell.execute_reply.started":"2024-03-25T14:24:58.265007Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["DEPRECATION: torchsde 0.2.5 has a non-standard dependency specifier numpy>=1.19.*; python_version >= \"3.7\". pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torchsde or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"]},{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["DEPRECATION: torchsde 0.2.5 has a non-standard dependency specifier numpy>=1.19.*; python_version >= \"3.7\". pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torchsde or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"]},{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["DEPRECATION: torchsde 0.2.5 has a non-standard dependency specifier numpy>=1.19.*; python_version >= \"3.7\". pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torchsde or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"]},{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["DEPRECATION: torchsde 0.2.5 has a non-standard dependency specifier numpy>=1.19.*; python_version >= \"3.7\". pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torchsde or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"]}],"source":["# Install dependencies\n","%pip install -q ipywidgets transformers tqdm\n","%pip install -q seqeval\n","%pip install -q accelerate\n","%pip install -q transformers[torch]"]},{"cell_type":"markdown","metadata":{},"source":["## Set Seed and CUDA"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:28:25.922157Z","iopub.status.busy":"2024-03-25T14:28:25.921178Z","iopub.status.idle":"2024-03-25T14:28:25.929740Z","shell.execute_reply":"2024-03-25T14:28:25.928558Z","shell.execute_reply.started":"2024-03-25T14:28:25.922122Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["PyTorch Version:  2.2.1+cu121\n","Using GPU.\n"]}],"source":["import torch\n","import torchtext\n","\n","SEED = 1234\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","print(\"PyTorch Version: \", torch.__version__)\n","print(\"torchtext Version: \", torchtext.__version__)\n","print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")"]},{"cell_type":"markdown","metadata":{},"source":["## Data Prep"]},{"cell_type":"markdown","metadata":{},"source":["### Download the Dataset\n","this will download the huggingface dataset ready for use"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:28:28.413008Z","iopub.status.busy":"2024-03-25T14:28:28.412639Z","iopub.status.idle":"2024-03-25T14:29:43.158021Z","shell.execute_reply":"2024-03-25T14:29:43.156962Z","shell.execute_reply.started":"2024-03-25T14:28:28.412980Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a8a6d701780482e9980915fd61fbdc2","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset plo_dfiltered_config/PLODfiltered to /root/.cache/huggingface/datasets/surrey-nlp___plo_dfiltered_config/PLODfiltered/0.0.2/bd402e453833e246144db398363ec772d4710a494fc4de15e48c7b1ddac6b82b...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0901662242244eaaa3e992c23193bdfb","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"44927e73497f4488b7b6f5808ba4e823","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/85.0M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f18ec658189f43bcbeb77fac8b648536","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/18.2M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8ec2b16e0c594703b8846c3f099649cf","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/18.2M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec83b3cd48eb460f98f0a28b01bc8386","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset plo_dfiltered_config downloaded and prepared to /root/.cache/huggingface/datasets/surrey-nlp___plo_dfiltered_config/PLODfiltered/0.0.2/bd402e453833e246144db398363ec772d4710a494fc4de15e48c7b1ddac6b82b. Subsequent calls will reuse this data.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3402f8e9779a4be1b9f2df0c7496a439","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"surrey-nlp/PLOD-filtered\")"]},{"cell_type":"markdown","metadata":{},"source":["### Get Label List\n","this gets the list of labels for the dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:29:56.074155Z","iopub.status.busy":"2024-03-25T14:29:56.073303Z","iopub.status.idle":"2024-03-25T14:29:56.079048Z","shell.execute_reply":"2024-03-25T14:29:56.078148Z","shell.execute_reply.started":"2024-03-25T14:29:56.074122Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['B-O', 'B-AC', 'I-AC', 'B-LF', 'I-LF']\n"]}],"source":["label_list = dataset[\"train\"].features[f\"ner_tags\"].feature.names\n","print(label_list)"]},{"cell_type":"markdown","metadata":{},"source":["### Split The Set Into Train, Val and Test Sets"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:30:00.770951Z","iopub.status.busy":"2024-03-25T14:30:00.770111Z","iopub.status.idle":"2024-03-25T14:30:00.777628Z","shell.execute_reply":"2024-03-25T14:30:00.775446Z","shell.execute_reply.started":"2024-03-25T14:30:00.770921Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["train size: 112652\n","train size: 24140\n","train size: 24140\n"]}],"source":["train = dataset['train']\n","print(f\"train size: {len(train)}\")\n","val = dataset['validation']\n","print(f\"train size: {len(val)}\")\n","test = dataset['test']\n","print(f\"train size: {len(test)}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Display Random Samples\n","this code displays some random samples from the train set"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:30:07.459609Z","iopub.status.busy":"2024-03-25T14:30:07.458742Z","iopub.status.idle":"2024-03-25T14:30:07.490355Z","shell.execute_reply":"2024-03-25T14:30:07.489300Z","shell.execute_reply.started":"2024-03-25T14:30:07.459575Z"},"trusted":true},"outputs":[{"data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tokens</th>\n","      <th>pos_tags</th>\n","      <th>ner_tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>85508</td>\n","      <td>[Comparison, of, different, cutpoints, and, conditions, for, the, receiver, operating, characteristic, (, ROC, ), curve, predicting, MPNST, vs., plexiform, neurofibroma, clinical, status, .]</td>\n","      <td>[NOUN, ADP, ADJ, NOUN, CCONJ, NOUN, ADP, DET, NOUN, VERB, ADJ, PUNCT, PROPN, PUNCT, NOUN, VERB, PROPN, ADP, ADJ, NOUN, ADJ, NOUN, PUNCT]</td>\n","      <td>[B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-LF, I-LF, I-LF, B-O, B-AC, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>98192</td>\n","      <td>[In, S., cerevisiae, ,, the, ubiquitin, -, like, protein, Atg8, is, C, -, terminally, conjugated, to, the, phospholipid, phosphatidylethanolamine, (, PE, ), to, generate, Atg8, -, PE, ., During, autophagosome, formation, ,, Atg8, -, PE, is, cleaved, by, Atg4, to, release, delipidated, Atg8, (, Atg8G116, ), and, PE, ., Although, delipidation, of, Atg8, -, PE, is, important, for, autophagosome, formation, ,, it, remains, controversial, whether, the, delipidation, reaction, is, required, for, targeting, of, Atg8, to, the, VICS, or, for, subsequent, IM, expansion, .]</td>\n","      <td>[ADP, PROPN, NOUN, PUNCT, DET, NOUN, PUNCT, ADJ, NOUN, PROPN, AUX, NOUN, PUNCT, ADV, VERB, ADP, DET, NOUN, NOUN, PUNCT, PROPN, PUNCT, ADP, VERB, PROPN, PUNCT, PROPN, PUNCT, ADP, NOUN, NOUN, PUNCT, PROPN, PUNCT, PROPN, AUX, VERB, ADP, PROPN, ADP, VERB, VERB, PROPN, PUNCT, NOUN, PUNCT, CCONJ, PROPN, PUNCT, SCONJ, NOUN, ADP, PROPN, PUNCT, PROPN, AUX, ADJ, ADP, NOUN, NOUN, PUNCT, PRON, VERB, ADJ, SCONJ, DET, NOUN, NOUN, AUX, VERB, ADP, NOUN, ADP, PROPN, ADP, DET, PROPN, CCONJ, ADP, ADJ, PROPN, NOUN, PUNCT]</td>\n","      <td>[B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-LF, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>77088</td>\n","      <td>[Moreover, ,, tissue, -, specific, rescue, experiments, indicate, that, the, circadian, clock, component, PERIOD, (, PER, ), [, 7, ], and, the, circadian, output, ion, channel, NARROW, ABDOMEN, (, NA, ), [, 16, ], are, each, required, in, the, PDF+, LNv, to, promote, robust, ,, sustained, DD, rhythmicity, .]</td>\n","      <td>[ADV, PUNCT, NOUN, PUNCT, ADJ, NOUN, NOUN, VERB, SCONJ, DET, ADJ, NOUN, NOUN, NOUN, PUNCT, PROPN, PUNCT, X, X, X, CCONJ, DET, ADJ, NOUN, NOUN, NOUN, NOUN, NOUN, PUNCT, PROPN, PUNCT, X, X, X, AUX, PRON, VERB, ADP, DET, PROPN, PROPN, PART, VERB, ADJ, PUNCT, ADJ, PROPN, NOUN, PUNCT]</td>\n","      <td>[B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-LF, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-LF, I-LF, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>23937</td>\n","      <td>[The, last, study, found, that, a, model, for, predicting, major, renal, events, using, eGFR, and, albumin, /, creatinine, ratio, (, ACR, ), (, AUC, :, 0.818, ), was, superior, to, models, with, either, of, the, predictors, alone, (, AUC, :, 0.779, for, eGFR, ,, and, 0.752, for, ACR, ), ;, all, three, models, were, inferior, to, an, expanded, model, with, five, additional, variables, (, AUC, :, 0.847, ), (, all, p&lt;0.05, for, AUC, comparison, ), [, 27, ], .]</td>\n","      <td>[DET, ADJ, NOUN, VERB, SCONJ, DET, NOUN, ADP, VERB, ADJ, ADJ, NOUN, VERB, PROPN, CCONJ, NOUN, SYM, NOUN, NOUN, PUNCT, PROPN, PUNCT, PUNCT, PROPN, PUNCT, NUM, PUNCT, AUX, ADJ, ADP, NOUN, ADP, PRON, ADP, DET, NOUN, ADV, PUNCT, PROPN, PUNCT, NUM, ADP, PROPN, PUNCT, CCONJ, NUM, ADP, PROPN, PUNCT, PUNCT, PRON, NUM, NOUN, AUX, ADJ, ADP, DET, VERB, NOUN, ADP, NUM, ADJ, NOUN, PUNCT, PROPN, PUNCT, NUM, PUNCT, PUNCT, PRON, NUM, ADP, PROPN, NOUN, PUNCT, PUNCT, NUM, PUNCT, PUNCT]</td>\n","      <td>[B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-LF, I-LF, I-LF, I-LF, B-O, B-AC, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>20983</td>\n","      <td>[Since, the, randomized, controlled, trial, conducted, by, Patchell, et, al.[6, ], showing, that, surgery, followed, by, radiotherapy, provided, superior, neurologic, outcomes, compared, to, radiotherapy, alone, in, patients, suffering, from, a, single, cervical, or, thoracic, SSM, with, a, life, expectancy, of, ≥, 3, months, ,, this, life, expectancy, threshold, has, been, widely, adopted, in, decision, -, making, for, surgical, treatment.[7–9, ], However, ,, clinicians, and, surgeons, tend, to, estimate, survival, in, patients, with, advanced, cancer, inaccurately.[10–13, ], Also, ,, although, several, studies, reported, that, surgical, intervention, improved, health, related, quality, of, life, (, HRQoL)[6, ,, 9, ,, 14–20, ], ,, SSM, ...]</td>\n","      <td>[SCONJ, DET, ADJ, ADJ, NOUN, VERB, ADP, PROPN, PROPN, X, PUNCT, VERB, SCONJ, NOUN, VERB, ADP, NOUN, VERB, ADJ, ADJ, NOUN, VERB, PART, NOUN, ADV, ADP, NOUN, VERB, ADP, DET, ADJ, ADJ, CCONJ, ADJ, PROPN, ADP, DET, NOUN, NOUN, ADP, PUNCT, NUM, NOUN, PUNCT, DET, NOUN, NOUN, NOUN, AUX, AUX, ADV, VERB, ADP, NOUN, PUNCT, NOUN, ADP, ADJ, NOUN, PUNCT, ADV, PUNCT, NOUN, CCONJ, NOUN, VERB, PART, VERB, NOUN, ADP, NOUN, ADP, ADJ, NOUN, ADV, PUNCT, ADV, PUNCT, SCONJ, ADJ, NOUN, VERB, SCONJ, ADJ, NOUN, VERB, NOUN, VERB, NOUN, ADP, NOUN, PUNCT, PROPN, PUNCT, NUM, PUNCT, NUM, PUNCT, PUNCT, PROPN, ...]</td>\n","      <td>[B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-LF, I-LF, I-LF, I-LF, I-LF, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, ...]</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>51185</td>\n","      <td>[The, transepithelial, electrical, resistance, (, TEER, ), was, measured, with, an, eipithelial, voltohmmeter, EVOM2, (, World, Precision, Instruments, ,, Sarasota, ,, FL, ,, USA, ), ., The, value, of, each, individual, transwell, was, calculated, by, subtracting, the, value, of, a, coated, transwell, without, cells, .]</td>\n","      <td>[DET, ADJ, ADJ, NOUN, PUNCT, PROPN, PUNCT, AUX, VERB, ADP, DET, ADJ, NOUN, PROPN, PUNCT, PROPN, PROPN, PROPN, PUNCT, PROPN, PUNCT, PROPN, PUNCT, PROPN, PUNCT, PUNCT, DET, NOUN, ADP, DET, ADJ, NOUN, AUX, VERB, ADP, VERB, DET, NOUN, ADP, DET, VERB, NOUN, ADP, NOUN, PUNCT]</td>\n","      <td>[B-O, B-LF, I-LF, I-LF, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O]</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>36894</td>\n","      <td>[In, order, to, further, clarify, the, molecular, mechanism(s, ), underlying, replicative, life, span, extension, by, calorie, restriction, in, yeast, ,, we, have, sought, to, directly, test, key, components, of, the, models, for, Sir2, -, dependent, CR, in, both, of, these, strains, .]</td>\n","      <td>[SCONJ, NOUN, PART, ADV, VERB, DET, ADJ, NOUN, PUNCT, VERB, ADJ, NOUN, NOUN, NOUN, ADP, NOUN, NOUN, ADP, NOUN, PUNCT, PRON, AUX, VERB, PART, ADV, VERB, ADJ, NOUN, ADP, DET, NOUN, ADP, NOUN, PUNCT, ADJ, PROPN, ADP, PRON, ADP, DET, NOUN, PUNCT]</td>\n","      <td>[B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-LF, I-LF, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O]</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>68588</td>\n","      <td>[While, the, relationship, between, behavior, and, GRN, activity, is, much, more, straightforward, in, the, gustatory, system, (, increased, neuronal, response, ,, increased, preference, behavior, ), ,, it, implies, that, another, G, -, protein, might, be, activated, downstream, of, SPR, ., G, -, protein, Gαi, /, s, increases, cAMP, levels, and, Gαq, enhances, phospholipase, C, (, PLC, ), and, calcium, signaling, [, 56, ], .]</td>\n","      <td>[SCONJ, DET, NOUN, ADP, NOUN, CCONJ, PROPN, NOUN, AUX, ADV, ADV, ADJ, ADP, DET, ADJ, NOUN, PUNCT, VERB, ADJ, NOUN, PUNCT, VERB, NOUN, NOUN, PUNCT, PUNCT, PRON, VERB, SCONJ, DET, NOUN, NOUN, NOUN, AUX, AUX, VERB, ADV, ADP, PROPN, PUNCT, NOUN, NOUN, NOUN, NOUN, SYM, NOUN, VERB, NOUN, NOUN, CCONJ, NOUN, VERB, NOUN, PROPN, PUNCT, PROPN, PUNCT, CCONJ, NOUN, NOUN, PUNCT, NUM, PUNCT, PUNCT]</td>\n","      <td>[B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-LF, I-LF, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O]</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>17274</td>\n","      <td>[We, conducted, ChIP, with, a, polyclonal, antibody, to, the, GFP, protein, followed, by, quantitative, real, -, time, PCR, (, ChIP, -, QRT, -, PCR, ), to, test, for, quantitative, enrichment, of, the, eight, candidate, gene, promoters, (, Figure, 3B, ), ., Primers, to, the, 5′, upstream, sequences, of, the, putative, target, genes, were, used, for, PCR, amplification, from, the, immunoprecipitated, DNA, from, wild, -, type, plants, compared, to, immunoprecipitated, DNA, from, transgenic, plants, expressing, the, SHR, :, GFP, fusion, under, control, of, the, native, 2.5, -, kb, SHR, promoter, [, 8, ], .]</td>\n","      <td>[PRON, VERB, PROPN, ADP, DET, ADJ, NOUN, ADP, DET, PROPN, NOUN, VERB, ADP, ADJ, ADJ, PUNCT, NOUN, NOUN, PUNCT, PROPN, PUNCT, PROPN, PUNCT, NOUN, PUNCT, ADP, VERB, ADP, ADJ, NOUN, ADP, DET, NUM, NOUN, NOUN, NOUN, PUNCT, PROPN, NUM, PUNCT, PUNCT, NOUN, ADP, DET, NUM, ADJ, NOUN, ADP, DET, ADJ, NOUN, NOUN, AUX, VERB, ADP, NOUN, NOUN, ADP, DET, VERB, NOUN, ADP, ADJ, PUNCT, NOUN, NOUN, VERB, ADP, VERB, NOUN, ADP, ADJ, NOUN, VERB, DET, PROPN, NOUN, PROPN, NOUN, ADP, NOUN, ADP, DET, ADJ, NUM, PUNCT, NOUN, PROPN, NOUN, PUNCT, NUM, PUNCT, PUNCT]</td>\n","      <td>[B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-LF, I-LF, I-LF, I-LF, B-O, B-O, B-AC, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O]</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>18533</td>\n","      <td>[At, the, end, of, stage, 1, ,, intervention, arms, could, be, dropped, if, p, &gt;, 0.2, compared, to, SOC, ,, or, for, safety, concerns, including, intimate, partner, violence, ,, guided, by, an, independent, data, and, safety, monitoring, board, (, DSMB, ), ., Further, ANC, days, ,, as, determined, to, be, needed, by, sample, size, re, -, calculation, ,, were, then, randomised, in, equal, proportions, to, the, remaining, trial, arms, for, stage, 2, .]</td>\n","      <td>[ADP, DET, NOUN, ADP, NOUN, NUM, PUNCT, NOUN, NOUN, AUX, AUX, VERB, SCONJ, NOUN, SYM, NUM, VERB, ADP, PROPN, PUNCT, CCONJ, ADP, NOUN, NOUN, VERB, ADJ, NOUN, NOUN, PUNCT, VERB, ADP, DET, ADJ, NOUN, CCONJ, NOUN, NOUN, NOUN, PUNCT, PROPN, PUNCT, PUNCT, ADJ, PROPN, NOUN, PUNCT, SCONJ, VERB, ADP, AUX, VERB, ADP, NOUN, NOUN, NOUN, NOUN, NOUN, PUNCT, AUX, ADV, VERB, ADP, ADJ, NOUN, ADP, DET, VERB, NOUN, NOUN, ADP, NOUN, NUM, PUNCT]</td>\n","      <td>[B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-LF, I-LF, I-LF, I-LF, I-LF, B-O, B-AC, B-O, B-O, B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-O]</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas as pd\n","from datasets import ClassLabel, Sequence\n","import random\n","from IPython.display import display, HTML\n","import matplotlib.pyplot as plt\n","def show_random_elements(dataset, num_examples=10):\n","    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n","    picks = []\n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(dataset)-1)\n","        while pick in picks:\n","            pick = random.randint(0, len(dataset)-1)\n","        picks.append(pick)\n","    \n","    df = pd.DataFrame(dataset[picks])\n","    for column, typ in dataset.features.items():\n","        if isinstance(typ, ClassLabel):\n","            df[column] = df[column].transform(lambda i: typ.names[i])\n","        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n","            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n","    display(HTML(df.to_html()))\n","\n","\n","show_random_elements(train)"]},{"cell_type":"markdown","metadata":{},"source":["### Set Task\n","in this project we are doing Named Entity Recognition so I set the task to \"ner\""]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:30:10.432182Z","iopub.status.busy":"2024-03-25T14:30:10.431528Z","iopub.status.idle":"2024-03-25T14:30:10.436432Z","shell.execute_reply":"2024-03-25T14:30:10.435436Z","shell.execute_reply.started":"2024-03-25T14:30:10.432151Z"},"trusted":true},"outputs":[],"source":["task = \"ner\""]},{"cell_type":"markdown","metadata":{},"source":["# Experiment 1 (Model)\n","HMM vs BERT"]},{"cell_type":"markdown","metadata":{},"source":["## HMM\n","The following is the implementation of an HMM model"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import nltk\n","from sklearn.metrics import precision_recall_fscore_support"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'train' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m[:][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      2\u001b[0m tags \u001b[38;5;241m=\u001b[39m train[:][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"]}],"source":["sentences = train[:][\"tokens\"]\n","tags = train[:][\"ner_tags\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def replace_numbers_with_tags(array, tags):\n","  \"\"\"\n","  Replaces numbers in a 2D array with corresponding tags from a provided list.\n","\n","  Args:\n","      array: A 2D list of numbers (e.g., [[0, 1, 2], [3, 4, 1]]).\n","      tags: A list of tags corresponding to the numbers in the array (e.g., ['B-O', 'B-AC', 'I-AC', 'B-LF', 'I-LF']).\n","\n","  Returns:\n","      A new 2D list with numbers replaced by tags.\n","  \"\"\"\n","\n","#   # Check if the lengths of the array and tags list match\n","#   if len(tags) != len(set([item for sublist in array for item in sublist])):\n","#     raise ValueError(\"Number of tags must match the unique numbers in the array.\")\n","\n","  # Create a dictionary for efficient lookup (number -> tag)\n","  tag_dict = dict(zip(sorted(set([item for sublist in array for item in sublist])), tags))\n","\n","  # Replace elements in the array using list comprehension\n","  return [[tag_dict[num] for num in row] for row in array]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["label_list = dataset[\"train\"].features[f\"ner_tags\"].feature.names\n","print(label_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tags = replace_numbers_with_tags(tags, label_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"sentence: {sentences[0]}\")\n","print(f\"tags: {tags[0]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["char_set = set()\n","for sentence in sentences:\n","    for word in sentence:\n","        for char in word:\n","            char_set.add(char)\n","char_set = list(char_set)\n","print(f\"char_set: {char_set}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def combine_lists_elementwise(list_A, list_B):\n","  \"\"\"\n","  Combines two 2D lists of strings element-wise into a 2D list of tuples.\n","\n","  Args:\n","      list_A: A 2D list of strings (e.g., [['A', 'A', 'A'], ['A', 'A', 'A']]).\n","      list_B: Another 2D list of strings with the same dimensions as list_A.\n","\n","  Returns:\n","      A 2D list of tuples, where each tuple combines corresponding elements from list_A and list_B.\n","\n","  Raises:\n","      ValueError: If the dimensions of list_A and list_B don't match.\n","  \"\"\"\n","\n","  # Check if dimensions match\n","  if len(list_A) != len(list_B) or len(list_A[0]) != len(list_B[0]):\n","    raise ValueError(\"Dimensions of lists A and B must be equal.\")\n","\n","  # Create the resulting list using list comprehension\n","  return [[(a, b) for a, b in zip(row_a, row_b)] for row_a, row_b in zip(list_A, list_B)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer = nltk.tag.hmm.HiddenMarkovModelTrainer(states=label_list, symbols=char_set)\n","data = combine_lists_elementwise(sentences.copy(), tags.copy())\n","print(data[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = trainer.train_supervised(data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import dill\n","\n","# Assuming you have trained your model (trainer, model, sentences, etc.)\n","\n","# Open a file for writing in binary mode\n","with open('hmm_model.dill', 'wb') as f:\n","  # Dill can handle more complex objects than pickle\n","  dill.dump(model, f)\n","\n","print(\"Model saved as hmm_model.dill\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Open the saved model file in binary read mode\n","with open('hmm_model.dill', 'rb') as f:\n","  # Load the model back into a variable using dill.load\n","  model = dill.load(f)\n","\n","print(\"Model loaded successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_sentences = dataset[\"test\"][:][\"tokens\"]\n","correct_tags = dataset[\"test\"][:][\"ner_tags\"]\n","correct_tags = replace_numbers_with_tags(correct_tags, label_list)\n","predicted = []\n","for sentence in test_sentences:\n","    test_result = model.tag(sentence)\n","    out_tags = []\n","    for word, tag in test_result:\n","        out_tags.append(tag)\n","    predicted.append(out_tags)\n","predicted[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["correct_tags = [item for sublist in correct_tags for item in sublist]\n","predicted = [item for sublist in predicted for item in sublist]\n","print(f\"number of predictions: {len(predicted)}\\nnumber of correct answers: {len(correct_tags)}\")\n","\n","print(correct_tags[:100])\n","print(predicted[:100])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["precision, recall, f1, support = precision_recall_fscore_support(correct_tags, predicted, labels = label_list)\n","print(f\"precision: {precision}\\nrecall: {recall}\\nf1-score: {f1}\\nsupport: {support}\")"]},{"cell_type":"markdown","metadata":{},"source":["## BERT\n","\n","The following is the implementation of BERT model"]},{"cell_type":"markdown","metadata":{},"source":["### Tokenizer"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:30:15.947484Z","iopub.status.busy":"2024-03-25T14:30:15.946575Z","iopub.status.idle":"2024-03-25T14:30:19.245567Z","shell.execute_reply":"2024-03-25T14:30:19.244742Z","shell.execute_reply.started":"2024-03-25T14:30:15.947451Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ad4b4998f364ff7a989439466f50a05","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c20f4186f56467eb19eb346c9454aa0","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f108755f0afd4987a93febd137e8f63d","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"443126261f6c4bcd8b67acdd14cd2ef9","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer\n","import transformers\n","model_checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True) # use AutoTokenizer because it defaults to fast tokenizers where as using the BERT Tokenizer does not\n","assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"]},{"cell_type":"markdown","metadata":{},"source":["#### Quick Example"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:30:21.666430Z","iopub.status.busy":"2024-03-25T14:30:21.665296Z","iopub.status.idle":"2024-03-25T14:30:21.675372Z","shell.execute_reply":"2024-03-25T14:30:21.674388Z","shell.execute_reply.started":"2024-03-25T14:30:21.666375Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['[CLS]', 'alternatively', ',', 'fi', '##bro', '##bla', '##sts', 'were', 'plate', '##d', 'sparsely', 'so', 'that', 'they', 'did', 'not', 'touch', 'each', 'other', 'and', 'induced', 'into', 'qui', '##escence', 'by', 'serum', 'starvation', 'and', 'monitored', 'after', '4', 'd', '(', 'serum', '-', 'starved', 'for', '4', 'd', '[', 'ss', '##4', ']', ')', 'or', '7', 'd', '(', 'serum', '-', 'starved', 'for', '7', 'd', '[', 'ss', '##7', ']', ')', '.', '[SEP]']\n"]}],"source":["# print an example tokenized text\n","example = train[0]\n","tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n","tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n","print(tokens)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:30:23.745220Z","iopub.status.busy":"2024-03-25T14:30:23.744350Z","iopub.status.idle":"2024-03-25T14:30:23.751594Z","shell.execute_reply":"2024-03-25T14:30:23.750593Z","shell.execute_reply.started":"2024-03-25T14:30:23.745190Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[None, 0, 1, 2, 2, 2, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 48, 49, 50, 51, None]\n","61 61\n"]}],"source":["# check the length of tokens is the same as in the dataset sample\n","len(example[f\"{task}_tags\"]), len(tokenized_input[\"input_ids\"])\n","print(tokenized_input.word_ids())\n","\n","word_ids = tokenized_input.word_ids()\n","aligned_labels = [-100 if i is None else example[f\"{task}_tags\"][i] for i in word_ids]\n","print(len(aligned_labels), len(tokenized_input[\"input_ids\"])) # if it prints the same number twice, then everything is working"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:30:28.935625Z","iopub.status.busy":"2024-03-25T14:30:28.934643Z","iopub.status.idle":"2024-03-25T14:30:28.943306Z","shell.execute_reply":"2024-03-25T14:30:28.942270Z","shell.execute_reply.started":"2024-03-25T14:30:28.935590Z"},"trusted":true},"outputs":[],"source":["label_all_tokens = True\n","def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True) ## For some models, you may need to set max_length to approximately 500.\n","\n","    labels = []\n","    for i, label in enumerate(examples[f\"{task}_tags\"]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:\n","            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n","            # ignored in the loss function.\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            # We set the label for the first token of each word.\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(label[word_idx])\n","            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n","            # the label_all_tokens flag.\n","            else:\n","                label_ids.append(label[word_idx] if label_all_tokens else -100)\n","            previous_word_idx = word_idx\n","\n","        labels.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:30:31.278257Z","iopub.status.busy":"2024-03-25T14:30:31.277363Z","iopub.status.idle":"2024-03-25T14:31:53.743889Z","shell.execute_reply":"2024-03-25T14:31:53.743102Z","shell.execute_reply.started":"2024-03-25T14:30:31.278224Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-25 14:30:35.107266: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-25 14:30:35.107379: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-25 14:30:35.235482: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23be2b642ae64547a5838378ca647831","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/113 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82c52483e8614231b3b60f10439a8114","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/25 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eec666e7b75f4f5d887805e522ece48e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/25 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f8107d8063545e4900f128151d1516a","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n","tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n","model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:32:34.184676Z","iopub.status.busy":"2024-03-25T14:32:34.184259Z","iopub.status.idle":"2024-03-25T14:32:34.216578Z","shell.execute_reply":"2024-03-25T14:32:34.215563Z","shell.execute_reply.started":"2024-03-25T14:32:34.184644Z"},"trusted":true},"outputs":[],"source":["model_name = model_checkpoint.split(\"/\")[-1]\n","batch_size = 16\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 1000,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=4,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=1000,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=['none'],\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:32:47.276898Z","iopub.status.busy":"2024-03-25T14:32:47.276519Z","iopub.status.idle":"2024-03-25T14:32:47.833368Z","shell.execute_reply":"2024-03-25T14:32:47.832481Z","shell.execute_reply.started":"2024-03-25T14:32:47.276868Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e635b9e8fc5d4c95ac943a00bfaf22a3","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'AC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n"," 'LF': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n"," 'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 38},\n"," 'overall_precision': 1.0,\n"," 'overall_recall': 1.0,\n"," 'overall_f1': 1.0,\n"," 'overall_accuracy': 1.0}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import DataCollatorForTokenClassification\n","from datasets import load_metric\n","data_collator = DataCollatorForTokenClassification(tokenizer)\n","metric = load_metric(\"seqeval\")\n","labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n","metric.compute(predictions=[labels], references=[labels])"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:32:51.230189Z","iopub.status.busy":"2024-03-25T14:32:51.229245Z","iopub.status.idle":"2024-03-25T14:32:51.240366Z","shell.execute_reply":"2024-03-25T14:32:51.239157Z","shell.execute_reply.started":"2024-03-25T14:32:51.230146Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:32:53.604821Z","iopub.status.busy":"2024-03-25T14:32:53.603833Z","iopub.status.idle":"2024-03-25T14:32:53.878274Z","shell.execute_reply":"2024-03-25T14:32:53.877448Z","shell.execute_reply.started":"2024-03-25T14:32:53.604777Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]}],"source":["from transformers import EarlyStoppingCallback\n","BERTtrainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:32:59.365176Z","iopub.status.busy":"2024-03-25T14:32:59.364737Z","iopub.status.idle":"2024-03-25T14:32:59.686606Z","shell.execute_reply":"2024-03-25T14:32:59.685582Z","shell.execute_reply.started":"2024-03-25T14:32:59.365141Z"},"trusted":true},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T14:33:01.883067Z","iopub.status.busy":"2024-03-25T14:33:01.882678Z","iopub.status.idle":"2024-03-25T15:30:47.313965Z","shell.execute_reply":"2024-03-25T15:30:47.313114Z","shell.execute_reply.started":"2024-03-25T14:33:01.883037Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='7041' max='7041' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7041/7041 57:44, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.195400</td>\n","      <td>0.193066</td>\n","      <td>0.939140</td>\n","      <td>0.920015</td>\n","      <td>0.929479</td>\n","      <td>0.923688</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.183400</td>\n","      <td>0.168574</td>\n","      <td>0.944362</td>\n","      <td>0.929060</td>\n","      <td>0.936648</td>\n","      <td>0.931450</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.159300</td>\n","      <td>0.158927</td>\n","      <td>0.947850</td>\n","      <td>0.930396</td>\n","      <td>0.939042</td>\n","      <td>0.933631</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.165900</td>\n","      <td>0.150122</td>\n","      <td>0.947932</td>\n","      <td>0.934383</td>\n","      <td>0.941109</td>\n","      <td>0.936172</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.157900</td>\n","      <td>0.146667</td>\n","      <td>0.949776</td>\n","      <td>0.936980</td>\n","      <td>0.943335</td>\n","      <td>0.938631</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.148900</td>\n","      <td>0.141664</td>\n","      <td>0.950283</td>\n","      <td>0.939457</td>\n","      <td>0.944839</td>\n","      <td>0.940253</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.151000</td>\n","      <td>0.140667</td>\n","      <td>0.951054</td>\n","      <td>0.939505</td>\n","      <td>0.945244</td>\n","      <td>0.940679</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=7041, training_loss=0.17479819706972477, metrics={'train_runtime': 3465.0789, 'train_samples_per_second': 32.511, 'train_steps_per_second': 2.032, 'total_flos': 9829844359441080.0, 'train_loss': 0.17479819706972477, 'epoch': 1.0})"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["BERTtrainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T15:42:28.418580Z","iopub.status.busy":"2024-03-25T15:42:28.417763Z","iopub.status.idle":"2024-03-25T15:45:55.185419Z","shell.execute_reply":"2024-03-25T15:45:55.184449Z","shell.execute_reply.started":"2024-03-25T15:42:28.418544Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6035' max='6035' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6035/6035 02:07]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 0.1406669169664383,\n"," 'eval_precision': 0.9510539780222387,\n"," 'eval_recall': 0.9395051506239975,\n"," 'eval_f1': 0.9452442902572998,\n"," 'eval_accuracy': 0.9406789553624619,\n"," 'eval_runtime': 206.7355,\n"," 'eval_samples_per_second': 116.768,\n"," 'eval_steps_per_second': 29.192,\n"," 'epoch': 1.0}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["BERTtrainer.evaluate() ## Final Evaluation on the validation set"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment 2 (Tokenization)"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment 3 (Loss Functions)"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment 4 (Hyperparameters)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
