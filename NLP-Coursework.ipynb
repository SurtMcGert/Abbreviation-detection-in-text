{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"code","source":"# Install dependencies\n%pip install -q -U ipywidgets transformers tqdm\n%pip install -q -U seqeval\n%pip install -q -U accelerate\n%pip install -q -U transformers[torch]\n%pip install -q --upgrade -U torch torchvision torchaudio torchtext\n%pip install -q dill==0.3.1.1\n%pip install -q numpy==1.14.3\n%pip install -q pyarrow==0.3.8\n%pip install -q multiprocess==0.70.16\n%pip install -q -U datasets==2.6.0\n%pip install -q fsspec==2023.9.2\n%pip install -q optuna","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datasets\nimport torch\nimport torchtext\nfrom datasets import load_dataset, Features, Value\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport os\nimport nltk\nimport subprocess\nfrom sklearn.metrics import classification_report\nfrom transformers import AutoTokenizer\nimport transformers\nfrom transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\nimport dill\nfrom transformers import DataCollatorForTokenClassification\nfrom datasets import load_metric\nimport numpy as np\nimport gc\nimport torch.nn as nn\nfrom datasets import DatasetDict, Dataset\nfrom transformers import BertForTokenClassification\nfrom transformers import BertTokenizerFast\nfrom optuna import create_study, Trial\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T10:57:08.362912Z","iopub.execute_input":"2024-04-22T10:57:08.363765Z","iopub.status.idle":"2024-04-22T10:57:26.484562Z","shell.execute_reply.started":"2024-04-22T10:57:08.363727Z","shell.execute_reply":"2024-04-22T10:57:26.483524Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-04-22 10:57:18.581764: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-22 10:57:18.581895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-22 10:57:18.692732: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Set Seed and CUDA","metadata":{}},{"cell_type":"code","source":"print(datasets.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T10:59:56.864213Z","iopub.execute_input":"2024-04-22T10:59:56.864932Z","iopub.status.idle":"2024-04-22T10:59:56.869630Z","shell.execute_reply.started":"2024-04-22T10:59:56.864898Z","shell.execute_reply":"2024-04-22T10:59:56.868683Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"2.6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"SEED = 1234\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nprint(\"PyTorch Version: \", torch.__version__)\nprint(\"torchtext Version: \", torchtext.__version__)\nprint(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T10:59:58.933039Z","iopub.execute_input":"2024-04-22T10:59:58.933976Z","iopub.status.idle":"2024-04-22T10:59:58.941855Z","shell.execute_reply.started":"2024-04-22T10:59:58.933942Z","shell.execute_reply":"2024-04-22T10:59:58.940766Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"PyTorch Version:  2.2.2+cu121\ntorchtext Version:  0.17.2+cpu\nUsing GPU.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Prep","metadata":{}},{"cell_type":"markdown","source":"### Download the Dataset\nthis will download the huggingface dataset ready for use","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"surrey-nlp/PLOD-CW\", cache_dir=None, download_mode=\"force_redownload\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:00:01.405575Z","iopub.execute_input":"2024-04-22T11:00:01.405932Z","iopub.status.idle":"2024-04-22T11:01:48.334013Z","shell.execute_reply.started":"2024-04-22T11:00:01.405905Z","shell.execute_reply":"2024-04-22T11:01:48.333091Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/8.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"716a03ebb2264149b43fa04230b6dc9a"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/surrey-nlp___parquet/surrey-nlp--PLOD-CW-843ef47e3e665cc1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e448c3425e124ce99bbc7cfdd07302c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/188k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1416e710de07454b9146a5d4dd58a9a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/28.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70b37c6b73354e19a7d306c9cb3072ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/28.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4b77cd1dc064392b793562461330ad3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"348cf88776e8412bbb16fa016915c8fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 tables [00:00, ? tables/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e49c6ce9d2542489be6d955e480ab98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 tables [00:00, ? tables/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbe89a86435f457abbd554c3eab6354d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 tables [00:00, ? tables/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c478d2a3ccd64b478ee05f10055f8253"}},"metadata":{}},{"name":"stdout","text":"Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/surrey-nlp___parquet/surrey-nlp--PLOD-CW-843ef47e3e665cc1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dffdd7549bd54060bf8b826dc600ceed"}},"metadata":{}}]},{"cell_type":"code","source":"print(type(dataset))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:03:42.695261Z","iopub.execute_input":"2024-04-22T11:03:42.696105Z","iopub.status.idle":"2024-04-22T11:03:42.701028Z","shell.execute_reply.started":"2024-04-22T11:03:42.696068Z","shell.execute_reply":"2024-04-22T11:03:42.700043Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<class 'datasets.dataset_dict.DatasetDict'>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Get Label List\nthis gets the list of labels for the dataset","metadata":{}},{"cell_type":"code","source":"label_list = ['B-O', 'B-AC', 'B-LF', 'I-LF']\nprint(label_list)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:03:45.231159Z","iopub.execute_input":"2024-04-22T11:03:45.231827Z","iopub.status.idle":"2024-04-22T11:03:45.236939Z","shell.execute_reply.started":"2024-04-22T11:03:45.231795Z","shell.execute_reply":"2024-04-22T11:03:45.235930Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"['B-O', 'B-AC', 'B-LF', 'I-LF']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Split The Set Into Train, Val and Test Sets","metadata":{}},{"cell_type":"code","source":"train = dataset['train']\nprint(f\"train size: {len(train)}\")\nval = dataset['validation']\nprint(f\"val size: {len(val)}\")\ntest = dataset['test']\nprint(f\"test size: {len(test)}\")\n\ndef flatten(A):\n    rt = []\n    for i in A:\n        if isinstance(i,list): rt.extend(flatten(i))\n        else: rt.append(i)\n    return rt\n\nfrom collections import Counter\nflat = flatten(train[\"ner_tags\"])\nprint(Counter(flat))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:03:47.195391Z","iopub.execute_input":"2024-04-22T11:03:47.196519Z","iopub.status.idle":"2024-04-22T11:03:47.277073Z","shell.execute_reply.started":"2024-04-22T11:03:47.196484Z","shell.execute_reply":"2024-04-22T11:03:47.276102Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"train size: 1072\nval size: 126\ntest size: 153\nCounter({'B-O': 32971, 'I-LF': 3231, 'B-AC': 2336, 'B-LF': 1462})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data visualization\nHere I visualize the dataset to be used for this course work and analyse its features","metadata":{}},{"cell_type":"code","source":"def analyze_nlp_dataset(data, output_folder):\n  \"\"\"\n  Analyzes and visualizes an NLP dataset with tokens, POS tags, and NER tags.\n\n  Args:\n      data: A dictionary containing separate lists for tokens, POS tags, and NER tags.\n          - data[\"tokens\"]: A list of lists of tokens.\n          - data[\"pos_tags\"]: A list of lists of POS tags.\n          - data[\"ner_tags\"]: A list of lists of NER tags.\n      output_folder: The folder path to save generated plots.\n  \"\"\"\n\n  try:\n    os.mkdir(output_folder)\n  except FileExistsError:\n    pass  # Folder already exists, continue\n\n  # POS Tag Analysis\n  all_pos_tags = [pos_tag for row in data[\"pos_tags\"] for pos_tag in row]\n  pos_tag_counts = Counter(all_pos_tags)\n\n  # Plot POS tag distribution\n  plt.figure(figsize=(8, 6))\n  plt.pie(pos_tag_counts.values(), labels=pos_tag_counts.keys(), autopct=\"%1.1f%%\")\n  plt.title(\"POS Tag Distribution\")\n  plt.savefig(f\"{output_folder}/pos_tag_distribution.png\")\n  plt.close()\n\n  # NER Tag Analysis\n  all_ner_tags = [ner_tag for row in data[\"ner_tags\"] for ner_tag in row]\n  ner_tag_counts = Counter(all_ner_tags)\n\n  # Plot NER tag distribution (if any named entities exist)\n  if ner_tag_counts:\n    plt.figure(figsize=(8, 6))\n    plt.bar(ner_tag_counts.keys(), ner_tag_counts.values())\n    plt.xlabel(\"NER Tag\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"NER Tag Distribution\")\n    plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels for better readability\n    plt.tight_layout()\n    plt.savefig(f\"{output_folder}/ner_tag_distribution.png\")\n    plt.close()\n  else:\n    print(\"No named entity tags found in the data for NER tag analysis.\")\n\n  # Analysis of POS tags within NER tags\n  pos_in_ner_tags = {}\n  for tokens, pos_tags, ner_tags in zip(data[\"tokens\"], data[\"pos_tags\"], data[\"ner_tags\"]):\n    for token, pos_tag, ner_tag in zip(tokens, pos_tags, ner_tags):\n      if ner_tag and ner_tag != \"O\":  # Consider only named entity tags (excluding \"O\")\n        pos_in_ner_tags.setdefault(ner_tag, []).append(pos_tag)\n\n  # Calculate POS tag proportions within each NER tag (if data exists)\n  if pos_in_ner_tags:\n    for ner_tag, pos_tag_list in pos_in_ner_tags.items():\n      pos_tag_counts_in_ner = Counter(pos_tag_list)\n      total_count = sum(pos_tag_counts_in_ner.values())\n      pos_in_ner_tags[ner_tag] = {tag: count / total_count for tag, count in pos_tag_counts_in_ner.items()}\n\n  # Print insights from POS tags within NER tags analysis (optional)\n  if pos_in_ner_tags:\n    print(\"\\nInsights from POS tags within NER tags:\")\n    for ner_tag, pos_tag_proportions in pos_in_ner_tags.items():\n      print(f\"- NER Tag: {ner_tag}\")\n      for pos_tag, proportion in pos_tag_proportions.items():\n        print(f\"  - Proportion of {pos_tag}: {proportion:.2f}\")\n\n\n    # Visualize POS tags within NER tags (if data exists)\n  if pos_in_ner_tags:\n    for ner_tag, pos_tag_proportions in pos_in_ner_tags.items():\n      plt.figure(figsize=(8, 6))\n      plt.bar(pos_tag_proportions.keys(), pos_tag_proportions.values())\n      plt.xlabel(\"POS Tag\")\n      plt.ylabel(\"Proportion\")\n      plt.title(f\"POS Tag Proportions within NER Tag: {ner_tag}\")\n      plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels for better readability\n      plt.tight_layout()\n      plt.savefig(f\"{output_folder}/pos_in_ner_{ner_tag}.png\")\n      plt.close()\n\n  print(\"Analysis complete. Plots saved to\", output_folder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analyze_nlp_dataset(train, \"train_set_analysis\")\nanalyze_nlp_dataset(val, \"val_set_analysis\")\nanalyze_nlp_dataset(test, \"test_set_analysis\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Pre-Processing","metadata":{}},{"cell_type":"markdown","source":"### Lemmatization","metadata":{}},{"cell_type":"code","source":"\nis_kaggle = (\n    \"KAGGLE_CLOUD\" in os.environ or \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n)\n\nif is_kaggle:\n    # Download and unzip wordnet\n    try:\n        nltk.data.find('wordnet.zip')\n    except:\n        nltk.download('wordnet', download_dir='/kaggle/working/')\n        command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n        subprocess.run(command.split())\n        nltk.data.path.append('/kaggle/working/')\n\n    # Now you can import the NLTK resources as usual\n    from nltk.corpus import wordnet\nelse:\n    nltk.download('wordnet')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:03:53.851030Z","iopub.execute_input":"2024-04-22T11:03:53.851973Z","iopub.status.idle":"2024-04-22T11:03:54.245441Z","shell.execute_reply.started":"2024-04-22T11:03:53.851939Z","shell.execute_reply":"2024-04-22T11:03:54.244294Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /kaggle/working/...\nArchive:  /kaggle/working/corpora/wordnet.zip\n   creating: /kaggle/working/corpora/wordnet/\n  inflating: /kaggle/working/corpora/wordnet/lexnames  \n  inflating: /kaggle/working/corpora/wordnet/data.verb  \n  inflating: /kaggle/working/corpora/wordnet/index.adv  \n  inflating: /kaggle/working/corpora/wordnet/adv.exc  \n  inflating: /kaggle/working/corpora/wordnet/index.verb  \n  inflating: /kaggle/working/corpora/wordnet/cntlist.rev  \n  inflating: /kaggle/working/corpora/wordnet/data.adj  \n  inflating: /kaggle/working/corpora/wordnet/index.adj  \n  inflating: /kaggle/working/corpora/wordnet/LICENSE  \n  inflating: /kaggle/working/corpora/wordnet/citation.bib  \n  inflating: /kaggle/working/corpora/wordnet/noun.exc  \n  inflating: /kaggle/working/corpora/wordnet/verb.exc  \n  inflating: /kaggle/working/corpora/wordnet/README  \n  inflating: /kaggle/working/corpora/wordnet/index.sense  \n  inflating: /kaggle/working/corpora/wordnet/data.noun  \n  inflating: /kaggle/working/corpora/wordnet/data.adv  \n  inflating: /kaggle/working/corpora/wordnet/index.noun  \n  inflating: /kaggle/working/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"the `combine_lists_elementwise` function turns two lists into one, pairing each element elementwise, maintaining the shape of the original list","metadata":{}},{"cell_type":"code","source":"def combine_lists_elementwise(list_A, list_B):\n  \"\"\"\n  Combines two 2D lists of strings element-wise into a 2D list of tuples.\n\n  Args:\n      list_A: A 2D list of strings (e.g., [['A', 'A', 'A'], ['A', 'A', 'A']]).\n      list_B: Another 2D list of strings with the same dimensions as list_A.\n\n  Returns:\n      A 2D list of tuples, where each tuple combines corresponding elements from list_A and list_B.\n\n  Raises:\n      ValueError: If the dimensions of list_A and list_B don't match.\n  \"\"\"\n\n  # Check if dimensions match\n  if len(list_A) != len(list_B) or len(list_A[0]) != len(list_B[0]):\n    raise ValueError(\"Dimensions of lists A and B must be equal.\")\n\n  # Create the resulting list using list comprehension\n  return [[(a, b) for a, b in zip(row_a, row_b)] for row_a, row_b in zip(list_A, list_B)]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:03:58.910187Z","iopub.execute_input":"2024-04-22T11:03:58.911131Z","iopub.status.idle":"2024-04-22T11:03:58.917845Z","shell.execute_reply.started":"2024-04-22T11:03:58.911093Z","shell.execute_reply":"2024-04-22T11:03:58.916763Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"the nltk lemmatize function takes a certain format for POS_tags so the `convert_pos_tag` maps a POS_tag from the dataset, to one in the required format. Its important to note that alot of data is lost due to the simplicity of the nltk lemmatize function","metadata":{}},{"cell_type":"code","source":"def convert_pos_tag(nltk_tag):\n    \"\"\"\n    Converts NLTK POS tags to the format expected by the lemmatizer.\n\n    Args:\n        nltk_tag: The POS tag in NLTK format (e.g., VBG, NNS).\n\n    Returns:\n        The corresponding POS tag for the lemmatizer (n, v, a, r, or s) or None if no match.\n    \"\"\"\n\n    tag_map = {\n        'NUM': '',  # Number (not handled by lemmatizer)\n        'CCONJ': '',  # Coordinating conjunction (not handled)\n        'PRON': '',  # Pronoun (not handled)\n        'NOUN': 'n',   # Noun\n        'SCONJ': '',  # Subordinating conjunction (not handled)\n        'SYM': '',   # Symbol (not handled)\n        'INTJ': '',  # Interjection (not handled)\n        'ADJ': 'a',    # Adjective\n        'ADP': '',   # Preposition (not handled)\n        'PUNCT': '',  # Punctuation (not handled)\n        'ADV': 'r',    # Adverb\n        'AUX': 'v',    # Auxiliary verb\n        'DET': '',   # Determiner (not handled)\n        'VERB': 'v',   # Verb\n        'X': '',      # Other (not handled)\n        'PART': '',   # Particle (not handled)\n        'PROPN': 'n',   # Proper noun\n    }\n    return tag_map.get(nltk_tag)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:04:00.573914Z","iopub.execute_input":"2024-04-22T11:04:00.574287Z","iopub.status.idle":"2024-04-22T11:04:00.581302Z","shell.execute_reply.started":"2024-04-22T11:04:00.574261Z","shell.execute_reply":"2024-04-22T11:04:00.580256Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"the `lemmatize_list` function takes the tokens and their respective pos_tags and lemmatizes the tokens","metadata":{}},{"cell_type":"code","source":"def lemmatize_list(data, pos_tags):\n    \"\"\"\n    Lemmatizes a 2D list of tokens using NLTK.\n\n    Args:\n        data: A 2D list of strings (tokens) to be lemmatized.\n\n    Returns:\n        A 2D list containing the lemmatized tokens.\n    \"\"\"\n\n    # Initialize the WordNet lemmatizer\n    lemmatizer = nltk.WordNetLemmatizer()\n\n    pos_tags = [[convert_pos_tag(tag) for tag in row] for row in pos_tags]\n\n    data = combine_lists_elementwise(data, pos_tags)\n\n\n    # Lemmatize with part-of-speech information\n    lemmatized_data = [[token if pos == '' else lemmatizer.lemmatize(token, pos) for token, pos in row] for row in data]\n\n    return lemmatized_data","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:04:02.500473Z","iopub.execute_input":"2024-04-22T11:04:02.501255Z","iopub.status.idle":"2024-04-22T11:04:02.507499Z","shell.execute_reply.started":"2024-04-22T11:04:02.501222Z","shell.execute_reply":"2024-04-22T11:04:02.506469Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Pre-Processing Pipeline\nthe `pre_process_data` function applies lemmatization and lowercase to the given data","metadata":{}},{"cell_type":"code","source":"def pre_process_data(tokens, pos_tags):\n    # lemmatize the data\n    data = lemmatize_list(tokens, pos_tags)\n    # lowercase the data\n    data = [[string.lower() for string in row] for row in data]\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:04:14.200310Z","iopub.execute_input":"2024-04-22T11:04:14.200954Z","iopub.status.idle":"2024-04-22T11:04:14.206445Z","shell.execute_reply.started":"2024-04-22T11:04:14.200923Z","shell.execute_reply":"2024-04-22T11:04:14.205105Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from datasets import DatasetDict, Dataset\ntrain_tokens = pre_process_data(train[\"tokens\"], train[\"pos_tags\"])\nval_tokens = pre_process_data(val[\"tokens\"], val[\"pos_tags\"])\ntest_tokens = pre_process_data(test[\"tokens\"], test[\"pos_tags\"])\noriginal_train_tokens = train[\"tokens\"]\noriginal_val_tokens = val[\"tokens\"]\nprint(f\"original train tokens: {original_train_tokens[0]}\\npre-processed train tokens: {train_tokens[0]}\")\nprint(f\"original val tokens: {original_val_tokens[0]}\\npre-processed val tokens: {val_tokens[0]}\")\n\ndataset = DatasetDict({\n    \"train\": Dataset.from_dict({\"tokens\": train_tokens, \"pos_tags\": train[\"pos_tags\"], \"ner_tags\": train[\"ner_tags\"]}),\n    \"validation\": Dataset.from_dict({\"tokens\": val_tokens, \"pos_tags\": val[\"pos_tags\"], \"ner_tags\": val[\"ner_tags\"]}),\n    \"test\": Dataset.from_dict({\"tokens\": test_tokens, \"pos_tags\": test[\"pos_tags\"], \"ner_tags\": test[\"ner_tags\"]}),\n})","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:04:16.770829Z","iopub.execute_input":"2024-04-22T11:04:16.771525Z","iopub.status.idle":"2024-04-22T11:04:19.449554Z","shell.execute_reply.started":"2024-04-22T11:04:16.771491Z","shell.execute_reply":"2024-04-22T11:04:19.448625Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"original train tokens: ['For', 'this', 'purpose', 'the', 'Gothenburg', 'Young', 'Persons', 'Empowerment', 'Scale', '(', 'GYPES', ')', 'was', 'developed', '.']\npre-processed train tokens: ['for', 'this', 'purpose', 'the', 'gothenburg', 'young', 'persons', 'empowerment', 'scale', '(', 'gypes', ')', 'be', 'develop', '.']\noriginal val tokens: ['=', 'Manual', 'Ability', 'Classification', 'System', ';', 'QUEST', '=', 'Quest', '-', 'Quality', 'of', 'upper', 'extremity', 'skills', 'test', ';', 'Cont', '=', 'control', ';', 'M', '=', 'male', ',', 'F', '=', 'female', ',', 'V', '=', 'verbal', ',', 'nonV', '=', 'non', '-', 'Verbal', ',', '|Quad', '=', 'quadriplegia', ',', 'Di', '=', 'Diplegia', ',', 'Hemi', '=', 'hemiplegia', '.']\npre-processed val tokens: ['=', 'manual', 'ability', 'classification', 'system', ';', 'quest', '=', 'quest', '-', 'quality', 'of', 'upper', 'extremity', 'skill', 'test', ';', 'cont', '=', 'control', ';', 'm', '=', 'male', ',', 'f', '=', 'female', ',', 'v', '=', 'verbal', ',', 'nonv', '=', 'non', '-', 'verbal', ',', '|quad', '=', 'quadriplegia', ',', 'di', '=', 'diplegia', ',', 'hemi', '=', 'hemiplegia', '.']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Set Task\nin this project we are doing Named Entity Recognition so I set the task to \"ner\"","metadata":{}},{"cell_type":"code","source":"task = \"ner\"","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:04:21.397378Z","iopub.execute_input":"2024-04-22T11:04:21.398223Z","iopub.status.idle":"2024-04-22T11:04:21.402627Z","shell.execute_reply.started":"2024-04-22T11:04:21.398189Z","shell.execute_reply":"2024-04-22T11:04:21.401562Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Experiment 1 (Model)\nHMM vs BERT","metadata":{}},{"cell_type":"markdown","source":"## HMM\nThe following is the implementation of an HMM model","metadata":{}},{"cell_type":"markdown","source":"### Library Import\nI am using the nltk library for the HMM implementation","metadata":{}},{"cell_type":"markdown","source":"create lists of the sentences and associated tags from the train set","metadata":{}},{"cell_type":"code","source":"# sentences = train[:][\"tokens\"]\n# tags = train[:][\"ner_tags\"]\n\nsentences = train_tokens\ntags = train[:][\"ner_tags\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"print out an example of the first sentence and its tags","metadata":{}},{"cell_type":"code","source":"print(f\"sentence: {sentences[0]}\")\nprint(f\"tags: {tags[0]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we generate a character set containing all the characters that can be used in the output of the model","metadata":{}},{"cell_type":"code","source":"def get_char_set(sentences):\n    char_set = set()\n    for sentence in sentences:\n        for word in sentence:\n            for char in word:\n                char_set.add(char)\n    char_set = list(char_set)\n    return char_set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_set = get_char_set(sentences)\nprint(f\"char_set: {char_set}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = nltk.tag.hmm.HiddenMarkovModelTrainer(states=label_list, symbols=char_set)\ndata = combine_lists_elementwise(sentences.copy(), tags.copy())\nprint(data[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERT\n\nThe following is the implementation of BERT model","metadata":{}},{"cell_type":"markdown","source":"### Tokenizer","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True) # use AutoTokenizer because it defaults to fast tokenizers where as using the BERT Tokenizer does not\nassert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:04:25.569531Z","iopub.execute_input":"2024-04-22T11:04:25.570188Z","iopub.status.idle":"2024-04-22T11:05:00.653321Z","shell.execute_reply.started":"2024-04-22T11:04:25.570157Z","shell.execute_reply":"2024-04-22T11:05:00.652414Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9af9f513791413ab2ab24d871e5eb7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32626ec3ad2443c6b1c950de9a2ec058"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b05a7734ede42e4a957b06d6adcbf7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0767316da9224d80b1b73b69159db78e"}},"metadata":{}}]},{"cell_type":"markdown","source":"I need to map the string tokens to numbers","metadata":{}},{"cell_type":"code","source":"def encode_tags(tag_sequences, possible_tags):\n    \"\"\"\n    Encodes a sequence of string tags into a list of corresponding integer tags.\n\n    Args:\n        tag_sequences: A 2d list of strings representing numerical tags.\n        possible_tags: A list of strings representing the possible textual labels.\n\n    Returns:\n        A list of strings representing the decoded textual tags.\n    \"\"\"\n\n    encoded_tags = [[possible_tags.index(tag) for tag in row] for row in tag_sequences]\n    return encoded_tags","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:05:13.978588Z","iopub.execute_input":"2024-04-22T11:05:13.978972Z","iopub.status.idle":"2024-04-22T11:05:13.985019Z","shell.execute_reply.started":"2024-04-22T11:05:13.978941Z","shell.execute_reply":"2024-04-22T11:05:13.984092Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"label_all_tokens = True\ndef tokenize_and_align_labels(data):\n    tokenized_inputs = tokenizer(data[\"tokens\"], truncation=True, is_split_into_words=True) ## For some models, you may need to set max_length to approximately 500.\n\n    labels = []\n    converted_tags = encode_tags(data[f\"{task}_tags\"], label_list)\n    for i, label in enumerate(converted_tags):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n            # ignored in the loss function.\n            if word_idx is None:\n                label_ids.append(-100)\n            # We set the label for the first token of each word.\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n            # the label_all_tokens flag.\n            else:\n                label_ids.append(label[word_idx] if label_all_tokens else -100)\n            previous_word_idx = word_idx\n\n        labels.append(label_ids)\n\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:05:16.377916Z","iopub.execute_input":"2024-04-22T11:05:16.378799Z","iopub.status.idle":"2024-04-22T11:05:16.386375Z","shell.execute_reply.started":"2024-04-22T11:05:16.378763Z","shell.execute_reply":"2024-04-22T11:05:16.385412Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:05:19.254285Z","iopub.execute_input":"2024-04-22T11:05:19.255083Z","iopub.status.idle":"2024-04-22T11:05:39.729806Z","shell.execute_reply.started":"2024-04-22T11:05:19.255025Z","shell.execute_reply":"2024-04-22T11:05:39.728693Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f7adffaf9a94b15b61a37db8df3b118"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f2fc3efaa64459d8ad77af7b0bb980e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"466c1bf66cb246bebbd280891eac899a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c43271f479764b86833130ecec4630e7"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = model_checkpoint.split(\"/\")[-1]\nbatch_size = 16\nargs = TrainingArguments(\n    f\"{model_name}-finetuned-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 100,\n    logging_steps = 100,\n    save_total_limit = 1,\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    weight_decay=0.001,\n    save_steps=1000,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=['none'],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer)\nmetric = load_metric(\"seqeval\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:06:33.550548Z","iopub.execute_input":"2024-04-22T11:06:33.551012Z","iopub.status.idle":"2024-04-22T11:06:34.591845Z","shell.execute_reply.started":"2024-04-22T11:06:33.550980Z","shell.execute_reply":"2024-04-22T11:06:34.591004Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_690/4079666221.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  metric = load_metric(\"seqeval\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d3d59e5e8a24dc3b4e6cf555fa17223"}},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    overall_results = metric.compute(predictions=true_predictions, references=true_labels)\n    \n    true_labels = [item for sublist in true_labels for item in sublist]\n    true_predictions = [item for sublist in true_predictions for item in sublist]\n    \n    \n    results = classification_report(true_labels, true_predictions, labels = label_list)\n    print(results)\n    return {\n        \"precision\": overall_results[\"overall_precision\"],\n        \"recall\": overall_results[\"overall_recall\"],\n        \"f1\": overall_results[\"overall_f1\"],\n        \"accuracy\": overall_results[\"overall_accuracy\"],\n    }","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:06:28.150458Z","iopub.execute_input":"2024-04-22T11:06:28.150887Z","iopub.status.idle":"2024-04-22T11:06:28.160233Z","shell.execute_reply.started":"2024-04-22T11:06:28.150854Z","shell.execute_reply":"2024-04-22T11:06:28.159099Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"BERTtrainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"markdown","source":"#### HMM Training","metadata":{}},{"cell_type":"markdown","source":"set up the hmm trainer and combine the tokens with their tags","metadata":{}},{"cell_type":"markdown","source":"train the model on the data","metadata":{}},{"cell_type":"code","source":"model = trainer.train_supervised(data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"save the model","metadata":{}},{"cell_type":"code","source":"def save_hmm(model, name):\n    # Open a file for writing in binary mode\n    with open(name, 'wb') as f:\n        # Dill can handle more complex objects than pickle\n        dill.dump(model, f)\n\n    print(f\"Model saved as {name}\")\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_hmm(model, \"hmm_model.dill\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"load the model","metadata":{}},{"cell_type":"code","source":"def load_hmm(name):\n    # Open the saved model file in binary read mode\n    with open(name, 'rb') as f:\n        # Load the model back into a variable using dill.load\n        model = dill.load(f)\n        print(\"Model loaded successfully!\")\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_hmm(\"hmm_model.dill\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### BERT Training","metadata":{}},{"cell_type":"markdown","source":"clear the cuda cache to avoid cuda memory issues","metadata":{}},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train bert","metadata":{}},{"cell_type":"code","source":"BERTtrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BERTtrainer.model.save_pretrained(\"/kaggle/working/BERT_save\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BERTtrainer.model.from_pretrained(\"/kaggle/working/BERT_save\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"markdown","source":"#### HMM evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate_hmm(model, test_sentences):\n    predicted = []\n    for sentence in test_sentences:\n        test_result = model.tag(sentence)\n        out_tags = []\n        for word, tag in test_result:\n            out_tags.append(tag)\n        predicted.append(out_tags)\n    return predicted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sentences = dataset[\"validation\"][:][\"tokens\"]\ntest_sentences = pre_process_data(test_sentences, dataset[\"validation\"][:][\"pos_tags\"])\ncorrect_tags = dataset[\"validation\"][:][\"ner_tags\"]\npredicted = evaluate_hmm(model, test_sentences)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correct_tags = [item for sublist in correct_tags for item in sublist]\npredicted = [item for sublist in predicted for item in sublist]\nprint(f\"number of predictions: {len(predicted)}\\nnumber of correct answers: {len(correct_tags)}\")\n\nprint(correct_tags[:100])\nprint(predicted[:100])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(correct_tags, predicted, labels = label_list))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### BERT evaluation","metadata":{}},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(\"model_saves\\\\BERT_save\", num_labels=len(label_list))\nBERTtrainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = BERTtrainer.evaluate()\nprint(metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiment 2 (Loss Functions)\ncross entropy vs MSE\nThis experiment is on BERT as HMM doesnt use loss functions due to its statistical nature rather than being a neural network","metadata":{}},{"cell_type":"markdown","source":"## Creating Custom Trainers","metadata":{}},{"cell_type":"code","source":"# cross entropy\nclass CustomBERTTrainerCrossEntropy(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        if self.label_smoother is not None and \"labels\" in inputs:\n            labels = inputs.pop(\"labels\")\n        else:\n            labels = None\n        outputs = model(**inputs)\n        if self.args.past_index >= 0:\n            self._past = outputs[self.args.past_index]\n\n        if labels is not None:\n            loss = self.label_smoother(outputs, labels)\n        else:\n            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n            loss = loss*loss\n            loss = loss.mean()\n\n        return (loss, outputs) if return_outputs else loss\n    \n    \n# MLSML\nclass CustomBERTTrainerMLSML(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.loss_fn = nn.MultiLabelSoftMarginLoss()\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        if self.label_smoother is not None and \"labels\" in inputs:\n            labels = inputs.pop(\"labels\")\n        else:\n            labels = None\n        outputs = model(**inputs)\n        if self.args.past_index >= 0:\n            self._past = outputs[self.args.past_index]\n\n        if labels is not None:\n            loss = self.label_smoother(outputs, labels)\n        else:\n            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n            loss = loss*loss\n            loss = loss.mean()\n\n        return (loss, outputs) if return_outputs else loss\n    \n    \n# KLDivLoss\nclass CustomBERTTrainerKLDiv(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.loss_fn = nn.KLDivLoss()\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        if self.label_smoother is not None and \"labels\" in inputs:\n            labels = inputs.pop(\"labels\")\n        else:\n            labels = None\n        outputs = model(**inputs)\n        if self.args.past_index >= 0:\n            self._past = outputs[self.args.past_index]\n\n        if labels is not None:\n            loss = self.label_smoother(outputs, labels)\n        else:\n            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n            loss = loss*loss\n            loss = loss.mean()\n\n        return (loss, outputs) if return_outputs else loss\n    \n\n# MSE\nclass CustomBERTTrainerMSE(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.loss_fn = nn.MSELoss()\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        if self.label_smoother is not None and \"labels\" in inputs:\n            labels = inputs.pop(\"labels\")\n        else:\n            labels = None\n        outputs = model(**inputs)\n        if self.args.past_index >= 0:\n            self._past = outputs[self.args.past_index]\n\n        if labels is not None:\n            loss = self.label_smoother(outputs, labels)\n        else:\n            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n            loss = loss*loss\n            loss = loss.mean()\n\n        return (loss, outputs) if return_outputs else loss\n  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args_CE = TrainingArguments(\n    f\"{model_name}-finetuned-CE-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 10,\n    logging_steps = 10,\n    save_total_limit = 1,\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=1,\n    num_train_epochs=5,\n    weight_decay=0.001,\n    save_steps=1000,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=['tensorbaord'],\n)\n\nargs_MLSML = TrainingArguments(\n    f\"{model_name}-finetuned-MLSML-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 10,\n    logging_steps = 10,\n    save_total_limit = 1,\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=1,\n    num_train_epochs=5,\n    weight_decay=0.001,\n    save_steps=1000,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=['tensorbaord'],\n)\n\nargs_KLDiv = TrainingArguments(\n    f\"{model_name}-finetuned-KLDiv-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 10,\n    logging_steps = 10,\n    save_total_limit = 1,\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=1,\n    num_train_epochs=5,\n    weight_decay=0.001,\n    save_steps=1000,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=['tensorbaord'],\n)\n\nargs_MSE = TrainingArguments(\n    f\"{model_name}-finetuned-MSE-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 10,\n    logging_steps = 10,\n    save_total_limit = 1,\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=1,\n    num_train_epochs=5,\n    weight_decay=0.001,\n    save_steps=1000,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=['tensorbaord'],\n)\nmodel_CE = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\nmodel_MLSML = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\nmodel_KLDiv = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\nmodel_MSE = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n\nBERTtrainer_CE = CustomBERTTrainerCrossEntropy(\n    model_CE,\n    args_CE,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_MLSML = CustomBERTTrainerMLSML(\n    model_MLSML,\n    args_MLSML,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_KLDiv = CustomBERTTrainerKLDiv(\n    model_KLDiv,\n    args_KLDiv,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_MSE = CustomBERTTrainerMSE(\n    model_MSE,\n    args_MSE,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BERTtrainer_CE.train()\nBERTtrainer_CE.model.save_pretrained(\"/kaggle/working/BERT_CE_save\")\nBERTtrainer_CE = None\ntorch.cuda.empty_cache()\ngc.collect()\n\nBERTtrainer_MLSML.train()\nBERTtrainer_MLSML.model.save_pretrained(\"/kaggle/working/BERT_MLSML_save\")\nBERTtrainer_MLSML = None\ntorch.cuda.empty_cache()\ngc.collect()\n\nBERTtrainer_KLDiv.train()\nBERTtrainer_KLDiv.model.save_pretrained(\"/kaggle/working/BERT_KLDiv_save\")\nBERTtrainer_KLDiv = None\ntorch.cuda.empty_cache()\ngc.collect()\n\nBERTtrainer_MSE.train()\nBERTtrainer_MSE.model.save_pretrained(\"/kaggle/working/BERT_MSE_save\")\nBERTtrainer_MSE = None\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_CE = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_CE_save\", num_labels=len(label_list))\nmodel_MLSML = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_MLSML_save\", num_labels=len(label_list))\nmodel_KLDiv = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_KLDiv_save\", num_labels=len(label_list))\nmodel_MSE = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_MSE_save\", num_labels=len(label_list))\n\nBERTtrainer_CE = CustomBERTTrainerCrossEntropy(\n    model_CE,\n    args_CE,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_MLSML = CustomBERTTrainerMLSML(\n    model_MLSML,\n    args_MLSML,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_KLDiv = CustomBERTTrainerKLDiv(\n    model_KLDiv,\n    args_KLDiv,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_MSE = CustomBERTTrainerMSE(\n    model_MSE,\n    args_MSE,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"CE_eval = BERTtrainer_CE.evaluate()\nprint(CE_eval)\nMLSML_eval = BERTtrainer_MLSML.evaluate()\nprint(MLSML_eval)\nKLDiv_eval = BERTtrainer_KLDiv.evaluate()\nprint(KLDiv_eval)\nMSE_eval = BERTtrainer_MSE.evaluate()\nprint(MSE_eval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiment 3 (Additional Training Samples from Optional Dataset)","metadata":{}},{"cell_type":"markdown","source":"lemmatization with Bag of Words VS Word2Vec","metadata":{}},{"cell_type":"markdown","source":"## Collecting Dataset","metadata":{}},{"cell_type":"code","source":"filtered_dataset = load_dataset(\"surrey-nlp/PLOD-filtered\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:06:43.862946Z","iopub.execute_input":"2024-04-22T11:06:43.863339Z","iopub.status.idle":"2024-04-22T11:08:44.588017Z","shell.execute_reply.started":"2024-04-22T11:06:43.863306Z","shell.execute_reply":"2024-04-22T11:08:44.587088Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2aa17a10cf134b6a87431623970166f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/7.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b4cff7d72354706a12e621c980e9cb8"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset plod-filtered/PLODfiltered to /root/.cache/huggingface/datasets/surrey-nlp___plod-filtered/PLODfiltered/0.0.2/bd402e453833e246144db398363ec772d4710a494fc4de15e48c7b1ddac6b82b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83db02f7d32543d8b14bdf40c913b7cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/85.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76a05b18c40744ddbd0717c5978d7664"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/18.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0f32e4639584463985b4cdcba73cce0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/18.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"843ff6230a724b4c8341c2ee4bb961b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0671202236d74db3864fc591237da729"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5aa643ac55de43169500947c81acda49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f9e902bb3ce4c39943d2cb6d9e6004a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0a754cb44354a9888b87ac3024ce645"}},"metadata":{}},{"name":"stdout","text":"Dataset plod-filtered downloaded and prepared to /root/.cache/huggingface/datasets/surrey-nlp___plod-filtered/PLODfiltered/0.0.2/bd402e453833e246144db398363ec772d4710a494fc4de15e48c7b1ddac6b82b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"393844a179264492a8527e067595dec8"}},"metadata":{}}]},{"cell_type":"code","source":"filtered_train = filtered_dataset[\"train\"]\nprint(f\"train size: {len(filtered_train)}\")\nfiltered_val = filtered_dataset[\"validation\"]\nprint(f\"val size: {len(filtered_val)}\")\nfiltered_test = filtered_dataset[\"test\"]\nprint(f\"test size: {len(filtered_test)}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:11:10.931366Z","iopub.execute_input":"2024-04-22T11:11:10.932307Z","iopub.status.idle":"2024-04-22T11:11:10.938337Z","shell.execute_reply.started":"2024-04-22T11:11:10.932273Z","shell.execute_reply":"2024-04-22T11:11:10.937188Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"train size: 112652\nval size: 24140\ntest size: 24140\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Extracting Data to Use\nI will have three tests, using three sizes of data acquired from the filtered dataset.\n\nsmall: 1072 extra samples to double the dataset size  \nmedium: 10000 extra samples  \nlarge: 50000 extra samples","metadata":{}},{"cell_type":"code","source":"def decode_tags(tag_sequences, possible_tags):\n    \"\"\"\n    Decodes a sequence of numerical tags into a list of corresponding textual labels.\n\n    Args:\n        tag_sequence: A list of integers representing numerical tags.\n        possible_tags: A list of strings representing the possible textual labels.\n\n    Returns:\n        A list of strings representing the decoded textual tags.\n    \"\"\"\n\n    decoded_tags = [[possible_tags[tag] for tag in row] for row in tag_sequences]\n    return decoded_tags\n\n\ndef build_dataset(filtered_set, cw_set, num_of_samples):\n    \"\"\"\n    Merges a specified number of rows from a larger list to a smaller list, ensuring no duplicates.\n\n    Args:\n        filtered_set: a split of the filtered dataset\n        cw_set: a split of the cw dataset\n        num_of_samples: The number of rows to add from the filtered set.\n\n    Returns:\n        new tokens, pos_tags and ner_tags lists\n    \"\"\"\n    # set up the initial lists\n    tokens = cw_set[\"tokens\"]\n    pos_tags = cw_set[\"pos_tags\"]\n    ner_tags = cw_set[\"ner_tags\"]\n     \n    # set up the filtered lists\n    # tokens\n    filtered_tokens = filtered_set[\"tokens\"]\n    # pos_tags\n    filtered_label_list = filtered_set.features[f\"pos_tags\"].feature.names\n    filtered_pos_tags = decode_tags(filtered_set[\"pos_tags\"], filtered_label_list)\n    # ner_tags\n    filtered_label_list = filtered_set.features[f\"ner_tags\"].feature.names\n    filtered_ner_tags = decode_tags(filtered_set[\"ner_tags\"], filtered_label_list)\n\n    # convert the tokens list to sets for efficient duplicate checking\n    tokens_set = set(tuple(row) for row in tokens)\n    filtered_tokens_set = set(tuple(row) for row in filtered_tokens)\n\n    # find rows to add\n    rows_to_add = []\n    for index, row in enumerate(filtered_tokens_set):\n        if tuple(row) not in tokens_set and len(rows_to_add) < num_of_samples:\n            rows_to_add.append(index)\n\n    # Merge and return the lists\n    tokens = tokens + [filtered_tokens[i] for i in rows_to_add]\n    pos_tags = pos_tags + [filtered_pos_tags[i] for i in rows_to_add]\n    ner_tags = ner_tags + [filtered_ner_tags[i] for i in rows_to_add]\n\n    return tokens, pos_tags, ner_tags\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:11:22.170943Z","iopub.execute_input":"2024-04-22T11:11:22.171865Z","iopub.status.idle":"2024-04-22T11:11:22.182548Z","shell.execute_reply.started":"2024-04-22T11:11:22.171820Z","shell.execute_reply":"2024-04-22T11:11:22.181597Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"small = 1072\nmedium = 10000\nlarge = 50000\ntokens_small, pos_tags_small, ner_tags_small = build_dataset(filtered_train, train, small)\ntokens_small = pre_process_data(tokens_small, pos_tags_small)\ntokens_medium, pos_tags_medium, ner_tags_medium = build_dataset(filtered_train, train, medium)\ntokens_medium = pre_process_data(tokens_medium, pos_tags_medium)\ntokens_large, pos_tags_large, ner_tags_large = build_dataset(filtered_train, train, large)\ntokens_large = pre_process_data(tokens_large, pos_tags_large)\nprint(f\"num of small samples: {len(tokens_small)}\\nnum of medium samples: {len(tokens_medium)}\\nnum of large samples: {len(tokens_large)}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:11:28.421392Z","iopub.execute_input":"2024-04-22T11:11:28.422163Z","iopub.status.idle":"2024-04-22T11:12:35.684269Z","shell.execute_reply.started":"2024-04-22T11:11:28.422131Z","shell.execute_reply":"2024-04-22T11:12:35.683333Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"num of small samples: 2144\nnum of medium samples: 11072\nnum of large samples: 51072\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"markdown","source":"### HMM Training","metadata":{}},{"cell_type":"code","source":"# create the character sets\nchar_set_small = get_char_set(tokens_small)\nchar_set_medium = get_char_set(tokens_medium)\nchar_set_large = get_char_set(tokens_large)\n\n# create trainers\n#small\ntrainer_small = nltk.tag.hmm.HiddenMarkovModelTrainer(states=label_list, symbols=char_set_small)\ndata_small = combine_lists_elementwise(tokens_small.copy(), ner_tags_small.copy())\n# medium\ntrainer_medium = nltk.tag.hmm.HiddenMarkovModelTrainer(states=label_list, symbols=char_set_medium)\ndata_medium = combine_lists_elementwise(tokens_medium.copy(), ner_tags_medium.copy())\n# large\ntrainer_large = nltk.tag.hmm.HiddenMarkovModelTrainer(states=label_list, symbols=char_set_large)\ndata_large = combine_lists_elementwise(tokens_large.copy(), ner_tags_large.copy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_small = trainer_small.train_supervised(data_small)\nmodel_medium = trainer_medium.train_supervised(data_medium)\nmodel_large = trainer_large.train_supervised(data_large)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_hmm(model_small, \"hmm_model_small.dill\")\nsave_hmm(model_medium, \"hmm_model_medium.dill\")\nsave_hmm(model_large, \"hmm_model_large.dill\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_small = load_hmm(\"hmm_model_small.dill\")\nmodel_medium = load_hmm(\"hmm_model_medium.dill\")\nmodel_large = load_hmm(\"hmm_model_large.dill\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BERT Training","metadata":{}},{"cell_type":"code","source":"# create 3 datasets\n\nsmall_datasets_dict = {\n    \"train\": Dataset.from_dict({\"tokens\": tokens_small, \"pos_tags\": pos_tags_small, \"ner_tags\": ner_tags_small})\n}\nmedium_datasets_dict = {\n    \"train\": Dataset.from_dict({\"tokens\": tokens_medium, \"pos_tags\": pos_tags_medium, \"ner_tags\": ner_tags_medium})\n}\nlarge_datasets_dict = {\n    \"train\": Dataset.from_dict({\"tokens\": tokens_large, \"pos_tags\": pos_tags_large, \"ner_tags\": ner_tags_large})\n}\n\nsmall_dataset = DatasetDict(small_datasets_dict)\nmedium_dataset = DatasetDict(medium_datasets_dict)\nlarge_dataset = DatasetDict(large_datasets_dict)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:12:42.603347Z","iopub.execute_input":"2024-04-22T11:12:42.604039Z","iopub.status.idle":"2024-04-22T11:12:43.778557Z","shell.execute_reply.started":"2024-04-22T11:12:42.604004Z","shell.execute_reply":"2024-04-22T11:12:43.777487Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"small_tokenized_dataset = small_dataset.map(tokenize_and_align_labels, batched=True)\nmedium_tokenized_dataset = medium_dataset.map(tokenize_and_align_labels, batched=True)\nlarge_tokenized_dataset = large_dataset.map(tokenize_and_align_labels, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:13:59.828037Z","iopub.execute_input":"2024-04-22T11:13:59.828462Z","iopub.status.idle":"2024-04-22T11:14:31.021982Z","shell.execute_reply.started":"2024-04-22T11:13:59.828433Z","shell.execute_reply":"2024-04-22T11:14:31.020974Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f139b39d1da4307a4f1f53c4fb1d6e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96deb908ed00467697c7d4d61eeab899"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/52 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fabf2db17ff04361a473df5fabc05b4d"}},"metadata":{}}]},{"cell_type":"code","source":"args_small = TrainingArguments(\n    f\"{model_name}-finetuned-small-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 20,\n    logging_steps = 20,\n    save_total_limit = 1,\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    weight_decay=0.001,\n    save_steps=0,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=[\"tensorboard\"],\n)\nargs_medium = TrainingArguments(\n    f\"{model_name}-finetuned-medium-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 50,\n    logging_steps = 50,\n    save_total_limit = 1,\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    weight_decay=0.001,\n    save_steps=0,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=[\"tensorboard\"],\n)\nargs_large = TrainingArguments(\n    f\"{model_name}-finetuned-large-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 300,\n    logging_steps = 300,\n    save_total_limit = 1,\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    weight_decay=0.001,\n    save_steps=0,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=[\"tensorboard\"],\n)\n\nmodel_small = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\nmodel_medium = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\nmodel_large = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n\n\nBERTtrainer_small = Trainer(\n    model_small,\n    args_small,\n    train_dataset=small_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_medium = Trainer(\n    model_medium,\n    args_medium,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_large = Trainer(\n    model_large,\n    args_large,\n    train_dataset=large_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BERTtrainer_small = None\n# BERTtrainer_medium = None\n# BERTtrainer_large = None\n# BERTtrainer = None\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BERTtrainer_small.train()\nBERTtrainer_small.model.save_pretrained(\"/kaggle/working/BERT_small_save\")\nBERTtrainer_small = None\ntorch.cuda.empty_cache()\ngc.collect()\n\nBERTtrainer_medium.train()\nBERTtrainer_medium.model.save_pretrained(\"/kaggle/working/BERT_medium_save\")\nBERTtrainer_medium = None\ntorch.cuda.empty_cache()\ngc.collect()\n\ngc.collect()\ntorch.cuda.empty_cache()\nBERTtrainer_large.train()\nBERTtrainer_large.model.save_pretrained(\"/kaggle/working/BERT_large_save\")\nBERTtrainer_large = None\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## evaluation","metadata":{}},{"cell_type":"markdown","source":"### HMM Evaluation","metadata":{}},{"cell_type":"code","source":"model_small = load_hmm(\"hmm_model_small.dill\")\nmodel_medium = load_hmm(\"hmm_model_medium.dill\")\nmodel_large = load_hmm(\"hmm_model_large.dill\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_small = evaluate_hmm(model_small, test_sentences)\npredicted_medium = evaluate_hmm(model_medium, test_sentences)\npredicted_large = evaluate_hmm(model_large, test_sentences)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_small = [item for sublist in predicted_small for item in sublist]\npredicted_medium = [item for sublist in predicted_medium for item in sublist]\npredicted_large = [item for sublist in predicted_large for item in sublist]\n\nprint(len(correct_tags))\nprint(len(predicted_small))\nprint(len(predicted_medium))\nprint(len(predicted_large))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(correct_tags, predicted_small, labels = label_list))\nprint(classification_report(correct_tags, predicted_medium, labels = label_list))\nprint(classification_report(correct_tags, predicted_large, labels = label_list))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = evaluate_hmm(model, test_sentences)\npredicted = [item for sublist in predicted for item in sublist]\nprint(classification_report(correct_tags, predicted, labels = label_list))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BERT Evaluation","metadata":{}},{"cell_type":"code","source":"model_small = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_small_save\", num_labels=len(label_list))\nmodel_medium = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_medium_save\", num_labels=len(label_list))\nmodel_large = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_large_save\", num_labels=len(label_list))\n\n\nBERTtrainer_small = Trainer(\n    model_small,\n    args_small,\n    train_dataset=small_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_medium = Trainer(\n    model_medium,\n    args_medium,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_large = Trainer(\n    model_large,\n    args_large,\n    train_dataset=large_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n# BERTtrainer_small.model.from_pretrained(\"model_saves/BERT_small_save\")\n# BERTtrainer_medium.model.from_pretrained(\"model_saves/BERT_medium_save\")\n# BERTtrainer_large.model.from_pretrained(\"model_saves/BERT_large_save\")\n\nsmall_metrics = BERTtrainer_small.evaluate()\nprint(small_metrics)\nmedium_metrics = BERTtrainer_medium.evaluate()\nprint(medium_metrics)\nlarge_metrics = BERTtrainer_large.evaluate()\nprint(large_metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiment 4 (Hyperparameters)","metadata":{}},{"cell_type":"markdown","source":"## Arguments For Each Test","metadata":{}},{"cell_type":"code","source":"args_one = TrainingArguments(\n    f\"{model_name}-finetuned-one-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 100,\n    logging_steps = 100,\n    save_total_limit = 1,\n    learning_rate=0.01,\n    adam_beta1=0.9,\n    adam_beta2=0.999,\n    adam_epsilon=1e-8,\n    max_grad_norm=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    weight_decay=0.001,\n    save_steps=0,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=[\"tensorboard\"],\n)\n\nargs_two = TrainingArguments(\n    f\"{model_name}-finetuned-two-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 100,\n    logging_steps = 100,\n    save_total_limit = 1,\n    learning_rate=0.001,\n    adam_beta1=0.9,\n    adam_beta2=0.999,\n    adam_epsilon=1e-8,\n    max_grad_norm=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    weight_decay=0.001,\n    save_steps=0,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=[\"tensorboard\"],\n)\n\nargs_three = TrainingArguments(\n    f\"{model_name}-finetuned-three-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 100,\n    logging_steps = 100,\n    save_total_limit = 1,\n    learning_rate=0.0001,\n    adam_beta1=0.9,\n    adam_beta2=0.999,\n    adam_epsilon=1e-8,\n    max_grad_norm=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    weight_decay=0.001,\n    save_steps=0,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=[\"tensorboard\"],\n)\n\nargs_four = TrainingArguments(\n    f\"{model_name}-finetuned-four-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 100,\n    logging_steps = 100,\n    save_total_limit = 1,\n    learning_rate=0.00001,\n    adam_beta1=0.9,\n    adam_beta2=0.999,\n    adam_epsilon=1e-8,\n    max_grad_norm=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    weight_decay=0.001,\n    save_steps=0,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=[\"tensorboard\"],\n)\n\n\nargs_five = TrainingArguments(\n    f\"{model_name}-finetuned-five-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 100,\n    logging_steps = 100,\n    save_total_limit = 1,\n    learning_rate=0.000001,\n    adam_beta1=0.9,\n    adam_beta2=0.999,\n    adam_epsilon=1e-8,\n    max_grad_norm=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    weight_decay=0.001,\n    save_steps=0,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=[\"tensorboard\"],\n)\n\nargs_six = TrainingArguments(\n    f\"{model_name}-finetuned-six-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 100,\n    logging_steps = 100,\n    save_total_limit = 1,\n    learning_rate=0.00001,\n    adam_beta1=0.9,\n    adam_beta2=0.999,\n    adam_epsilon=1e-8,\n    max_grad_norm=1,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=1,\n    num_train_epochs=5,\n    weight_decay=0.001,\n    save_steps=0,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=[\"tensorboard\"],\n)\n\nargs_seven = TrainingArguments(\n    f\"{model_name}-finetuned-seven-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 100,\n    logging_steps = 100,\n    save_total_limit = 1,\n    learning_rate=0.00001,\n    adam_beta1=0.5,\n    adam_beta2=0.5,\n    adam_epsilon=1e-8,\n    max_grad_norm=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=1,\n    num_train_epochs=3,\n    weight_decay=0.001,\n    save_steps=0,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=[\"tensorboard\"],\n)\n\nargs_eight = TrainingArguments(\n    f\"{model_name}-finetuned-eight-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 100,\n    logging_steps = 100,\n    save_total_limit = 1,\n    learning_rate=0.00001,\n    adam_beta1=0.2,\n    adam_beta2=0.2,\n    adam_epsilon=1e-8,\n    max_grad_norm=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=1,\n    num_train_epochs=3,\n    weight_decay=0.001,\n    save_steps=0,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=[\"tensorboard\"],\n)\n\nargs_nine = TrainingArguments(\n    f\"{model_name}-finetuned-nine-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 100,\n    logging_steps = 100,\n    save_total_limit = 1,\n    learning_rate=0.00001,\n    adam_beta1=0.1,\n    adam_beta2=0.9,\n    adam_epsilon=1e-8,\n    max_grad_norm=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=1,\n    num_train_epochs=3,\n    weight_decay=0.001,\n    save_steps=0,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=[\"tensorboard\"],\n)\n\n\nargs_ten = TrainingArguments(\n    f\"{model_name}-finetuned-ten-{task}\",\n    evaluation_strategy ='steps',\n    eval_steps = 100,\n    logging_steps = 100,\n    save_total_limit = 1,\n    learning_rate=0.00001,\n    adam_beta1=0.9,\n    adam_beta2=0.1,\n    adam_epsilon=1e-8,\n    max_grad_norm=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=1,\n    num_train_epochs=5,\n    weight_decay=0.001,\n    save_steps=0,\n    metric_for_best_model = 'f1',\n    load_best_model_at_end=True,\n    report_to=[\"tensorboard\"],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BERTtrainer_ex_4 = Trainer(\n    model_ex_4,\n    args_one,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4.train()\nBERTtrainer_ex_4.model.save_pretrained(\"model_saves/BERT_ex4_one_save\")\nBERTtrainer_ex_4 = None\ntorch.cuda.empty_cache()\ngc.collect()\n\nmodel_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\nBERTtrainer_ex_4 = Trainer(\n    model_ex_4,\n    args_two,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4.train()\nBERTtrainer_ex_4.model.save_pretrained(\"model_saves/BERT_ex4_two_save\")\nBERTtrainer_ex_4 = None\ntorch.cuda.empty_cache()\ngc.collect()\n\nmodel_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\nBERTtrainer_ex_4 = Trainer(\n    model_ex_4,\n    args_four,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4.train()\nBERTtrainer_ex_4.model.save_pretrained(\"model_saves/BERT_ex4_four_save\")\nBERTtrainer_ex_4 = None\ntorch.cuda.empty_cache()\ngc.collect()\n\n\nmodel_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\nBERTtrainer_ex_4 = Trainer(\n    model_ex_4,\n    args_five,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4.train()\nBERTtrainer_ex_4.model.save_pretrained(\"model_saves/BERT_ex4_five_save\")\nBERTtrainer_ex_4 = None\ntorch.cuda.empty_cache()\ngc.collect()\n\nmodel_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\nBERTtrainer_ex_4 = Trainer(\n    model_ex_4,\n    args_seven,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4.train()\nBERTtrainer_ex_4.model.save_pretrained(\"model_saves/BERT_ex4_seven_save\")\nBERTtrainer_ex_4 = None\ntorch.cuda.empty_cache()\ngc.collect()\n\nmodel_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\nBERTtrainer_ex_4 = Trainer(\n    model_ex_4,\n    args_eight,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4.train()\nBERTtrainer_ex_4.model.save_pretrained(\"model_saves/BERT_ex4_eight_save\")\nBERTtrainer_ex_4 = None\ntorch.cuda.empty_cache()\ngc.collect()\n\nmodel_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\nBERTtrainer_ex_4 = Trainer(\n    model_ex_4,\n    args_nine,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4.train()\nBERTtrainer_ex_4.model.save_pretrained(\"model_saves/BERT_ex4_nine_save\")\nBERTtrainer_ex_4 = None\ntorch.cuda.empty_cache()\ngc.collect()\n\nmodel_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\nBERTtrainer_ex_4 = Trainer(\n    model_ex_4,\n    args_ten,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4.train()\nBERTtrainer_ex_4.model.save_pretrained(\"model_saves/BERT_ex4_ten_save\")\nBERTtrainer_ex_4 = None\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"model_ex_4_one = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_one_save\", num_labels=len(label_list))\nmodel_ex_4_two = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_two_save\", num_labels=len(label_list))\nmodel_ex_4_three = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_three_save\", num_labels=len(label_list))\nmodel_ex_4_four = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_four_save\", num_labels=len(label_list))\nmodel_ex_4_five = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_five_save\", num_labels=len(label_list))\nmodel_ex_4_six = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_six_save\", num_labels=len(label_list))\nmodel_ex_4_seven = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_seven_save\", num_labels=len(label_list))\nmodel_ex_4_eight = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_eight_save\", num_labels=len(label_list))\nmodel_ex_4_nine = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_nine_save\", num_labels=len(label_list))\nmodel_ex_4_ten = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_ten_save\", num_labels=len(label_list))\n\nBERTtrainer_ex_4_one = Trainer(\n    model_ex_4_one,\n    args_one,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4_two = Trainer(\n    model_ex_4_two,\n    args_two,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4_three = Trainer(\n    model_ex_4_three,\n    args_three,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4_four = Trainer(\n    model_ex_4_four,\n    args_four,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4_five = Trainer(\n    model_ex_4_five,\n    args_five,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4_six = Trainer(\n    model_ex_4_six,\n    args_six,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4_seven = Trainer(\n    model_ex_4_seven,\n    args_seven,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4_eight = Trainer(\n    model_ex_4_eight,\n    args_eight,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4_nine = Trainer(\n    model_ex_4_nine,\n    args_nine,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nBERTtrainer_ex_4_ten = Trainer(\n    model_ex_4_ten,\n    args_ten,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = BERTtrainer_ex_4_one.evaluate()\nprint(metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = BERTtrainer_ex_4_two.evaluate()\nprint(metrics)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = BERTtrainer_ex_4_three.evaluate()\nprint(metrics)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = BERTtrainer_ex_4_four.evaluate()\nprint(metrics)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = BERTtrainer_ex_4_five.evaluate()\nprint(metrics)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = BERTtrainer_ex_4_six.evaluate()\nprint(metrics)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = BERTtrainer_ex_4_seven.evaluate()\nprint(metrics)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = BERTtrainer_ex_4_eight.evaluate()\nprint(metrics)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = BERTtrainer_ex_4_nine.evaluate()\nprint(metrics)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = BERTtrainer_ex_4_ten.evaluate()\nprint(metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Automatic Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"model_name = \"bert-base-uncased\"","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:14:39.461506Z","iopub.execute_input":"2024-04-22T11:14:39.462279Z","iopub.status.idle":"2024-04-22T11:14:39.466721Z","shell.execute_reply.started":"2024-04-22T11:14:39.462242Z","shell.execute_reply":"2024-04-22T11:14:39.465692Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def optuna_hp_space(trial):\n    return {\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 3e-5, log=True),\n        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16]),\n        \"num_train_epochs\": trial.suggest_int(\"epochs\", 1, 3),\n        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0005, 0.01),\n        \"adam_beta1\": trial.suggest_float(\"adam_beta1\", 0.1, 0.9),\n        \"adam_beta2\": trial.suggest_float(\"adam_beta2\", 0.1, 0.9)\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:14:59.915825Z","iopub.execute_input":"2024-04-22T11:14:59.916605Z","iopub.status.idle":"2024-04-22T11:14:59.922579Z","shell.execute_reply.started":"2024-04-22T11:14:59.916571Z","shell.execute_reply":"2024-04-22T11:14:59.921624Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def model_init(trial):\n    return BertForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:15:06.091636Z","iopub.execute_input":"2024-04-22T11:15:06.092020Z","iopub.status.idle":"2024-04-22T11:15:06.097262Z","shell.execute_reply.started":"2024-04-22T11:15:06.091991Z","shell.execute_reply":"2024-04-22T11:15:06.096085Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train_args = TrainingArguments(\n        f\"{model_name}-finetuned-optuna-{task}\",\n        evaluation_strategy ='steps',\n        # eval_steps = 10,\n        # logging_steps = 10,\n        save_total_limit = 1, \n        adam_epsilon=1e-8,\n        max_grad_norm=1,\n        per_device_eval_batch_size=1,\n        save_steps=0,\n        metric_for_best_model = 'f1',\n        load_best_model_at_end=True,\n        report_to=[\"none\"],\n    )","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:15:09.553958Z","iopub.execute_input":"2024-04-22T11:15:09.554585Z","iopub.status.idle":"2024-04-22T11:15:09.610122Z","shell.execute_reply.started":"2024-04-22T11:15:09.554552Z","shell.execute_reply":"2024-04-22T11:15:09.609076Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=None,\n    args=train_args,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    model_init=model_init,\n    data_collator=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:15:15.300180Z","iopub.execute_input":"2024-04-22T11:15:15.300952Z","iopub.status.idle":"2024-04-22T11:15:17.483604Z","shell.execute_reply.started":"2024-04-22T11:15:15.300919Z","shell.execute_reply":"2024-04-22T11:15:17.482747Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:15:20.739772Z","iopub.execute_input":"2024-04-22T11:15:20.740491Z","iopub.status.idle":"2024-04-22T11:15:21.297544Z","shell.execute_reply.started":"2024-04-22T11:15:20.740457Z","shell.execute_reply":"2024-04-22T11:15:21.296570Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"61"},"metadata":{}}]},{"cell_type":"code","source":"def custom_objective(eval_result):\n  f1 = eval_result[\"eval_f1\"]\n  loss = eval_result[\"eval_loss\"]\n  return f1, loss\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:18:23.410473Z","iopub.execute_input":"2024-04-22T11:18:23.411277Z","iopub.status.idle":"2024-04-22T11:18:23.416287Z","shell.execute_reply.started":"2024-04-22T11:18:23.411241Z","shell.execute_reply":"2024-04-22T11:18:23.415217Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"best_trials = trainer.hyperparameter_search(\n    direction=[\"maximize\", \"minimize\"],\n    backend=\"optuna\",\n    hp_space=optuna_hp_space,\n    n_trials=20,\n    compute_objective=custom_objective,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:18:25.566486Z","iopub.execute_input":"2024-04-22T11:18:25.567361Z","iopub.status.idle":"2024-04-22T13:54:15.038440Z","shell.execute_reply.started":"2024-04-22T11:18:25.567329Z","shell.execute_reply":"2024-04-22T13:54:15.037631Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"[I 2024-04-22 11:18:25,569] A new study created in memory with name: no-name-f8da4f76-b586-4596-bed1-44bddb9646ce\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4152/4152 09:25, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.309600</td>\n      <td>0.197858</td>\n      <td>0.940979</td>\n      <td>0.927144</td>\n      <td>0.934010</td>\n      <td>0.928255</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.228400</td>\n      <td>0.185127</td>\n      <td>0.945914</td>\n      <td>0.933256</td>\n      <td>0.939543</td>\n      <td>0.933915</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.203900</td>\n      <td>0.173009</td>\n      <td>0.947719</td>\n      <td>0.940360</td>\n      <td>0.944025</td>\n      <td>0.938810</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.175700</td>\n      <td>0.167916</td>\n      <td>0.949784</td>\n      <td>0.943664</td>\n      <td>0.946714</td>\n      <td>0.940952</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.183600</td>\n      <td>0.164975</td>\n      <td>0.949119</td>\n      <td>0.943003</td>\n      <td>0.946051</td>\n      <td>0.940187</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.167600</td>\n      <td>0.171188</td>\n      <td>0.949900</td>\n      <td>0.942838</td>\n      <td>0.946356</td>\n      <td>0.940187</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.159200</td>\n      <td>0.164691</td>\n      <td>0.947953</td>\n      <td>0.944821</td>\n      <td>0.946384</td>\n      <td>0.941410</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.152300</td>\n      <td>0.165060</td>\n      <td>0.950100</td>\n      <td>0.943664</td>\n      <td>0.946871</td>\n      <td>0.940799</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.82      0.85      0.83       563\n        B-LF       0.76      0.77      0.76       290\n        I-LF       0.74      0.91      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.82      0.87      0.84      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.94      0.96      5197\n        B-AC       0.82      0.90      0.86       563\n        B-LF       0.76      0.82      0.79       290\n        I-LF       0.76      0.92      0.83       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.90      0.86      6537\nweighted avg       0.94      0.93      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.84      0.86       563\n        B-LF       0.78      0.82      0.80       290\n        I-LF       0.79      0.90      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.86      0.89      0.88       563\n        B-LF       0.79      0.82      0.81       290\n        I-LF       0.79      0.88      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.88      0.87       563\n        B-LF       0.79      0.81      0.80       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.87      0.87       563\n        B-LF       0.79      0.83      0.81       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.85      0.90      0.87       563\n        B-LF       0.80      0.84      0.82       290\n        I-LF       0.81      0.87      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.87      0.87       563\n        B-LF       0.80      0.83      0.81       290\n        I-LF       0.79      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 11:27:54,487] Trial 0 finished with values: [0.9468711147948612, 0.16505979001522064] and parameters: {'learning_rate': 1.4862333383535989e-05, 'per_device_train_batch_size': 8, 'epochs': 3, 'weight_decay': 0.007632839440960023, 'adam_beta1': 0.2374782514693039, 'adam_beta2': 0.6740412716573023}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5536' max='5536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5536/5536 07:10, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.318600</td>\n      <td>0.211133</td>\n      <td>0.929394</td>\n      <td>0.932926</td>\n      <td>0.931157</td>\n      <td>0.925501</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.236600</td>\n      <td>0.190024</td>\n      <td>0.946470</td>\n      <td>0.934743</td>\n      <td>0.940570</td>\n      <td>0.933915</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.222600</td>\n      <td>0.193186</td>\n      <td>0.940199</td>\n      <td>0.935074</td>\n      <td>0.937629</td>\n      <td>0.932385</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.216300</td>\n      <td>0.199294</td>\n      <td>0.945787</td>\n      <td>0.930943</td>\n      <td>0.938307</td>\n      <td>0.931467</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.198000</td>\n      <td>0.170724</td>\n      <td>0.949141</td>\n      <td>0.940360</td>\n      <td>0.944730</td>\n      <td>0.938351</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.190200</td>\n      <td>0.183322</td>\n      <td>0.948633</td>\n      <td>0.945812</td>\n      <td>0.947220</td>\n      <td>0.940799</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.158300</td>\n      <td>0.182057</td>\n      <td>0.948957</td>\n      <td>0.939865</td>\n      <td>0.944389</td>\n      <td>0.938351</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.165800</td>\n      <td>0.178504</td>\n      <td>0.949161</td>\n      <td>0.943830</td>\n      <td>0.946488</td>\n      <td>0.940034</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.171800</td>\n      <td>0.182520</td>\n      <td>0.948880</td>\n      <td>0.944490</td>\n      <td>0.946680</td>\n      <td>0.940646</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.163600</td>\n      <td>0.180918</td>\n      <td>0.950964</td>\n      <td>0.945151</td>\n      <td>0.948049</td>\n      <td>0.941869</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.162800</td>\n      <td>0.177528</td>\n      <td>0.950457</td>\n      <td>0.944490</td>\n      <td>0.947464</td>\n      <td>0.941410</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.95      0.96      0.96      5197\n        B-AC       0.88      0.75      0.81       563\n        B-LF       0.71      0.81      0.76       290\n        I-LF       0.82      0.80      0.81       487\n\n    accuracy                           0.93      6537\n   macro avg       0.84      0.83      0.83      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.87      0.83      0.85       563\n        B-LF       0.76      0.76      0.76       290\n        I-LF       0.76      0.90      0.83       487\n\n    accuracy                           0.93      6537\n   macro avg       0.84      0.86      0.85      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.78      0.91      0.84       563\n        B-LF       0.80      0.79      0.79       290\n        I-LF       0.79      0.87      0.83       487\n\n    accuracy                           0.93      6537\n   macro avg       0.84      0.88      0.86      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.94      0.96      5197\n        B-AC       0.84      0.89      0.87       563\n        B-LF       0.75      0.80      0.77       290\n        I-LF       0.74      0.92      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.89      0.85      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.89      0.83      0.86       563\n        B-LF       0.77      0.81      0.79       290\n        I-LF       0.78      0.90      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.88      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.97      0.97      5197\n        B-AC       0.90      0.83      0.87       563\n        B-LF       0.80      0.81      0.80       290\n        I-LF       0.81      0.87      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.87      0.87      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.84      0.91      0.88       563\n        B-LF       0.75      0.81      0.78       290\n        I-LF       0.79      0.91      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.84      0.90      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.85      0.91      0.88       563\n        B-LF       0.78      0.80      0.79       290\n        I-LF       0.80      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.90      0.85      0.87       563\n        B-LF       0.80      0.79      0.80       290\n        I-LF       0.80      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.87      0.87      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.89      0.88       563\n        B-LF       0.79      0.81      0.80       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.89      0.88       563\n        B-LF       0.78      0.81      0.80       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 11:35:08,182] Trial 1 finished with values: [0.9474643685780577, 0.1775275617837906] and parameters: {'learning_rate': 1.5201145492997306e-05, 'per_device_train_batch_size': 4, 'epochs': 2, 'weight_decay': 0.0013132813377010506, 'adam_beta1': 0.34055734532514564, 'adam_beta2': 0.10271393282928454}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1384' max='1384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1384/1384 06:31, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.296500</td>\n      <td>0.201248</td>\n      <td>0.939069</td>\n      <td>0.926813</td>\n      <td>0.932901</td>\n      <td>0.928102</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.205400</td>\n      <td>0.170630</td>\n      <td>0.945137</td>\n      <td>0.939204</td>\n      <td>0.942161</td>\n      <td>0.936515</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.98      0.94      0.96      5197\n        B-AC       0.78      0.90      0.84       563\n        B-LF       0.75      0.80      0.78       290\n        I-LF       0.75      0.91      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.82      0.89      0.85      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.86      0.86      0.86       563\n        B-LF       0.77      0.81      0.79       290\n        I-LF       0.79      0.87      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.88      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 11:41:43,796] Trial 2 finished with values: [0.9421610871726881, 0.17062970995903015] and parameters: {'learning_rate': 1.4225203287307486e-05, 'per_device_train_batch_size': 16, 'epochs': 2, 'weight_decay': 0.009441913650384852, 'adam_beta1': 0.1007033905579739, 'adam_beta2': 0.10660561316440083}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1384' max='1384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1384/1384 03:08, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.322400</td>\n      <td>0.204400</td>\n      <td>0.938735</td>\n      <td>0.926483</td>\n      <td>0.932568</td>\n      <td>0.925807</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.234100</td>\n      <td>0.189254</td>\n      <td>0.940902</td>\n      <td>0.931109</td>\n      <td>0.935979</td>\n      <td>0.930855</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.81      0.85      0.83       563\n        B-LF       0.74      0.75      0.75       290\n        I-LF       0.74      0.90      0.81       487\n\n    accuracy                           0.93      6537\n   macro avg       0.82      0.86      0.84      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.80      0.89      0.84       563\n        B-LF       0.75      0.80      0.78       290\n        I-LF       0.77      0.90      0.83       487\n\n    accuracy                           0.93      6537\n   macro avg       0.82      0.88      0.85      6537\nweighted avg       0.94      0.93      0.93      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 11:44:55,014] Trial 3 finished with values: [0.9359794071244706, 0.18925434350967407] and parameters: {'learning_rate': 1.1666504209998573e-05, 'per_device_train_batch_size': 8, 'epochs': 1, 'weight_decay': 0.009348845981118455, 'adam_beta1': 0.46219664966634755, 'adam_beta2': 0.7363835231366819}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='692' max='692' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [692/692 03:15, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.317400</td>\n      <td>0.201056</td>\n      <td>0.932945</td>\n      <td>0.926318</td>\n      <td>0.929619</td>\n      <td>0.925807</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.97      0.94      0.96      5197\n        B-AC       0.79      0.87      0.83       563\n        B-LF       0.72      0.79      0.76       290\n        I-LF       0.77      0.87      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.81      0.87      0.84      6537\nweighted avg       0.93      0.93      0.93      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 11:48:13,086] Trial 4 finished with values: [0.9296194976374036, 0.20105579495429993] and parameters: {'learning_rate': 1.0495334892765539e-05, 'per_device_train_batch_size': 16, 'epochs': 1, 'weight_decay': 0.004177094768599791, 'adam_beta1': 0.5947944115007203, 'adam_beta2': 0.7119370854788608}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2076' max='2076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2076/2076 09:49, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.306500</td>\n      <td>0.192577</td>\n      <td>0.940393</td>\n      <td>0.933091</td>\n      <td>0.936728</td>\n      <td>0.932538</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.205500</td>\n      <td>0.169847</td>\n      <td>0.946440</td>\n      <td>0.940030</td>\n      <td>0.943224</td>\n      <td>0.937739</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.194200</td>\n      <td>0.175756</td>\n      <td>0.947263</td>\n      <td>0.937717</td>\n      <td>0.942466</td>\n      <td>0.938045</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.178100</td>\n      <td>0.169647</td>\n      <td>0.948097</td>\n      <td>0.938543</td>\n      <td>0.943296</td>\n      <td>0.938351</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.80      0.90      0.85       563\n        B-LF       0.76      0.82      0.79       290\n        I-LF       0.78      0.89      0.83       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.89      0.86      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.85      0.87      0.86       563\n        B-LF       0.77      0.81      0.79       290\n        I-LF       0.79      0.88      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.88      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.97      5197\n        B-AC       0.84      0.88      0.86       563\n        B-LF       0.75      0.82      0.78       290\n        I-LF       0.78      0.91      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.84      0.89      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.96      0.97      5197\n        B-AC       0.85      0.87      0.86       563\n        B-LF       0.77      0.83      0.80       290\n        I-LF       0.78      0.91      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.84      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 11:58:05,625] Trial 5 finished with values: [0.9432959734329597, 0.16964735090732574] and parameters: {'learning_rate': 1.1314749336651628e-05, 'per_device_train_batch_size': 16, 'epochs': 3, 'weight_decay': 0.0011584680785346278, 'adam_beta1': 0.5034383028375595, 'adam_beta2': 0.5419724118157591}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4152/4152 09:31, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.316300</td>\n      <td>0.199075</td>\n      <td>0.940684</td>\n      <td>0.927474</td>\n      <td>0.934032</td>\n      <td>0.927796</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.231300</td>\n      <td>0.189801</td>\n      <td>0.944444</td>\n      <td>0.932430</td>\n      <td>0.938399</td>\n      <td>0.932844</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.207400</td>\n      <td>0.173616</td>\n      <td>0.947062</td>\n      <td>0.939865</td>\n      <td>0.943449</td>\n      <td>0.938657</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.180200</td>\n      <td>0.169473</td>\n      <td>0.948663</td>\n      <td>0.943334</td>\n      <td>0.945991</td>\n      <td>0.940646</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.188300</td>\n      <td>0.167097</td>\n      <td>0.949093</td>\n      <td>0.942508</td>\n      <td>0.945789</td>\n      <td>0.940340</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.173500</td>\n      <td>0.172104</td>\n      <td>0.949883</td>\n      <td>0.942508</td>\n      <td>0.946181</td>\n      <td>0.940340</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.165700</td>\n      <td>0.166612</td>\n      <td>0.947098</td>\n      <td>0.943499</td>\n      <td>0.945295</td>\n      <td>0.940034</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.159000</td>\n      <td>0.166092</td>\n      <td>0.951057</td>\n      <td>0.943830</td>\n      <td>0.947430</td>\n      <td>0.941257</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.82      0.85      0.84       563\n        B-LF       0.75      0.78      0.76       290\n        I-LF       0.74      0.90      0.81       487\n\n    accuracy                           0.93      6537\n   macro avg       0.82      0.87      0.84      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.82      0.90      0.86       563\n        B-LF       0.75      0.82      0.78       290\n        I-LF       0.76      0.91      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.89      0.86      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.83      0.85       563\n        B-LF       0.78      0.82      0.80       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.86      0.89      0.88       563\n        B-LF       0.79      0.82      0.80       290\n        I-LF       0.80      0.87      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.88      0.87       563\n        B-LF       0.79      0.81      0.80       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.86      0.87      0.87       563\n        B-LF       0.79      0.83      0.81       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.84      0.91      0.87       563\n        B-LF       0.79      0.83      0.81       290\n        I-LF       0.80      0.87      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.88      0.88       563\n        B-LF       0.79      0.82      0.81       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 12:07:39,987] Trial 6 finished with values: [0.9474295190713101, 0.1660916656255722] and parameters: {'learning_rate': 1.3399762647745683e-05, 'per_device_train_batch_size': 8, 'epochs': 3, 'weight_decay': 0.007269802275169065, 'adam_beta1': 0.149874201367758, 'adam_beta2': 0.19821008957090014}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2768' max='2768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2768/2768 06:21, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.302300</td>\n      <td>0.194392</td>\n      <td>0.943466</td>\n      <td>0.929126</td>\n      <td>0.936241</td>\n      <td>0.930090</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.225900</td>\n      <td>0.181027</td>\n      <td>0.947008</td>\n      <td>0.935900</td>\n      <td>0.941421</td>\n      <td>0.935903</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.200300</td>\n      <td>0.168838</td>\n      <td>0.945986</td>\n      <td>0.940360</td>\n      <td>0.943165</td>\n      <td>0.937739</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.171100</td>\n      <td>0.166799</td>\n      <td>0.949917</td>\n      <td>0.943169</td>\n      <td>0.946531</td>\n      <td>0.940187</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.178500</td>\n      <td>0.164056</td>\n      <td>0.948709</td>\n      <td>0.941186</td>\n      <td>0.944933</td>\n      <td>0.939881</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.83      0.85      0.84       563\n        B-LF       0.76      0.77      0.76       290\n        I-LF       0.75      0.92      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.87      0.85      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.82      0.90      0.86       563\n        B-LF       0.77      0.83      0.80       290\n        I-LF       0.77      0.91      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.83      0.90      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.89      0.81      0.85       563\n        B-LF       0.79      0.82      0.81       290\n        I-LF       0.80      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.87      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.86      0.89      0.88       563\n        B-LF       0.79      0.82      0.80       290\n        I-LF       0.79      0.88      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.96      0.97      5197\n        B-AC       0.86      0.88      0.87       563\n        B-LF       0.78      0.82      0.80       290\n        I-LF       0.79      0.90      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 12:14:22,881] Trial 7 finished with values: [0.9449328246807099, 0.16405589878559113] and parameters: {'learning_rate': 1.5409178867839693e-05, 'per_device_train_batch_size': 8, 'epochs': 2, 'weight_decay': 0.008213818647612382, 'adam_beta1': 0.5535504813003239, 'adam_beta2': 0.3698941353184395}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8304' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8304/8304 10:56, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.360100</td>\n      <td>0.227067</td>\n      <td>0.922407</td>\n      <td>0.926978</td>\n      <td>0.924687</td>\n      <td>0.920300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.247800</td>\n      <td>0.201302</td>\n      <td>0.942002</td>\n      <td>0.931109</td>\n      <td>0.936524</td>\n      <td>0.930090</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.232000</td>\n      <td>0.197683</td>\n      <td>0.936576</td>\n      <td>0.931935</td>\n      <td>0.934250</td>\n      <td>0.928713</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.223000</td>\n      <td>0.206536</td>\n      <td>0.945143</td>\n      <td>0.930778</td>\n      <td>0.937906</td>\n      <td>0.931773</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.208000</td>\n      <td>0.180025</td>\n      <td>0.944204</td>\n      <td>0.936560</td>\n      <td>0.940367</td>\n      <td>0.934832</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.201700</td>\n      <td>0.183984</td>\n      <td>0.946503</td>\n      <td>0.941186</td>\n      <td>0.943837</td>\n      <td>0.937892</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.173800</td>\n      <td>0.195228</td>\n      <td>0.944946</td>\n      <td>0.932926</td>\n      <td>0.938898</td>\n      <td>0.933609</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.182500</td>\n      <td>0.181368</td>\n      <td>0.949525</td>\n      <td>0.941682</td>\n      <td>0.945587</td>\n      <td>0.939881</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.189800</td>\n      <td>0.184901</td>\n      <td>0.946817</td>\n      <td>0.941186</td>\n      <td>0.943993</td>\n      <td>0.937892</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.181400</td>\n      <td>0.182158</td>\n      <td>0.949567</td>\n      <td>0.942508</td>\n      <td>0.946024</td>\n      <td>0.940799</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.181500</td>\n      <td>0.181307</td>\n      <td>0.950773</td>\n      <td>0.944490</td>\n      <td>0.947621</td>\n      <td>0.940952</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.162900</td>\n      <td>0.188151</td>\n      <td>0.948709</td>\n      <td>0.941186</td>\n      <td>0.944933</td>\n      <td>0.938504</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.164800</td>\n      <td>0.184391</td>\n      <td>0.948517</td>\n      <td>0.940525</td>\n      <td>0.944504</td>\n      <td>0.938198</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.161500</td>\n      <td>0.179575</td>\n      <td>0.947237</td>\n      <td>0.943169</td>\n      <td>0.945199</td>\n      <td>0.938963</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.153400</td>\n      <td>0.181222</td>\n      <td>0.947998</td>\n      <td>0.942673</td>\n      <td>0.945328</td>\n      <td>0.938963</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.164200</td>\n      <td>0.179604</td>\n      <td>0.948164</td>\n      <td>0.942838</td>\n      <td>0.945494</td>\n      <td>0.939116</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.95      0.96      0.96      5197\n        B-AC       0.86      0.74      0.80       563\n        B-LF       0.69      0.79      0.74       290\n        I-LF       0.80      0.79      0.79       487\n\n    accuracy                           0.92      6537\n   macro avg       0.83      0.82      0.82      6537\nweighted avg       0.92      0.92      0.92      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.85      0.83      0.84       563\n        B-LF       0.77      0.77      0.77       290\n        I-LF       0.74      0.89      0.81       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.86      0.84      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.77      0.91      0.83       563\n        B-LF       0.77      0.79      0.78       290\n        I-LF       0.79      0.86      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.88      0.85      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.94      0.96      5197\n        B-AC       0.84      0.89      0.86       563\n        B-LF       0.74      0.80      0.77       290\n        I-LF       0.74      0.92      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.82      0.89      0.85      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.88      0.80      0.84       563\n        B-LF       0.78      0.81      0.79       290\n        I-LF       0.79      0.89      0.83       487\n\n    accuracy                           0.93      6537\n   macro avg       0.85      0.86      0.86      6537\nweighted avg       0.94      0.93      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.89      0.82      0.85       563\n        B-LF       0.79      0.82      0.81       290\n        I-LF       0.80      0.87      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.87      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.94      0.96      5197\n        B-AC       0.81      0.92      0.86       563\n        B-LF       0.74      0.83      0.78       290\n        I-LF       0.77      0.92      0.84       487\n\n    accuracy                           0.93      6537\n   macro avg       0.82      0.90      0.86      6537\nweighted avg       0.94      0.93      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.97      5197\n        B-AC       0.84      0.90      0.87       563\n        B-LF       0.78      0.81      0.80       290\n        I-LF       0.79      0.90      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.96      0.96      0.96      5197\n        B-AC       0.89      0.81      0.85       563\n        B-LF       0.79      0.80      0.80       290\n        I-LF       0.80      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.87      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.87      0.87       563\n        B-LF       0.78      0.82      0.80       290\n        I-LF       0.79      0.90      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.87      0.87       563\n        B-LF       0.79      0.82      0.81       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.87      0.88      0.87       563\n        B-LF       0.76      0.83      0.79       290\n        I-LF       0.79      0.89      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.87      0.88      0.87       563\n        B-LF       0.76      0.81      0.79       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.89      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.86      0.89      0.88       563\n        B-LF       0.78      0.81      0.79       290\n        I-LF       0.80      0.86      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.87      0.88      0.88       563\n        B-LF       0.77      0.82      0.80       290\n        I-LF       0.79      0.87      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.87      0.88      0.88       563\n        B-LF       0.78      0.81      0.79       290\n        I-LF       0.79      0.87      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 12:25:41,160] Trial 8 finished with values: [0.9454937044400266, 0.17960412800312042] and parameters: {'learning_rate': 1.0249444777628393e-05, 'per_device_train_batch_size': 4, 'epochs': 3, 'weight_decay': 0.004295765732187533, 'adam_beta1': 0.15317764726933403, 'adam_beta2': 0.603950742546258}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5536' max='5536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5536/5536 07:18, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.311500</td>\n      <td>0.208569</td>\n      <td>0.931091</td>\n      <td>0.933091</td>\n      <td>0.932090</td>\n      <td>0.927184</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.234600</td>\n      <td>0.186045</td>\n      <td>0.946841</td>\n      <td>0.935734</td>\n      <td>0.941255</td>\n      <td>0.935291</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.220400</td>\n      <td>0.187701</td>\n      <td>0.944334</td>\n      <td>0.938873</td>\n      <td>0.941596</td>\n      <td>0.936209</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.215600</td>\n      <td>0.199774</td>\n      <td>0.945610</td>\n      <td>0.930613</td>\n      <td>0.938052</td>\n      <td>0.931008</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.195600</td>\n      <td>0.168157</td>\n      <td>0.950008</td>\n      <td>0.941847</td>\n      <td>0.945910</td>\n      <td>0.940340</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.188200</td>\n      <td>0.181541</td>\n      <td>0.949487</td>\n      <td>0.947134</td>\n      <td>0.948309</td>\n      <td>0.942328</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.155300</td>\n      <td>0.182210</td>\n      <td>0.950284</td>\n      <td>0.941021</td>\n      <td>0.945630</td>\n      <td>0.939881</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.163300</td>\n      <td>0.176552</td>\n      <td>0.950839</td>\n      <td>0.945812</td>\n      <td>0.948319</td>\n      <td>0.942022</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.167900</td>\n      <td>0.180659</td>\n      <td>0.948216</td>\n      <td>0.943830</td>\n      <td>0.946018</td>\n      <td>0.939881</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.160700</td>\n      <td>0.179706</td>\n      <td>0.950899</td>\n      <td>0.943830</td>\n      <td>0.947351</td>\n      <td>0.940952</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.158700</td>\n      <td>0.175805</td>\n      <td>0.951382</td>\n      <td>0.943995</td>\n      <td>0.947674</td>\n      <td>0.940952</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.96      0.96      0.96      5197\n        B-AC       0.88      0.76      0.81       563\n        B-LF       0.72      0.83      0.77       290\n        I-LF       0.81      0.81      0.81       487\n\n    accuracy                           0.93      6537\n   macro avg       0.84      0.84      0.84      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.87      0.84      0.85       563\n        B-LF       0.76      0.78      0.77       290\n        I-LF       0.77      0.90      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.84      0.87      0.85      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.79      0.91      0.85       563\n        B-LF       0.80      0.80      0.80       290\n        I-LF       0.81      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.84      0.89      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.94      0.96      5197\n        B-AC       0.84      0.90      0.87       563\n        B-LF       0.74      0.80      0.77       290\n        I-LF       0.73      0.92      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.82      0.89      0.85      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.89      0.83      0.86       563\n        B-LF       0.78      0.82      0.80       290\n        I-LF       0.79      0.90      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.97      0.97      5197\n        B-AC       0.90      0.83      0.87       563\n        B-LF       0.81      0.81      0.81       290\n        I-LF       0.82      0.87      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.87      0.87      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.97      5197\n        B-AC       0.84      0.92      0.88       563\n        B-LF       0.76      0.82      0.79       290\n        I-LF       0.79      0.91      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.84      0.90      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.86      0.91      0.89       563\n        B-LF       0.79      0.81      0.80       290\n        I-LF       0.81      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.90      0.85      0.87       563\n        B-LF       0.80      0.78      0.79       290\n        I-LF       0.80      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.87      0.87      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.88      0.88       563\n        B-LF       0.78      0.82      0.80       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.89      0.88       563\n        B-LF       0.78      0.81      0.80       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 12:33:21,132] Trial 9 finished with values: [0.9476739364789784, 0.17580465972423553] and parameters: {'learning_rate': 1.827772538829445e-05, 'per_device_train_batch_size': 4, 'epochs': 2, 'weight_decay': 0.007513860111426424, 'adam_beta1': 0.407210540866042, 'adam_beta2': 0.21499684396083066}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2076' max='2076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2076/2076 09:50, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.413800</td>\n      <td>0.362308</td>\n      <td>0.855737</td>\n      <td>0.890798</td>\n      <td>0.872916</td>\n      <td>0.867217</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.364400</td>\n      <td>0.329907</td>\n      <td>0.862402</td>\n      <td>0.886337</td>\n      <td>0.874206</td>\n      <td>0.871348</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.328900</td>\n      <td>0.327370</td>\n      <td>0.872009</td>\n      <td>0.897076</td>\n      <td>0.884365</td>\n      <td>0.882821</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.276700</td>\n      <td>0.308086</td>\n      <td>0.878268</td>\n      <td>0.904675</td>\n      <td>0.891276</td>\n      <td>0.889552</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.92      0.93      0.93      5197\n        B-AC       0.68      0.81      0.74       563\n        B-LF       0.55      0.42      0.48       290\n        I-LF       0.66      0.51      0.58       487\n\n    accuracy                           0.87      6537\n   macro avg       0.70      0.67      0.68      6537\nweighted avg       0.86      0.87      0.86      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.94      0.92      0.93      5197\n        B-AC       0.68      0.82      0.75       563\n        B-LF       0.52      0.46      0.49       290\n        I-LF       0.62      0.62      0.62       487\n\n    accuracy                           0.87      6537\n   macro avg       0.69      0.71      0.70      6537\nweighted avg       0.87      0.87      0.87      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.94      0.93      0.94      5197\n        B-AC       0.71      0.83      0.76       563\n        B-LF       0.54      0.54      0.54       290\n        I-LF       0.68      0.63      0.65       487\n\n    accuracy                           0.88      6537\n   macro avg       0.72      0.73      0.72      6537\nweighted avg       0.89      0.88      0.88      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.94      0.95      0.94      5197\n        B-AC       0.76      0.79      0.77       563\n        B-LF       0.58      0.50      0.53       290\n        I-LF       0.70      0.64      0.67       487\n\n    accuracy                           0.89      6537\n   macro avg       0.74      0.72      0.73      6537\nweighted avg       0.89      0.89      0.89      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 12:43:31,982] Trial 10 finished with values: [0.8912760416666667, 0.30808576941490173] and parameters: {'learning_rate': 2.013578678866205e-05, 'per_device_train_batch_size': 16, 'epochs': 3, 'weight_decay': 0.0018976159397410437, 'adam_beta1': 0.8437793353524975, 'adam_beta2': 0.269862505957472}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8304' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8304/8304 10:53, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.302500</td>\n      <td>0.209972</td>\n      <td>0.932638</td>\n      <td>0.930943</td>\n      <td>0.931790</td>\n      <td>0.927184</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.234500</td>\n      <td>0.190260</td>\n      <td>0.947069</td>\n      <td>0.934082</td>\n      <td>0.940531</td>\n      <td>0.934527</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.221000</td>\n      <td>0.188993</td>\n      <td>0.944176</td>\n      <td>0.936065</td>\n      <td>0.940103</td>\n      <td>0.934374</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.214200</td>\n      <td>0.191973</td>\n      <td>0.945955</td>\n      <td>0.931109</td>\n      <td>0.938473</td>\n      <td>0.931008</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.192700</td>\n      <td>0.163898</td>\n      <td>0.952023</td>\n      <td>0.940856</td>\n      <td>0.946406</td>\n      <td>0.939575</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.182700</td>\n      <td>0.181438</td>\n      <td>0.950058</td>\n      <td>0.949116</td>\n      <td>0.949587</td>\n      <td>0.943093</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.148700</td>\n      <td>0.165883</td>\n      <td>0.952206</td>\n      <td>0.944656</td>\n      <td>0.948416</td>\n      <td>0.942328</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.156200</td>\n      <td>0.181623</td>\n      <td>0.949338</td>\n      <td>0.947299</td>\n      <td>0.948317</td>\n      <td>0.940646</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.162900</td>\n      <td>0.196692</td>\n      <td>0.945861</td>\n      <td>0.943830</td>\n      <td>0.944844</td>\n      <td>0.936974</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.156000</td>\n      <td>0.170212</td>\n      <td>0.950613</td>\n      <td>0.947629</td>\n      <td>0.949119</td>\n      <td>0.942481</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.150800</td>\n      <td>0.182385</td>\n      <td>0.955211</td>\n      <td>0.947794</td>\n      <td>0.951489</td>\n      <td>0.944317</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.121500</td>\n      <td>0.185553</td>\n      <td>0.951430</td>\n      <td>0.944986</td>\n      <td>0.948197</td>\n      <td>0.941410</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.119100</td>\n      <td>0.184829</td>\n      <td>0.952681</td>\n      <td>0.947960</td>\n      <td>0.950315</td>\n      <td>0.944164</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.119100</td>\n      <td>0.184354</td>\n      <td>0.947742</td>\n      <td>0.949777</td>\n      <td>0.948758</td>\n      <td>0.942634</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.111100</td>\n      <td>0.192993</td>\n      <td>0.949868</td>\n      <td>0.948455</td>\n      <td>0.949161</td>\n      <td>0.942481</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.116700</td>\n      <td>0.198606</td>\n      <td>0.951993</td>\n      <td>0.946803</td>\n      <td>0.949391</td>\n      <td>0.942022</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.86      0.81      0.83       563\n        B-LF       0.68      0.84      0.75       290\n        I-LF       0.79      0.84      0.81       487\n\n    accuracy                           0.93      6537\n   macro avg       0.82      0.86      0.84      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.88      0.84      0.86       563\n        B-LF       0.76      0.78      0.77       290\n        I-LF       0.76      0.92      0.83       487\n\n    accuracy                           0.93      6537\n   macro avg       0.84      0.87      0.85      6537\nweighted avg       0.94      0.93      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.94      0.96      5197\n        B-AC       0.78      0.93      0.85       563\n        B-LF       0.79      0.81      0.80       290\n        I-LF       0.79      0.90      0.84       487\n\n    accuracy                           0.93      6537\n   macro avg       0.84      0.90      0.86      6537\nweighted avg       0.94      0.93      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.94      0.96      5197\n        B-AC       0.86      0.90      0.88       563\n        B-LF       0.74      0.82      0.78       290\n        I-LF       0.72      0.92      0.81       487\n\n    accuracy                           0.93      6537\n   macro avg       0.82      0.90      0.86      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.90      0.84      0.87       563\n        B-LF       0.78      0.82      0.80       290\n        I-LF       0.78      0.92      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.97      0.97      5197\n        B-AC       0.90      0.85      0.87       563\n        B-LF       0.81      0.81      0.81       290\n        I-LF       0.83      0.85      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.88      0.87      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.96      0.97      5197\n        B-AC       0.85      0.92      0.89       563\n        B-LF       0.78      0.83      0.80       290\n        I-LF       0.80      0.90      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.90      0.88      6537\nweighted avg       0.95      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.91      0.89       563\n        B-LF       0.78      0.80      0.79       290\n        I-LF       0.81      0.85      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.96      0.97      0.96      5197\n        B-AC       0.90      0.83      0.87       563\n        B-LF       0.81      0.77      0.79       290\n        I-LF       0.80      0.84      0.82       487\n\n    accuracy                           0.94      6537\n   macro avg       0.87      0.85      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.89      0.88       563\n        B-LF       0.78      0.81      0.79       290\n        I-LF       0.83      0.87      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.96      0.97      5197\n        B-AC       0.89      0.89      0.89       563\n        B-LF       0.79      0.85      0.82       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.90      0.88      6537\nweighted avg       0.95      0.94      0.95      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.90      0.89       563\n        B-LF       0.77      0.81      0.79       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.90      0.89       563\n        B-LF       0.78      0.86      0.82       290\n        I-LF       0.82      0.89      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.90      0.88      6537\nweighted avg       0.95      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.91      0.89       563\n        B-LF       0.80      0.81      0.81       290\n        I-LF       0.84      0.83      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.87      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.89      0.89      0.89       563\n        B-LF       0.78      0.80      0.79       290\n        I-LF       0.82      0.86      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.89      0.89       563\n        B-LF       0.78      0.82      0.80       290\n        I-LF       0.80      0.87      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 12:54:46,767] Trial 11 finished with values: [0.9493912035119688, 0.19860602915287018] and parameters: {'learning_rate': 2.3559867008430577e-05, 'per_device_train_batch_size': 4, 'epochs': 3, 'weight_decay': 0.004443194177945597, 'adam_beta1': 0.705204700300799, 'adam_beta2': 0.5365038647556237}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5536' max='5536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5536/5536 07:15, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.317300</td>\n      <td>0.212878</td>\n      <td>0.930613</td>\n      <td>0.930613</td>\n      <td>0.930613</td>\n      <td>0.925807</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.236200</td>\n      <td>0.190022</td>\n      <td>0.945625</td>\n      <td>0.933752</td>\n      <td>0.939651</td>\n      <td>0.932997</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.223200</td>\n      <td>0.191983</td>\n      <td>0.942311</td>\n      <td>0.936395</td>\n      <td>0.939344</td>\n      <td>0.934068</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.216900</td>\n      <td>0.195794</td>\n      <td>0.945488</td>\n      <td>0.931274</td>\n      <td>0.938327</td>\n      <td>0.931620</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.196900</td>\n      <td>0.169544</td>\n      <td>0.947184</td>\n      <td>0.939204</td>\n      <td>0.943177</td>\n      <td>0.937280</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.189100</td>\n      <td>0.177075</td>\n      <td>0.948739</td>\n      <td>0.944821</td>\n      <td>0.946776</td>\n      <td>0.940952</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.157600</td>\n      <td>0.175883</td>\n      <td>0.950275</td>\n      <td>0.940856</td>\n      <td>0.945542</td>\n      <td>0.939575</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.165400</td>\n      <td>0.172438</td>\n      <td>0.951739</td>\n      <td>0.944821</td>\n      <td>0.948267</td>\n      <td>0.941869</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.170200</td>\n      <td>0.176630</td>\n      <td>0.950166</td>\n      <td>0.944986</td>\n      <td>0.947569</td>\n      <td>0.941104</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.162000</td>\n      <td>0.178073</td>\n      <td>0.951390</td>\n      <td>0.944160</td>\n      <td>0.947761</td>\n      <td>0.941410</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.161600</td>\n      <td>0.174818</td>\n      <td>0.951248</td>\n      <td>0.944490</td>\n      <td>0.947857</td>\n      <td>0.941563</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.96      0.96      0.96      5197\n        B-AC       0.87      0.77      0.82       563\n        B-LF       0.70      0.83      0.76       290\n        I-LF       0.79      0.83      0.81       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.85      0.84      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.87      0.83      0.85       563\n        B-LF       0.76      0.75      0.76       290\n        I-LF       0.76      0.90      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.84      0.86      0.85      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.79      0.91      0.84       563\n        B-LF       0.79      0.80      0.80       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.93      6537\n   macro avg       0.84      0.89      0.86      6537\nweighted avg       0.94      0.93      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.94      0.96      5197\n        B-AC       0.84      0.89      0.87       563\n        B-LF       0.73      0.79      0.76       290\n        I-LF       0.75      0.92      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.82      0.89      0.85      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.89      0.82      0.86       563\n        B-LF       0.77      0.81      0.79       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.87      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.97      0.97      5197\n        B-AC       0.90      0.83      0.87       563\n        B-LF       0.80      0.81      0.80       290\n        I-LF       0.81      0.87      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.87      0.87      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.85      0.91      0.88       563\n        B-LF       0.76      0.82      0.79       290\n        I-LF       0.79      0.91      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.84      0.90      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.96      0.97      5197\n        B-AC       0.86      0.91      0.88       563\n        B-LF       0.79      0.81      0.80       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.90      0.85      0.87       563\n        B-LF       0.80      0.80      0.80       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.87      0.87      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.89      0.88       563\n        B-LF       0.78      0.81      0.80       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.89      0.88       563\n        B-LF       0.78      0.81      0.80       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 13:02:03,337] Trial 12 finished with values: [0.9478570836442013, 0.17481805384159088] and parameters: {'learning_rate': 1.698913795368728e-05, 'per_device_train_batch_size': 4, 'epochs': 2, 'weight_decay': 0.0027984591554001785, 'adam_beta1': 0.6102149262820477, 'adam_beta2': 0.5560883112575032}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5536' max='5536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5536/5536 07:15, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.349100</td>\n      <td>0.225178</td>\n      <td>0.922419</td>\n      <td>0.927144</td>\n      <td>0.924775</td>\n      <td>0.919841</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.245600</td>\n      <td>0.199913</td>\n      <td>0.941353</td>\n      <td>0.930778</td>\n      <td>0.936036</td>\n      <td>0.929478</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.230600</td>\n      <td>0.197187</td>\n      <td>0.937095</td>\n      <td>0.932761</td>\n      <td>0.934923</td>\n      <td>0.929019</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.222600</td>\n      <td>0.205801</td>\n      <td>0.943365</td>\n      <td>0.930117</td>\n      <td>0.936694</td>\n      <td>0.930855</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.207300</td>\n      <td>0.180488</td>\n      <td>0.944195</td>\n      <td>0.936395</td>\n      <td>0.940279</td>\n      <td>0.934832</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.201100</td>\n      <td>0.183661</td>\n      <td>0.945847</td>\n      <td>0.940691</td>\n      <td>0.943262</td>\n      <td>0.937586</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.173400</td>\n      <td>0.189598</td>\n      <td>0.947746</td>\n      <td>0.937882</td>\n      <td>0.942788</td>\n      <td>0.937586</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.182000</td>\n      <td>0.178989</td>\n      <td>0.950391</td>\n      <td>0.943169</td>\n      <td>0.946766</td>\n      <td>0.941257</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.188600</td>\n      <td>0.178859</td>\n      <td>0.947333</td>\n      <td>0.942012</td>\n      <td>0.944665</td>\n      <td>0.938351</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.180400</td>\n      <td>0.178429</td>\n      <td>0.950216</td>\n      <td>0.942838</td>\n      <td>0.946513</td>\n      <td>0.940493</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.180400</td>\n      <td>0.175636</td>\n      <td>0.948313</td>\n      <td>0.942673</td>\n      <td>0.945485</td>\n      <td>0.939728</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.95      0.96      0.95      5197\n        B-AC       0.86      0.73      0.79       563\n        B-LF       0.70      0.79      0.74       290\n        I-LF       0.80      0.78      0.79       487\n\n    accuracy                           0.92      6537\n   macro avg       0.83      0.82      0.82      6537\nweighted avg       0.92      0.92      0.92      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.85      0.83      0.84       563\n        B-LF       0.76      0.76      0.76       290\n        I-LF       0.74      0.89      0.81       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.86      0.84      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.78      0.91      0.84       563\n        B-LF       0.77      0.79      0.78       290\n        I-LF       0.79      0.86      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.87      0.85      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.94      0.96      5197\n        B-AC       0.83      0.89      0.86       563\n        B-LF       0.74      0.80      0.77       290\n        I-LF       0.74      0.91      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.82      0.89      0.85      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.96      0.96      0.96      5197\n        B-AC       0.89      0.79      0.84       563\n        B-LF       0.78      0.81      0.79       290\n        I-LF       0.79      0.89      0.83       487\n\n    accuracy                           0.93      6537\n   macro avg       0.85      0.86      0.86      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.88      0.81      0.85       563\n        B-LF       0.80      0.82      0.81       290\n        I-LF       0.80      0.87      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.87      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.82      0.91      0.86       563\n        B-LF       0.77      0.83      0.80       290\n        I-LF       0.79      0.91      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.84      0.90      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.96      0.97      5197\n        B-AC       0.85      0.89      0.87       563\n        B-LF       0.79      0.81      0.80       290\n        I-LF       0.80      0.90      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.89      0.82      0.86       563\n        B-LF       0.80      0.80      0.80       290\n        I-LF       0.80      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.87      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.87      0.87       563\n        B-LF       0.78      0.82      0.80       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.87      0.87       563\n        B-LF       0.79      0.81      0.80       290\n        I-LF       0.79      0.88      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 13:09:20,252] Trial 13 finished with values: [0.9454846727423363, 0.17563563585281372] and parameters: {'learning_rate': 1.150669206895087e-05, 'per_device_train_batch_size': 4, 'epochs': 2, 'weight_decay': 0.006281367410989544, 'adam_beta1': 0.1871750780467215, 'adam_beta2': 0.5061683427840943}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='692' max='692' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [692/692 03:15, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.265100</td>\n      <td>0.178269</td>\n      <td>0.945630</td>\n      <td>0.936726</td>\n      <td>0.941157</td>\n      <td>0.935750</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.83      0.89      0.86       563\n        B-LF       0.77      0.80      0.78       290\n        I-LF       0.77      0.90      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.84      0.89      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 13:12:36,774] Trial 14 finished with values: [0.9411569424848536, 0.17826907336711884] and parameters: {'learning_rate': 2.9541073025632464e-05, 'per_device_train_batch_size': 16, 'epochs': 1, 'weight_decay': 0.0024850622156338503, 'adam_beta1': 0.1759041389275743, 'adam_beta2': 0.43487899400270924}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8304' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8304/8304 10:48, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.308800</td>\n      <td>0.208085</td>\n      <td>0.928854</td>\n      <td>0.931769</td>\n      <td>0.930309</td>\n      <td>0.925807</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.234400</td>\n      <td>0.182524</td>\n      <td>0.947703</td>\n      <td>0.937056</td>\n      <td>0.942349</td>\n      <td>0.936209</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.219800</td>\n      <td>0.187624</td>\n      <td>0.942301</td>\n      <td>0.936230</td>\n      <td>0.939256</td>\n      <td>0.934527</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.215700</td>\n      <td>0.195393</td>\n      <td>0.946599</td>\n      <td>0.931274</td>\n      <td>0.938874</td>\n      <td>0.931314</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.195300</td>\n      <td>0.165651</td>\n      <td>0.950342</td>\n      <td>0.942177</td>\n      <td>0.946242</td>\n      <td>0.940340</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.187700</td>\n      <td>0.178310</td>\n      <td>0.949801</td>\n      <td>0.947134</td>\n      <td>0.948466</td>\n      <td>0.942634</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.154000</td>\n      <td>0.184699</td>\n      <td>0.948572</td>\n      <td>0.938543</td>\n      <td>0.943531</td>\n      <td>0.936821</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.162900</td>\n      <td>0.175512</td>\n      <td>0.950548</td>\n      <td>0.946308</td>\n      <td>0.948423</td>\n      <td>0.942328</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.168500</td>\n      <td>0.187739</td>\n      <td>0.947028</td>\n      <td>0.942177</td>\n      <td>0.944596</td>\n      <td>0.938351</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.162100</td>\n      <td>0.176960</td>\n      <td>0.950863</td>\n      <td>0.946308</td>\n      <td>0.948580</td>\n      <td>0.942940</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.158500</td>\n      <td>0.186488</td>\n      <td>0.953887</td>\n      <td>0.946638</td>\n      <td>0.950249</td>\n      <td>0.943552</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.133600</td>\n      <td>0.184158</td>\n      <td>0.950525</td>\n      <td>0.942673</td>\n      <td>0.946583</td>\n      <td>0.939422</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.132600</td>\n      <td>0.186464</td>\n      <td>0.951114</td>\n      <td>0.944986</td>\n      <td>0.948040</td>\n      <td>0.941410</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.131600</td>\n      <td>0.179799</td>\n      <td>0.949105</td>\n      <td>0.945812</td>\n      <td>0.947456</td>\n      <td>0.941104</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.120500</td>\n      <td>0.184355</td>\n      <td>0.949088</td>\n      <td>0.945482</td>\n      <td>0.947281</td>\n      <td>0.939728</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.129600</td>\n      <td>0.189555</td>\n      <td>0.950980</td>\n      <td>0.945482</td>\n      <td>0.948223</td>\n      <td>0.940952</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.96      0.96      0.96      5197\n        B-AC       0.88      0.76      0.81       563\n        B-LF       0.69      0.82      0.75       290\n        I-LF       0.81      0.80      0.81       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.84      0.83      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.87      0.85      0.86       563\n        B-LF       0.77      0.78      0.77       290\n        I-LF       0.77      0.90      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.84      0.87      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.79      0.92      0.85       563\n        B-LF       0.79      0.79      0.79       290\n        I-LF       0.80      0.89      0.85       487\n\n    accuracy                           0.93      6537\n   macro avg       0.84      0.89      0.86      6537\nweighted avg       0.94      0.93      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.94      0.96      5197\n        B-AC       0.84      0.90      0.87       563\n        B-LF       0.74      0.82      0.78       290\n        I-LF       0.73      0.92      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.82      0.90      0.86      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.90      0.84      0.87       563\n        B-LF       0.79      0.81      0.80       290\n        I-LF       0.79      0.90      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.97      0.97      5197\n        B-AC       0.91      0.84      0.87       563\n        B-LF       0.80      0.81      0.80       290\n        I-LF       0.82      0.87      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.87      0.87      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.83      0.93      0.88       563\n        B-LF       0.74      0.82      0.78       290\n        I-LF       0.78      0.90      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.83      0.90      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.86      0.92      0.89       563\n        B-LF       0.79      0.80      0.79       290\n        I-LF       0.81      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.96      0.96      0.96      5197\n        B-AC       0.90      0.83      0.87       563\n        B-LF       0.79      0.77      0.78       290\n        I-LF       0.80      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.86      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.89      0.88       563\n        B-LF       0.78      0.83      0.80       290\n        I-LF       0.82      0.89      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.88      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.89      0.88      0.89       563\n        B-LF       0.80      0.83      0.81       290\n        I-LF       0.80      0.90      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.88      6537\nweighted avg       0.95      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.87      0.89      0.88       563\n        B-LF       0.76      0.81      0.79       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.89      0.89       563\n        B-LF       0.78      0.82      0.80       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.86      0.91      0.88       563\n        B-LF       0.77      0.81      0.79       290\n        I-LF       0.82      0.87      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.88      0.90      0.89       563\n        B-LF       0.77      0.80      0.79       290\n        I-LF       0.80      0.86      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.90      0.89       563\n        B-LF       0.77      0.80      0.79       290\n        I-LF       0.80      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 13:23:26,680] Trial 15 finished with values: [0.9482230138348107, 0.18955543637275696] and parameters: {'learning_rate': 2.029574429072882e-05, 'per_device_train_batch_size': 4, 'epochs': 3, 'weight_decay': 0.0012902936554091212, 'adam_beta1': 0.5425236973348518, 'adam_beta2': 0.5898902029449955}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='692' max='692' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [692/692 03:14, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.293800</td>\n      <td>0.193286</td>\n      <td>0.939041</td>\n      <td>0.931439</td>\n      <td>0.935224</td>\n      <td>0.931008</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.81      0.88      0.84       563\n        B-LF       0.75      0.81      0.78       290\n        I-LF       0.77      0.89      0.83       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.88      0.85      6537\nweighted avg       0.94      0.93      0.93      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 13:26:42,835] Trial 16 finished with values: [0.9352243509994194, 0.19328567385673523] and parameters: {'learning_rate': 1.4809355913082218e-05, 'per_device_train_batch_size': 16, 'epochs': 1, 'weight_decay': 0.0050545147021264225, 'adam_beta1': 0.44291032955181664, 'adam_beta2': 0.7297754228448994}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4152/4152 09:26, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.302600</td>\n      <td>0.196806</td>\n      <td>0.940398</td>\n      <td>0.927970</td>\n      <td>0.934143</td>\n      <td>0.928255</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.228500</td>\n      <td>0.185794</td>\n      <td>0.946417</td>\n      <td>0.933752</td>\n      <td>0.940042</td>\n      <td>0.934374</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.203300</td>\n      <td>0.174054</td>\n      <td>0.948043</td>\n      <td>0.940525</td>\n      <td>0.944269</td>\n      <td>0.938810</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.173900</td>\n      <td>0.167455</td>\n      <td>0.949834</td>\n      <td>0.944656</td>\n      <td>0.947238</td>\n      <td>0.941716</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.182100</td>\n      <td>0.167364</td>\n      <td>0.950332</td>\n      <td>0.945151</td>\n      <td>0.947735</td>\n      <td>0.941869</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.165500</td>\n      <td>0.171006</td>\n      <td>0.951248</td>\n      <td>0.944490</td>\n      <td>0.947857</td>\n      <td>0.942022</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.156400</td>\n      <td>0.165482</td>\n      <td>0.948518</td>\n      <td>0.946638</td>\n      <td>0.947577</td>\n      <td>0.942175</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.149200</td>\n      <td>0.167220</td>\n      <td>0.952072</td>\n      <td>0.945151</td>\n      <td>0.948599</td>\n      <td>0.941869</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.82      0.85      0.84       563\n        B-LF       0.75      0.76      0.76       290\n        I-LF       0.75      0.90      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.82      0.86      0.84      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.83      0.89      0.86       563\n        B-LF       0.77      0.82      0.79       290\n        I-LF       0.75      0.92      0.83       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.89      0.86      6537\nweighted avg       0.94      0.93      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.84      0.86       563\n        B-LF       0.78      0.83      0.81       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.86      0.90      0.88       563\n        B-LF       0.79      0.81      0.80       290\n        I-LF       0.80      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.88      0.88       563\n        B-LF       0.80      0.81      0.80       290\n        I-LF       0.80      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.87      0.87       563\n        B-LF       0.79      0.84      0.82       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.85      0.91      0.88       563\n        B-LF       0.80      0.83      0.81       290\n        I-LF       0.82      0.86      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.88      0.88       563\n        B-LF       0.80      0.82      0.81       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 13:36:11,161] Trial 17 finished with values: [0.9485989056541204, 0.16721966862678528] and parameters: {'learning_rate': 1.723364186510463e-05, 'per_device_train_batch_size': 8, 'epochs': 3, 'weight_decay': 0.0017073697673903134, 'adam_beta1': 0.1625004708363173, 'adam_beta2': 0.21453363300338024}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8304' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8304/8304 10:48, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.496100</td>\n      <td>0.452719</td>\n      <td>0.827874</td>\n      <td>0.868495</td>\n      <td>0.847698</td>\n      <td>0.839070</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.482400</td>\n      <td>0.421618</td>\n      <td>0.836878</td>\n      <td>0.884024</td>\n      <td>0.859806</td>\n      <td>0.846413</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.476500</td>\n      <td>0.407417</td>\n      <td>0.840748</td>\n      <td>0.883529</td>\n      <td>0.861608</td>\n      <td>0.852073</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.457200</td>\n      <td>0.395494</td>\n      <td>0.837571</td>\n      <td>0.881712</td>\n      <td>0.859074</td>\n      <td>0.850696</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.445100</td>\n      <td>0.382363</td>\n      <td>0.844100</td>\n      <td>0.882868</td>\n      <td>0.863049</td>\n      <td>0.855285</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.407900</td>\n      <td>0.375195</td>\n      <td>0.846445</td>\n      <td>0.886998</td>\n      <td>0.866247</td>\n      <td>0.856662</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.376400</td>\n      <td>0.368541</td>\n      <td>0.840395</td>\n      <td>0.884685</td>\n      <td>0.861972</td>\n      <td>0.852685</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.388700</td>\n      <td>0.355951</td>\n      <td>0.849218</td>\n      <td>0.887659</td>\n      <td>0.868013</td>\n      <td>0.860486</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.390700</td>\n      <td>0.355951</td>\n      <td>0.851969</td>\n      <td>0.893772</td>\n      <td>0.872370</td>\n      <td>0.862781</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.387100</td>\n      <td>0.349839</td>\n      <td>0.854834</td>\n      <td>0.891128</td>\n      <td>0.872604</td>\n      <td>0.865076</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.370800</td>\n      <td>0.344777</td>\n      <td>0.854344</td>\n      <td>0.893441</td>\n      <td>0.873456</td>\n      <td>0.865841</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.329100</td>\n      <td>0.360944</td>\n      <td>0.848331</td>\n      <td>0.881546</td>\n      <td>0.864620</td>\n      <td>0.857427</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.319400</td>\n      <td>0.355182</td>\n      <td>0.854462</td>\n      <td>0.887494</td>\n      <td>0.870665</td>\n      <td>0.863852</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.318500</td>\n      <td>0.358893</td>\n      <td>0.852425</td>\n      <td>0.891294</td>\n      <td>0.871426</td>\n      <td>0.865535</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.302800</td>\n      <td>0.346420</td>\n      <td>0.855134</td>\n      <td>0.894267</td>\n      <td>0.874263</td>\n      <td>0.868288</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.313200</td>\n      <td>0.347200</td>\n      <td>0.854335</td>\n      <td>0.890468</td>\n      <td>0.872027</td>\n      <td>0.866605</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.90      0.93      0.91      5197\n        B-AC       0.63      0.67      0.65       563\n        B-LF       0.36      0.29      0.32       290\n        I-LF       0.56      0.42      0.48       487\n\n    accuracy                           0.84      6537\n   macro avg       0.61      0.58      0.59      6537\nweighted avg       0.83      0.84      0.83      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.87      0.96      0.91      5197\n        B-AC       0.76      0.53      0.63       563\n        B-LF       0.59      0.10      0.18       290\n        I-LF       0.60      0.40      0.48       487\n\n    accuracy                           0.85      6537\n   macro avg       0.70      0.50      0.55      6537\nweighted avg       0.83      0.85      0.82      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.90      0.94      0.92      5197\n        B-AC       0.65      0.73      0.69       563\n        B-LF       0.52      0.24      0.33       290\n        I-LF       0.62      0.44      0.51       487\n\n    accuracy                           0.85      6537\n   macro avg       0.67      0.59      0.61      6537\nweighted avg       0.84      0.85      0.84      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.90      0.94      0.92      5197\n        B-AC       0.67      0.71      0.69       563\n        B-LF       0.48      0.31      0.38       290\n        I-LF       0.58      0.43      0.49       487\n\n    accuracy                           0.85      6537\n   macro avg       0.66      0.60      0.62      6537\nweighted avg       0.84      0.85      0.84      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.89      0.95      0.92      5197\n        B-AC       0.71      0.62      0.66       563\n        B-LF       0.60      0.26      0.37       290\n        I-LF       0.60      0.50      0.54       487\n\n    accuracy                           0.86      6537\n   macro avg       0.70      0.58      0.62      6537\nweighted avg       0.84      0.86      0.84      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.90      0.94      0.92      5197\n        B-AC       0.71      0.69      0.70       563\n        B-LF       0.59      0.33      0.43       290\n        I-LF       0.59      0.45      0.51       487\n\n    accuracy                           0.86      6537\n   macro avg       0.70      0.60      0.64      6537\nweighted avg       0.84      0.86      0.85      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.90      0.94      0.92      5197\n        B-AC       0.67      0.73      0.70       563\n        B-LF       0.52      0.34      0.42       290\n        I-LF       0.61      0.40      0.49       487\n\n    accuracy                           0.85      6537\n   macro avg       0.68      0.60      0.63      6537\nweighted avg       0.84      0.85      0.84      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.91      0.94      0.92      5197\n        B-AC       0.70      0.72      0.71       563\n        B-LF       0.49      0.41      0.44       290\n        I-LF       0.64      0.47      0.54       487\n\n    accuracy                           0.86      6537\n   macro avg       0.69      0.63      0.65      6537\nweighted avg       0.85      0.86      0.86      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.90      0.95      0.92      5197\n        B-AC       0.73      0.69      0.71       563\n        B-LF       0.57      0.32      0.41       290\n        I-LF       0.65      0.45      0.53       487\n\n    accuracy                           0.86      6537\n   macro avg       0.71      0.60      0.64      6537\nweighted avg       0.85      0.86      0.85      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.91      0.94      0.92      5197\n        B-AC       0.74      0.73      0.73       563\n        B-LF       0.55      0.35      0.43       290\n        I-LF       0.63      0.50      0.56       487\n\n    accuracy                           0.87      6537\n   macro avg       0.71      0.63      0.66      6537\nweighted avg       0.86      0.87      0.86      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.91      0.94      0.92      5197\n        B-AC       0.73      0.74      0.74       563\n        B-LF       0.58      0.39      0.46       290\n        I-LF       0.64      0.48      0.55       487\n\n    accuracy                           0.87      6537\n   macro avg       0.71      0.64      0.67      6537\nweighted avg       0.86      0.87      0.86      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.92      0.92      0.92      5197\n        B-AC       0.69      0.79      0.74       563\n        B-LF       0.52      0.43      0.47       290\n        I-LF       0.57      0.52      0.54       487\n\n    accuracy                           0.86      6537\n   macro avg       0.68      0.66      0.67      6537\nweighted avg       0.85      0.86      0.86      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.91      0.93      0.92      5197\n        B-AC       0.74      0.74      0.74       563\n        B-LF       0.57      0.42      0.49       290\n        I-LF       0.59      0.52      0.55       487\n\n    accuracy                           0.86      6537\n   macro avg       0.70      0.66      0.68      6537\nweighted avg       0.86      0.86      0.86      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.91      0.93      0.92      5197\n        B-AC       0.71      0.78      0.74       563\n        B-LF       0.60      0.43      0.50       290\n        I-LF       0.62      0.50      0.55       487\n\n    accuracy                           0.87      6537\n   macro avg       0.71      0.66      0.68      6537\nweighted avg       0.86      0.87      0.86      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.91      0.94      0.93      5197\n        B-AC       0.73      0.76      0.74       563\n        B-LF       0.60      0.42      0.49       290\n        I-LF       0.63      0.49      0.55       487\n\n    accuracy                           0.87      6537\n   macro avg       0.72      0.65      0.68      6537\nweighted avg       0.86      0.87      0.86      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.92      0.93      0.92      5197\n        B-AC       0.72      0.78      0.75       563\n        B-LF       0.58      0.45      0.51       290\n        I-LF       0.61      0.52      0.56       487\n\n    accuracy                           0.87      6537\n   macro avg       0.71      0.67      0.68      6537\nweighted avg       0.86      0.87      0.86      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 13:47:01,277] Trial 18 finished with values: [0.8720271800679502, 0.3471996784210205] and parameters: {'learning_rate': 1.3370912455129997e-05, 'per_device_train_batch_size': 4, 'epochs': 3, 'weight_decay': 0.0074114268772986665, 'adam_beta1': 0.8963563183856276, 'adam_beta2': 0.4387426559131742}. \nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5536' max='5536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5536/5536 07:12, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.298500</td>\n      <td>0.222004</td>\n      <td>0.921066</td>\n      <td>0.931109</td>\n      <td>0.926060</td>\n      <td>0.921218</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.236000</td>\n      <td>0.189011</td>\n      <td>0.948178</td>\n      <td>0.937056</td>\n      <td>0.942584</td>\n      <td>0.936515</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.220100</td>\n      <td>0.189335</td>\n      <td>0.940844</td>\n      <td>0.935404</td>\n      <td>0.938116</td>\n      <td>0.932079</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.211700</td>\n      <td>0.193782</td>\n      <td>0.946291</td>\n      <td>0.931439</td>\n      <td>0.938806</td>\n      <td>0.931008</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.191400</td>\n      <td>0.169646</td>\n      <td>0.949059</td>\n      <td>0.941847</td>\n      <td>0.945439</td>\n      <td>0.940187</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.183400</td>\n      <td>0.175379</td>\n      <td>0.949983</td>\n      <td>0.947629</td>\n      <td>0.948805</td>\n      <td>0.942787</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.146900</td>\n      <td>0.170882</td>\n      <td>0.950683</td>\n      <td>0.942673</td>\n      <td>0.946661</td>\n      <td>0.939728</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.154900</td>\n      <td>0.173223</td>\n      <td>0.952507</td>\n      <td>0.947629</td>\n      <td>0.950062</td>\n      <td>0.943399</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.160000</td>\n      <td>0.174321</td>\n      <td>0.949220</td>\n      <td>0.944986</td>\n      <td>0.947098</td>\n      <td>0.940952</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.150400</td>\n      <td>0.171833</td>\n      <td>0.951739</td>\n      <td>0.944821</td>\n      <td>0.948267</td>\n      <td>0.941410</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.146500</td>\n      <td>0.170141</td>\n      <td>0.953554</td>\n      <td>0.946308</td>\n      <td>0.949917</td>\n      <td>0.942940</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.94      0.97      0.96      5197\n        B-AC       0.90      0.67      0.77       563\n        B-LF       0.75      0.74      0.75       290\n        I-LF       0.84      0.77      0.80       487\n\n    accuracy                           0.92      6537\n   macro avg       0.86      0.79      0.82      6537\nweighted avg       0.92      0.92      0.92      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.86      0.87      0.86       563\n        B-LF       0.76      0.79      0.77       290\n        I-LF       0.77      0.91      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.84      0.88      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.78      0.92      0.84       563\n        B-LF       0.79      0.79      0.79       290\n        I-LF       0.79      0.87      0.83       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.88      0.86      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.86      0.87      0.87       563\n        B-LF       0.74      0.81      0.77       290\n        I-LF       0.73      0.92      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.89      0.85      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.90      0.83      0.86       563\n        B-LF       0.78      0.82      0.80       290\n        I-LF       0.80      0.90      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.97      0.97      5197\n        B-AC       0.91      0.85      0.88       563\n        B-LF       0.81      0.81      0.81       290\n        I-LF       0.82      0.86      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.87      0.87      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.86      0.92      0.89       563\n        B-LF       0.76      0.83      0.79       290\n        I-LF       0.78      0.89      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.84      0.90      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.91      0.90       563\n        B-LF       0.77      0.82      0.80       290\n        I-LF       0.80      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.88      6537\nweighted avg       0.95      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.90      0.85      0.88       563\n        B-LF       0.79      0.80      0.80       290\n        I-LF       0.81      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.87      0.87      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.89      0.88       563\n        B-LF       0.78      0.81      0.80       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.89      0.89      0.89       563\n        B-LF       0.79      0.82      0.80       290\n        I-LF       0.80      0.89      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.88      6537\nweighted avg       0.94      0.94      0.94      6537\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-22 13:54:15,031] Trial 19 finished with values: [0.9499170812603648, 0.17014054954051971] and parameters: {'learning_rate': 2.7372546773241383e-05, 'per_device_train_batch_size': 4, 'epochs': 2, 'weight_decay': 0.007683180956122042, 'adam_beta1': 0.1251657644168966, 'adam_beta2': 0.8712430313955226}. \n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(best_trials[4])","metadata":{"execution":{"iopub.status.busy":"2024-04-22T14:00:55.820580Z","iopub.execute_input":"2024-04-22T14:00:55.821416Z","iopub.status.idle":"2024-04-22T14:00:55.826839Z","shell.execute_reply.started":"2024-04-22T14:00:55.821378Z","shell.execute_reply":"2024-04-22T14:00:55.825606Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"BestRun(run_id='19', objective=[0.9499170812603648, 0.17014054954051971], hyperparameters={'learning_rate': 2.7372546773241383e-05, 'per_device_train_batch_size': 4, 'epochs': 2, 'weight_decay': 0.007683180956122042, 'adam_beta1': 0.1251657644168966, 'adam_beta2': 0.8712430313955226}, run_summary=None)\n","output_type":"stream"}]},{"cell_type":"code","source":"best_learning_rate = best_trials[4].hyperparameters[\"learning_rate\"] # 2.7372546773241383e-05\nbest_per_device_train_batch_size = best_trials[4].hyperparameters['per_device_train_batch_size'] # 4\nbest_epochs = best_trials[4].hyperparameters['epochs'] # 2\nbest_weight_decay = best_trials[4].hyperparameters['weight_decay'] # 0.007683180956122042\nbest_adam_beta1 = best_trials[4].hyperparameters['adam_beta1'] # 0.1251657644168966\nbest_adam_beta2 = best_trials[4].hyperparameters['adam_beta2'] # 0.8712430313955226","metadata":{"execution":{"iopub.status.busy":"2024-04-22T14:01:21.881207Z","iopub.execute_input":"2024-04-22T14:01:21.881938Z","iopub.status.idle":"2024-04-22T14:01:21.887932Z","shell.execute_reply.started":"2024-04-22T14:01:21.881901Z","shell.execute_reply":"2024-04-22T14:01:21.886871Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"print(f\"best_learning_rate: {best_learning_rate}\")\nprint(f\"best_per_device_train_batch_size: {best_per_device_train_batch_size}\")\nprint(f\"best_epochs: {best_epochs}\")\nprint(f\"best_weight_decay: {best_weight_decay}\")\nprint(f\"best_adam_beta1: {best_adam_beta1}\")\nprint(f\"best_adam_beta2: {best_adam_beta2}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T14:01:24.213314Z","iopub.execute_input":"2024-04-22T14:01:24.213686Z","iopub.status.idle":"2024-04-22T14:01:24.219159Z","shell.execute_reply.started":"2024-04-22T14:01:24.213659Z","shell.execute_reply":"2024-04-22T14:01:24.218356Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"best_learning_rate: 2.7372546773241383e-05\nbest_per_device_train_batch_size: 4\nbest_epochs: 2\nbest_weight_decay: 0.007683180956122042\nbest_adam_beta1: 0.1251657644168966\nbest_adam_beta2: 0.8712430313955226\n","output_type":"stream"}]},{"cell_type":"code","source":"best_train_args = TrainingArguments(\n        f\"{model_name}-finetuned-best-{task}\",\n        evaluation_strategy ='steps',\n        eval_steps = 100,\n        logging_steps = 100,\n        save_total_limit = 1, \n        adam_epsilon=1e-8,\n        max_grad_norm=1,\n        per_device_eval_batch_size=1,\n        save_steps=0,\n        metric_for_best_model = 'f1',\n        learning_rate = best_learning_rate,\n        per_device_train_batch_size = best_per_device_train_batch_size,\n        num_train_epochs = best_epochs,\n        weight_decay = best_weight_decay,\n        adam_beta1 = best_adam_beta1,\n        adam_beta2 = best_adam_beta2,\n        load_best_model_at_end=True,\n        report_to=[\"tensorboard\"],\n    )","metadata":{"execution":{"iopub.status.busy":"2024-04-22T14:01:28.405828Z","iopub.execute_input":"2024-04-22T14:01:28.406236Z","iopub.status.idle":"2024-04-22T14:01:28.432349Z","shell.execute_reply.started":"2024-04-22T14:01:28.406205Z","shell.execute_reply":"2024-04-22T14:01:28.431400Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"model = BertForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\nbest_trainer = Trainer(\n    model=model,\n    args=best_train_args,\n    train_dataset=medium_tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    model_init=model_init,\n    data_collator=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T14:01:33.717848Z","iopub.execute_input":"2024-04-22T14:01:33.718254Z","iopub.status.idle":"2024-04-22T14:01:34.500010Z","shell.execute_reply.started":"2024-04-22T14:01:33.718223Z","shell.execute_reply":"2024-04-22T14:01:34.499072Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:409: FutureWarning: `Trainer` requires either a `model` or `model_init` argument, but not both. `model_init` will overwrite your model when calling the `train` method. This will become a fatal error in the next release.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"best_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T14:01:42.750500Z","iopub.execute_input":"2024-04-22T14:01:42.751151Z","iopub.status.idle":"2024-04-22T14:10:17.105463Z","shell.execute_reply.started":"2024-04-22T14:01:42.751111Z","shell.execute_reply":"2024-04-22T14:10:17.104495Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5536' max='5536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5536/5536 08:32, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.452500</td>\n      <td>0.283832</td>\n      <td>0.900033</td>\n      <td>0.904345</td>\n      <td>0.902184</td>\n      <td>0.899036</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.279600</td>\n      <td>0.240837</td>\n      <td>0.923562</td>\n      <td>0.920205</td>\n      <td>0.921880</td>\n      <td>0.915099</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.250000</td>\n      <td>0.257836</td>\n      <td>0.923798</td>\n      <td>0.911284</td>\n      <td>0.917498</td>\n      <td>0.911274</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.252900</td>\n      <td>0.235453</td>\n      <td>0.935159</td>\n      <td>0.919709</td>\n      <td>0.927370</td>\n      <td>0.919994</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.257700</td>\n      <td>0.222004</td>\n      <td>0.921066</td>\n      <td>0.931109</td>\n      <td>0.926060</td>\n      <td>0.921218</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.248000</td>\n      <td>0.224666</td>\n      <td>0.929561</td>\n      <td>0.920040</td>\n      <td>0.924776</td>\n      <td>0.920912</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.251300</td>\n      <td>0.187811</td>\n      <td>0.942848</td>\n      <td>0.937552</td>\n      <td>0.940192</td>\n      <td>0.937280</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.231100</td>\n      <td>0.189980</td>\n      <td>0.943815</td>\n      <td>0.935239</td>\n      <td>0.939507</td>\n      <td>0.935138</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.234100</td>\n      <td>0.184323</td>\n      <td>0.945482</td>\n      <td>0.936891</td>\n      <td>0.941167</td>\n      <td>0.934985</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.215400</td>\n      <td>0.189011</td>\n      <td>0.948178</td>\n      <td>0.937056</td>\n      <td>0.942584</td>\n      <td>0.936515</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.212600</td>\n      <td>0.211461</td>\n      <td>0.935296</td>\n      <td>0.928961</td>\n      <td>0.932118</td>\n      <td>0.926419</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.210800</td>\n      <td>0.196647</td>\n      <td>0.946383</td>\n      <td>0.927309</td>\n      <td>0.936749</td>\n      <td>0.930090</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.241500</td>\n      <td>0.190835</td>\n      <td>0.943415</td>\n      <td>0.933752</td>\n      <td>0.938559</td>\n      <td>0.932079</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.214000</td>\n      <td>0.185232</td>\n      <td>0.940737</td>\n      <td>0.936230</td>\n      <td>0.938478</td>\n      <td>0.931161</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.221400</td>\n      <td>0.189335</td>\n      <td>0.940844</td>\n      <td>0.935404</td>\n      <td>0.938116</td>\n      <td>0.932079</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.223900</td>\n      <td>0.190142</td>\n      <td>0.948893</td>\n      <td>0.941682</td>\n      <td>0.945274</td>\n      <td>0.939728</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.230600</td>\n      <td>0.175565</td>\n      <td>0.944233</td>\n      <td>0.942673</td>\n      <td>0.943452</td>\n      <td>0.936668</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.195100</td>\n      <td>0.181100</td>\n      <td>0.945403</td>\n      <td>0.941186</td>\n      <td>0.943290</td>\n      <td>0.936056</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.204400</td>\n      <td>0.178343</td>\n      <td>0.950158</td>\n      <td>0.941682</td>\n      <td>0.945901</td>\n      <td>0.939269</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.204400</td>\n      <td>0.193782</td>\n      <td>0.946291</td>\n      <td>0.931439</td>\n      <td>0.938806</td>\n      <td>0.931008</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.185500</td>\n      <td>0.169228</td>\n      <td>0.949409</td>\n      <td>0.942508</td>\n      <td>0.945946</td>\n      <td>0.939116</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.192100</td>\n      <td>0.171833</td>\n      <td>0.940441</td>\n      <td>0.944325</td>\n      <td>0.942379</td>\n      <td>0.936515</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.210500</td>\n      <td>0.164060</td>\n      <td>0.939563</td>\n      <td>0.945151</td>\n      <td>0.942349</td>\n      <td>0.937739</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.185400</td>\n      <td>0.163746</td>\n      <td>0.951130</td>\n      <td>0.945316</td>\n      <td>0.948214</td>\n      <td>0.942481</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.183500</td>\n      <td>0.169646</td>\n      <td>0.949059</td>\n      <td>0.941847</td>\n      <td>0.945439</td>\n      <td>0.940187</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.199200</td>\n      <td>0.163052</td>\n      <td>0.951677</td>\n      <td>0.946803</td>\n      <td>0.949234</td>\n      <td>0.943399</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.197500</td>\n      <td>0.165654</td>\n      <td>0.951565</td>\n      <td>0.944490</td>\n      <td>0.948014</td>\n      <td>0.941104</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.203200</td>\n      <td>0.162650</td>\n      <td>0.952405</td>\n      <td>0.945482</td>\n      <td>0.948931</td>\n      <td>0.943093</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.165100</td>\n      <td>0.174253</td>\n      <td>0.949352</td>\n      <td>0.944490</td>\n      <td>0.946915</td>\n      <td>0.940799</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.152000</td>\n      <td>0.175379</td>\n      <td>0.949983</td>\n      <td>0.947629</td>\n      <td>0.948805</td>\n      <td>0.942787</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.139200</td>\n      <td>0.188689</td>\n      <td>0.951696</td>\n      <td>0.940691</td>\n      <td>0.946162</td>\n      <td>0.939881</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.155100</td>\n      <td>0.169818</td>\n      <td>0.951236</td>\n      <td>0.947464</td>\n      <td>0.949346</td>\n      <td>0.942634</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.150200</td>\n      <td>0.177386</td>\n      <td>0.949057</td>\n      <td>0.947960</td>\n      <td>0.948508</td>\n      <td>0.941410</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.131300</td>\n      <td>0.181350</td>\n      <td>0.949209</td>\n      <td>0.941682</td>\n      <td>0.945430</td>\n      <td>0.938810</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.158900</td>\n      <td>0.170882</td>\n      <td>0.950683</td>\n      <td>0.942673</td>\n      <td>0.946661</td>\n      <td>0.939728</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.156000</td>\n      <td>0.176310</td>\n      <td>0.953954</td>\n      <td>0.944656</td>\n      <td>0.949282</td>\n      <td>0.942634</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.152000</td>\n      <td>0.167823</td>\n      <td>0.950349</td>\n      <td>0.945482</td>\n      <td>0.947909</td>\n      <td>0.940646</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.157600</td>\n      <td>0.165629</td>\n      <td>0.951410</td>\n      <td>0.947794</td>\n      <td>0.949599</td>\n      <td>0.942328</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.152800</td>\n      <td>0.175131</td>\n      <td>0.953008</td>\n      <td>0.944821</td>\n      <td>0.948897</td>\n      <td>0.942634</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.156200</td>\n      <td>0.173223</td>\n      <td>0.952507</td>\n      <td>0.947629</td>\n      <td>0.950062</td>\n      <td>0.943399</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.159800</td>\n      <td>0.162766</td>\n      <td>0.951005</td>\n      <td>0.945977</td>\n      <td>0.948484</td>\n      <td>0.942481</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.167800</td>\n      <td>0.169783</td>\n      <td>0.952555</td>\n      <td>0.945316</td>\n      <td>0.948922</td>\n      <td>0.942787</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.154900</td>\n      <td>0.163940</td>\n      <td>0.955515</td>\n      <td>0.947464</td>\n      <td>0.951472</td>\n      <td>0.945388</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.162200</td>\n      <td>0.164661</td>\n      <td>0.949710</td>\n      <td>0.945316</td>\n      <td>0.947508</td>\n      <td>0.941104</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.155300</td>\n      <td>0.174321</td>\n      <td>0.949220</td>\n      <td>0.944986</td>\n      <td>0.947098</td>\n      <td>0.940952</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.160800</td>\n      <td>0.170351</td>\n      <td>0.953151</td>\n      <td>0.944490</td>\n      <td>0.948801</td>\n      <td>0.941410</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.136100</td>\n      <td>0.170407</td>\n      <td>0.951374</td>\n      <td>0.943830</td>\n      <td>0.947587</td>\n      <td>0.940493</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.155400</td>\n      <td>0.166329</td>\n      <td>0.948935</td>\n      <td>0.942508</td>\n      <td>0.945711</td>\n      <td>0.939269</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.160100</td>\n      <td>0.167926</td>\n      <td>0.950250</td>\n      <td>0.943499</td>\n      <td>0.946862</td>\n      <td>0.940187</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.139700</td>\n      <td>0.171833</td>\n      <td>0.951739</td>\n      <td>0.944821</td>\n      <td>0.948267</td>\n      <td>0.941410</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.154600</td>\n      <td>0.167274</td>\n      <td>0.951795</td>\n      <td>0.945977</td>\n      <td>0.948877</td>\n      <td>0.942328</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.147500</td>\n      <td>0.168243</td>\n      <td>0.952603</td>\n      <td>0.946308</td>\n      <td>0.949445</td>\n      <td>0.942634</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.143000</td>\n      <td>0.171045</td>\n      <td>0.952563</td>\n      <td>0.945482</td>\n      <td>0.949009</td>\n      <td>0.942175</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.144400</td>\n      <td>0.171684</td>\n      <td>0.953546</td>\n      <td>0.946142</td>\n      <td>0.949830</td>\n      <td>0.942787</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.143200</td>\n      <td>0.170141</td>\n      <td>0.953554</td>\n      <td>0.946308</td>\n      <td>0.949917</td>\n      <td>0.942940</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.96      0.93      0.94      5197\n        B-AC       0.66      0.87      0.75       563\n        B-LF       0.70      0.64      0.67       290\n        I-LF       0.76      0.76      0.76       487\n\n    accuracy                           0.90      6537\n   macro avg       0.77      0.80      0.78      6537\nweighted avg       0.91      0.90      0.90      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.96      0.94      0.95      5197\n        B-AC       0.78      0.83      0.80       563\n        B-LF       0.71      0.75      0.73       290\n        I-LF       0.75      0.82      0.78       487\n\n    accuracy                           0.92      6537\n   macro avg       0.80      0.84      0.82      6537\nweighted avg       0.92      0.92      0.92      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.93      0.95      5197\n        B-AC       0.78      0.82      0.80       563\n        B-LF       0.68      0.79      0.73       290\n        I-LF       0.70      0.87      0.78       487\n\n    accuracy                           0.91      6537\n   macro avg       0.78      0.85      0.81      6537\nweighted avg       0.92      0.91      0.91      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.94      0.95      5197\n        B-AC       0.82      0.82      0.82       563\n        B-LF       0.74      0.76      0.75       290\n        I-LF       0.71      0.89      0.79       487\n\n    accuracy                           0.92      6537\n   macro avg       0.81      0.85      0.83      6537\nweighted avg       0.92      0.92      0.92      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.94      0.97      0.96      5197\n        B-AC       0.90      0.67      0.77       563\n        B-LF       0.75      0.74      0.75       290\n        I-LF       0.84      0.77      0.80       487\n\n    accuracy                           0.92      6537\n   macro avg       0.86      0.79      0.82      6537\nweighted avg       0.92      0.92      0.92      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.93      0.95      5197\n        B-AC       0.74      0.90      0.82       563\n        B-LF       0.68      0.84      0.76       290\n        I-LF       0.77      0.89      0.83       487\n\n    accuracy                           0.92      6537\n   macro avg       0.79      0.89      0.84      6537\nweighted avg       0.93      0.92      0.92      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.85      0.85      0.85       563\n        B-LF       0.75      0.84      0.79       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.84      0.88      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.86      0.84      0.85       563\n        B-LF       0.74      0.83      0.78       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.84      0.88      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.87      0.85      0.86       563\n        B-LF       0.76      0.81      0.79       290\n        I-LF       0.77      0.88      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.84      0.88      0.86      6537\nweighted avg       0.94      0.93      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.86      0.87      0.86       563\n        B-LF       0.76      0.79      0.77       290\n        I-LF       0.77      0.91      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.84      0.88      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.95      0.96      0.96      5197\n        B-AC       0.89      0.72      0.80       563\n        B-LF       0.77      0.81      0.79       290\n        I-LF       0.77      0.86      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.85      0.84      0.84      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.87      0.80      0.83       563\n        B-LF       0.77      0.80      0.78       290\n        I-LF       0.73      0.94      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.84      0.87      0.85      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.85      0.85      0.85       563\n        B-LF       0.77      0.79      0.78       290\n        I-LF       0.76      0.89      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.84      0.87      0.85      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.84      0.88      0.86       563\n        B-LF       0.76      0.78      0.77       290\n        I-LF       0.77      0.85      0.81       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.87      0.85      6537\nweighted avg       0.93      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.78      0.92      0.84       563\n        B-LF       0.79      0.79      0.79       290\n        I-LF       0.79      0.87      0.83       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.88      0.86      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.84      0.85       563\n        B-LF       0.79      0.84      0.82       290\n        I-LF       0.80      0.90      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.96      0.97      0.96      5197\n        B-AC       0.89      0.83      0.86       563\n        B-LF       0.81      0.78      0.79       290\n        I-LF       0.79      0.84      0.82       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.85      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.95      0.96      5197\n        B-AC       0.82      0.91      0.86       563\n        B-LF       0.80      0.80      0.80       290\n        I-LF       0.79      0.86      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.88      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.97      5197\n        B-AC       0.84      0.91      0.87       563\n        B-LF       0.78      0.83      0.81       290\n        I-LF       0.78      0.89      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.86      0.87      0.87       563\n        B-LF       0.74      0.81      0.77       290\n        I-LF       0.73      0.92      0.82       487\n\n    accuracy                           0.93      6537\n   macro avg       0.83      0.89      0.85      6537\nweighted avg       0.94      0.93      0.93      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.90      0.84      0.87       563\n        B-LF       0.78      0.81      0.79       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.87      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.96      0.97      0.96      5197\n        B-AC       0.91      0.79      0.85       563\n        B-LF       0.84      0.76      0.80       290\n        I-LF       0.81      0.82      0.82       487\n\n    accuracy                           0.94      6537\n   macro avg       0.88      0.84      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.85      0.89      0.87       563\n        B-LF       0.79      0.79      0.79       290\n        I-LF       0.83      0.81      0.82       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.86      0.86      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.89      0.88       563\n        B-LF       0.79      0.81      0.80       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.90      0.83      0.86       563\n        B-LF       0.78      0.82      0.80       290\n        I-LF       0.80      0.90      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.90      0.88       563\n        B-LF       0.81      0.81      0.81       290\n        I-LF       0.80      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.87      0.89      0.88      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.85      0.87       563\n        B-LF       0.82      0.82      0.82       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.87      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.96      0.97      5197\n        B-AC       0.85      0.91      0.88       563\n        B-LF       0.82      0.82      0.82       290\n        I-LF       0.79      0.91      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.90      0.88      6537\nweighted avg       0.95      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.89      0.87      0.88       563\n        B-LF       0.78      0.83      0.80       290\n        I-LF       0.80      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.97      0.97      5197\n        B-AC       0.91      0.85      0.88       563\n        B-LF       0.81      0.81      0.81       290\n        I-LF       0.82      0.86      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.87      0.87      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.97      5197\n        B-AC       0.89      0.87      0.88       563\n        B-LF       0.74      0.84      0.79       290\n        I-LF       0.79      0.91      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.90      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.91      0.89       563\n        B-LF       0.77      0.83      0.80       290\n        I-LF       0.81      0.87      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.90      0.89       563\n        B-LF       0.80      0.81      0.80       290\n        I-LF       0.81      0.85      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.83      0.93      0.88       563\n        B-LF       0.79      0.80      0.80       290\n        I-LF       0.79      0.90      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.90      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.95      0.96      5197\n        B-AC       0.86      0.92      0.89       563\n        B-LF       0.76      0.83      0.79       290\n        I-LF       0.78      0.89      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.84      0.90      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.90      0.87      0.88       563\n        B-LF       0.77      0.81      0.79       290\n        I-LF       0.79      0.91      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.89      0.89      0.89       563\n        B-LF       0.77      0.80      0.79       290\n        I-LF       0.79      0.87      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.90      0.88      0.89       563\n        B-LF       0.78      0.83      0.80       290\n        I-LF       0.81      0.86      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.88      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.90      0.87      0.88       563\n        B-LF       0.79      0.81      0.80       290\n        I-LF       0.79      0.90      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.91      0.90       563\n        B-LF       0.77      0.82      0.80       290\n        I-LF       0.80      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.88      6537\nweighted avg       0.95      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.90      0.88       563\n        B-LF       0.78      0.83      0.81       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.96      0.97      5197\n        B-AC       0.87      0.90      0.89       563\n        B-LF       0.79      0.84      0.81       290\n        I-LF       0.80      0.90      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.90      0.88      6537\nweighted avg       0.95      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.98      0.96      0.97      5197\n        B-AC       0.89      0.89      0.89       563\n        B-LF       0.80      0.83      0.81       290\n        I-LF       0.81      0.91      0.86       487\n\n    accuracy                           0.95      6537\n   macro avg       0.87      0.90      0.88      6537\nweighted avg       0.95      0.95      0.95      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.90      0.88       563\n        B-LF       0.79      0.82      0.80       290\n        I-LF       0.80      0.87      0.83       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.90      0.85      0.88       563\n        B-LF       0.79      0.80      0.80       290\n        I-LF       0.81      0.88      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.87      0.87      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.89      0.88       563\n        B-LF       0.79      0.82      0.81       290\n        I-LF       0.79      0.90      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.89      0.88       563\n        B-LF       0.78      0.81      0.79       290\n        I-LF       0.79      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.96      5197\n        B-AC       0.86      0.89      0.88       563\n        B-LF       0.77      0.82      0.80       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.87      0.89      0.88       563\n        B-LF       0.76      0.82      0.79       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.85      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.89      0.88       563\n        B-LF       0.78      0.81      0.80       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.89      0.88       563\n        B-LF       0.79      0.81      0.80       290\n        I-LF       0.81      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.89      0.89       563\n        B-LF       0.79      0.81      0.80       290\n        I-LF       0.81      0.89      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.88      0.89      0.89       563\n        B-LF       0.78      0.82      0.80       290\n        I-LF       0.80      0.89      0.84       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.89      0.89      0.89       563\n        B-LF       0.78      0.82      0.80       290\n        I-LF       0.80      0.89      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.87      6537\nweighted avg       0.94      0.94      0.94      6537\n\n              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.89      0.89      0.89       563\n        B-LF       0.79      0.82      0.80       290\n        I-LF       0.80      0.89      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.88      6537\nweighted avg       0.94      0.94      0.94      6537\n\n","output_type":"stream"},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5536, training_loss=0.19065164322453426, metrics={'train_runtime': 512.8642, 'train_samples_per_second': 43.177, 'train_steps_per_second': 10.794, 'total_flos': 1206512638087008.0, 'train_loss': 0.19065164322453426, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"best_trainer.model.save_pretrained(\"model_saves/BERT_best_save\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T14:23:06.489428Z","iopub.execute_input":"2024-04-22T14:23:06.489868Z","iopub.status.idle":"2024-04-22T14:23:07.215909Z","shell.execute_reply.started":"2024-04-22T14:23:06.489835Z","shell.execute_reply":"2024-04-22T14:23:07.214815Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"best_metrics = best_trainer.evaluate()\nprint(best_metrics)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T14:23:08.609902Z","iopub.execute_input":"2024-04-22T14:23:08.610272Z","iopub.status.idle":"2024-04-22T14:23:10.444858Z","shell.execute_reply.started":"2024-04-22T14:23:08.610245Z","shell.execute_reply":"2024-04-22T14:23:10.443946Z"},"trusted":true},"execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [126/126 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         B-O       0.97      0.96      0.97      5197\n        B-AC       0.89      0.89      0.89       563\n        B-LF       0.79      0.82      0.80       290\n        I-LF       0.80      0.89      0.85       487\n\n    accuracy                           0.94      6537\n   macro avg       0.86      0.89      0.88      6537\nweighted avg       0.94      0.94      0.94      6537\n\n{'eval_loss': 0.1702021062374115, 'eval_precision': 0.9535541867820876, 'eval_recall': 0.946307616058153, 'eval_f1': 0.9499170812603648, 'eval_accuracy': 0.9429401866299526, 'eval_runtime': 1.8242, 'eval_samples_per_second': 69.07, 'eval_steps_per_second': 69.07, 'epoch': 2.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\nshutil.rmtree(\"/kaggle/working/bert-base-uncased-finetuned-best-ner\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}