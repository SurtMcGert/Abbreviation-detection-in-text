{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Install dependencies\n","%pip install -q -U ipywidgets transformers tqdm\n","%pip install -q -U seqeval\n","%pip install -q -U accelerate\n","%pip install -q -U transformers[torch]\n","%pip install -q --upgrade -U torch torchvision torchaudio torchtext\n","%pip install -q dill==0.3.1.1\n","%pip install -q numpy==1.14.3\n","%pip install -q pyarrow==0.3.8\n","%pip install -q multiprocess==0.70.16\n","%pip install -q -U datasets==2.6.0\n","%pip install -q fsspec==2023.9.2\n","%pip install -q optuna\n","%pip install -q torch\n","%pip install -q datasets\n","%pip install -q collections\n","%pip install -q nltk"]},{"cell_type":"code","execution_count":188,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T10:57:08.363765Z","iopub.status.busy":"2024-04-22T10:57:08.362912Z","iopub.status.idle":"2024-04-22T10:57:26.484562Z","shell.execute_reply":"2024-04-22T10:57:26.483524Z","shell.execute_reply.started":"2024-04-22T10:57:08.363727Z"},"trusted":true},"outputs":[],"source":["import datasets\n","import torch\n","import torchtext\n","from datasets import load_dataset, Features, Value\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","import os\n","import nltk\n","import subprocess\n","from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, matthews_corrcoef\n","from transformers import AutoTokenizer\n","import transformers\n","from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n","import dill\n","from transformers import DataCollatorForTokenClassification\n","from datasets import load_metric\n","import numpy as np\n","import gc\n","import torch.nn as nn\n","from datasets import DatasetDict, Dataset\n","from transformers import BertForTokenClassification\n","from transformers import BertTokenizerFast\n","from optuna import create_study, Trial\n"]},{"cell_type":"markdown","metadata":{},"source":["## Set Seed and CUDA"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T10:59:56.864932Z","iopub.status.busy":"2024-04-22T10:59:56.864213Z","iopub.status.idle":"2024-04-22T10:59:56.869630Z","shell.execute_reply":"2024-04-22T10:59:56.868683Z","shell.execute_reply.started":"2024-04-22T10:59:56.864898Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.18.0\n"]}],"source":["print(datasets.__version__)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T10:59:58.933976Z","iopub.status.busy":"2024-04-22T10:59:58.933039Z","iopub.status.idle":"2024-04-22T10:59:58.941855Z","shell.execute_reply":"2024-04-22T10:59:58.940766Z","shell.execute_reply.started":"2024-04-22T10:59:58.933942Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["PyTorch Version:  2.2.1+cu121\n","torchtext Version:  0.17.1+cpu\n","Using GPU.\n"]}],"source":["SEED = 1234\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","print(\"PyTorch Version: \", torch.__version__)\n","print(\"torchtext Version: \", torchtext.__version__)\n","print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")"]},{"cell_type":"markdown","metadata":{},"source":["## Data Prep"]},{"cell_type":"markdown","metadata":{},"source":["### Download the Dataset\n","this will download the huggingface dataset ready for use"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:00:01.405932Z","iopub.status.busy":"2024-04-22T11:00:01.405575Z","iopub.status.idle":"2024-04-22T11:01:48.334013Z","shell.execute_reply":"2024-04-22T11:01:48.333091Z","shell.execute_reply.started":"2024-04-22T11:00:01.405905Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2e1df449093c4b3eb5d68c72d377746c","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/8.37k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Downloading data: 100%|██████████| 188k/188k [00:00<00:00, 643kB/s]\n","Downloading data: 100%|██████████| 28.4k/28.4k [00:00<00:00, 90.1kB/s]\n","Downloading data: 100%|██████████| 28.7k/28.7k [00:00<00:00, 142kB/s]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09f92c8ecc244bf7813490d66097c757","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/1072 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"61043bb0d490430084ed4d5dfbb0c5ce","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/126 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"656235e9fe40410e9ed12064a681d1e8","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/153 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dataset = load_dataset(\"surrey-nlp/PLOD-CW\", cache_dir=None, download_mode=\"force_redownload\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:03:42.696105Z","iopub.status.busy":"2024-04-22T11:03:42.695261Z","iopub.status.idle":"2024-04-22T11:03:42.701028Z","shell.execute_reply":"2024-04-22T11:03:42.700043Z","shell.execute_reply.started":"2024-04-22T11:03:42.696068Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'datasets.dataset_dict.DatasetDict'>\n"]}],"source":["print(type(dataset))"]},{"cell_type":"markdown","metadata":{},"source":["### Get Label List\n","this gets the list of labels for the dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:03:45.231827Z","iopub.status.busy":"2024-04-22T11:03:45.231159Z","iopub.status.idle":"2024-04-22T11:03:45.236939Z","shell.execute_reply":"2024-04-22T11:03:45.235930Z","shell.execute_reply.started":"2024-04-22T11:03:45.231795Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['B-O', 'B-AC', 'B-LF', 'I-LF']\n"]}],"source":["label_list = ['B-O', 'B-AC', 'B-LF', 'I-LF']\n","print(label_list)"]},{"cell_type":"markdown","metadata":{},"source":["### Split The Set Into Train, Val and Test Sets"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:03:47.196519Z","iopub.status.busy":"2024-04-22T11:03:47.195391Z","iopub.status.idle":"2024-04-22T11:03:47.277073Z","shell.execute_reply":"2024-04-22T11:03:47.276102Z","shell.execute_reply.started":"2024-04-22T11:03:47.196484Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["train size: 1072\n","val size: 126\n","test size: 153\n","Counter({'B-O': 32971, 'I-LF': 3231, 'B-AC': 2336, 'B-LF': 1462})\n"]}],"source":["train = dataset['train']\n","print(f\"train size: {len(train)}\")\n","val = dataset['validation']\n","print(f\"val size: {len(val)}\")\n","test = dataset['test']\n","print(f\"test size: {len(test)}\")\n","\n","def flatten(A):\n","    rt = []\n","    for i in A:\n","        if isinstance(i,list): rt.extend(flatten(i))\n","        else: rt.append(i)\n","    return rt\n","\n","from collections import Counter\n","flat = flatten(train[\"ner_tags\"])\n","print(Counter(flat))"]},{"cell_type":"markdown","metadata":{},"source":["## Data visualization\n","Here I visualize the dataset to be used for this course work and analyse its features"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def analyze_nlp_dataset(data, output_folder):\n","  \"\"\"\n","  Analyzes and visualizes an NLP dataset with tokens, POS tags, and NER tags.\n","\n","  Args:\n","      data: A dictionary containing separate lists for tokens, POS tags, and NER tags.\n","          - data[\"tokens\"]: A list of lists of tokens.\n","          - data[\"pos_tags\"]: A list of lists of POS tags.\n","          - data[\"ner_tags\"]: A list of lists of NER tags.\n","      output_folder: The folder path to save generated plots.\n","  \"\"\"\n","\n","  try:\n","    os.mkdir(output_folder)\n","  except FileExistsError:\n","    pass  # Folder already exists, continue\n","\n","  # POS Tag Analysis\n","  all_pos_tags = [pos_tag for row in data[\"pos_tags\"] for pos_tag in row]\n","  pos_tag_counts = Counter(all_pos_tags)\n","\n","  # Plot POS tag distribution\n","  plt.figure(figsize=(8, 6))\n","  plt.pie(pos_tag_counts.values(), labels=pos_tag_counts.keys(), autopct=\"%1.1f%%\")\n","  plt.title(\"POS Tag Distribution\")\n","  plt.savefig(f\"{output_folder}/pos_tag_distribution.png\")\n","  plt.close()\n","\n","  # NER Tag Analysis\n","  all_ner_tags = [ner_tag for row in data[\"ner_tags\"] for ner_tag in row]\n","  ner_tag_counts = Counter(all_ner_tags)\n","\n","  # Plot NER tag distribution (if any named entities exist)\n","  if ner_tag_counts:\n","    plt.figure(figsize=(8, 6))\n","    plt.bar(ner_tag_counts.keys(), ner_tag_counts.values())\n","    plt.xlabel(\"NER Tag\")\n","    plt.ylabel(\"Frequency\")\n","    plt.title(\"NER Tag Distribution\")\n","    plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels for better readability\n","    plt.tight_layout()\n","    plt.savefig(f\"{output_folder}/ner_tag_distribution.png\")\n","    plt.close()\n","  else:\n","    print(\"No named entity tags found in the data for NER tag analysis.\")\n","\n","  # Analysis of POS tags within NER tags\n","  pos_in_ner_tags = {}\n","  for tokens, pos_tags, ner_tags in zip(data[\"tokens\"], data[\"pos_tags\"], data[\"ner_tags\"]):\n","    for token, pos_tag, ner_tag in zip(tokens, pos_tags, ner_tags):\n","      if ner_tag and ner_tag != \"O\":  # Consider only named entity tags (excluding \"O\")\n","        pos_in_ner_tags.setdefault(ner_tag, []).append(pos_tag)\n","\n","  # Calculate POS tag proportions within each NER tag (if data exists)\n","  if pos_in_ner_tags:\n","    for ner_tag, pos_tag_list in pos_in_ner_tags.items():\n","      pos_tag_counts_in_ner = Counter(pos_tag_list)\n","      total_count = sum(pos_tag_counts_in_ner.values())\n","      pos_in_ner_tags[ner_tag] = {tag: count / total_count for tag, count in pos_tag_counts_in_ner.items()}\n","\n","  # Print insights from POS tags within NER tags analysis (optional)\n","  if pos_in_ner_tags:\n","    print(\"\\nInsights from POS tags within NER tags:\")\n","    for ner_tag, pos_tag_proportions in pos_in_ner_tags.items():\n","      print(f\"- NER Tag: {ner_tag}\")\n","      for pos_tag, proportion in pos_tag_proportions.items():\n","        print(f\"  - Proportion of {pos_tag}: {proportion:.2f}\")\n","\n","\n","    # Visualize POS tags within NER tags (if data exists)\n","  if pos_in_ner_tags:\n","    for ner_tag, pos_tag_proportions in pos_in_ner_tags.items():\n","      plt.figure(figsize=(8, 6))\n","      plt.bar(pos_tag_proportions.keys(), pos_tag_proportions.values())\n","      plt.xlabel(\"POS Tag\")\n","      plt.ylabel(\"Proportion\")\n","      plt.title(f\"POS Tag Proportions within NER Tag: {ner_tag}\")\n","      plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels for better readability\n","      plt.tight_layout()\n","      plt.savefig(f\"{output_folder}/pos_in_ner_{ner_tag}.png\")\n","      plt.close()\n","\n","  print(\"Analysis complete. Plots saved to\", output_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["analyze_nlp_dataset(train, \"train_set_analysis\")\n","analyze_nlp_dataset(val, \"val_set_analysis\")\n","analyze_nlp_dataset(test, \"test_set_analysis\")"]},{"cell_type":"markdown","metadata":{},"source":["## Data Pre-Processing"]},{"cell_type":"markdown","metadata":{},"source":["### Lemmatization"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:03:53.851973Z","iopub.status.busy":"2024-04-22T11:03:53.851030Z","iopub.status.idle":"2024-04-22T11:03:54.245441Z","shell.execute_reply":"2024-04-22T11:03:54.244294Z","shell.execute_reply.started":"2024-04-22T11:03:53.851939Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\harry\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["\n","is_kaggle = (\n","    \"KAGGLE_CLOUD\" in os.environ or \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",")\n","\n","if is_kaggle:\n","    # Download and unzip wordnet\n","    try:\n","        nltk.data.find('wordnet.zip')\n","    except:\n","        nltk.download('wordnet', download_dir='/kaggle/working/')\n","        command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n","        subprocess.run(command.split())\n","        nltk.data.path.append('/kaggle/working/')\n","\n","    # Now you can import the NLTK resources as usual\n","    from nltk.corpus import wordnet\n","else:\n","    nltk.download('wordnet')\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["the `combine_lists_elementwise` function turns two lists into one, pairing each element elementwise, maintaining the shape of the original list"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:03:58.911131Z","iopub.status.busy":"2024-04-22T11:03:58.910187Z","iopub.status.idle":"2024-04-22T11:03:58.917845Z","shell.execute_reply":"2024-04-22T11:03:58.916763Z","shell.execute_reply.started":"2024-04-22T11:03:58.911093Z"},"trusted":true},"outputs":[],"source":["def combine_lists_elementwise(list_A, list_B):\n","  \"\"\"\n","  Combines two 2D lists of strings element-wise into a 2D list of tuples.\n","\n","  Args:\n","      list_A: A 2D list of strings (e.g., [['A', 'A', 'A'], ['A', 'A', 'A']]).\n","      list_B: Another 2D list of strings with the same dimensions as list_A.\n","\n","  Returns:\n","      A 2D list of tuples, where each tuple combines corresponding elements from list_A and list_B.\n","\n","  Raises:\n","      ValueError: If the dimensions of list_A and list_B don't match.\n","  \"\"\"\n","\n","  # Check if dimensions match\n","  if len(list_A) != len(list_B) or len(list_A[0]) != len(list_B[0]):\n","    raise ValueError(\"Dimensions of lists A and B must be equal.\")\n","\n","  # Create the resulting list using list comprehension\n","  return [[(a, b) for a, b in zip(row_a, row_b)] for row_a, row_b in zip(list_A, list_B)]"]},{"cell_type":"markdown","metadata":{},"source":["the nltk lemmatize function takes a certain format for POS_tags so the `convert_pos_tag` maps a POS_tag from the dataset, to one in the required format. Its important to note that alot of data is lost due to the simplicity of the nltk lemmatize function"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:04:00.574287Z","iopub.status.busy":"2024-04-22T11:04:00.573914Z","iopub.status.idle":"2024-04-22T11:04:00.581302Z","shell.execute_reply":"2024-04-22T11:04:00.580256Z","shell.execute_reply.started":"2024-04-22T11:04:00.574261Z"},"trusted":true},"outputs":[],"source":["def convert_pos_tag(nltk_tag):\n","    \"\"\"\n","    Converts NLTK POS tags to the format expected by the lemmatizer.\n","\n","    Args:\n","        nltk_tag: The POS tag in NLTK format (e.g., VBG, NNS).\n","\n","    Returns:\n","        The corresponding POS tag for the lemmatizer (n, v, a, r, or s) or None if no match.\n","    \"\"\"\n","\n","    tag_map = {\n","        'NUM': '',  # Number (not handled by lemmatizer)\n","        'CCONJ': '',  # Coordinating conjunction (not handled)\n","        'PRON': '',  # Pronoun (not handled)\n","        'NOUN': 'n',   # Noun\n","        'SCONJ': '',  # Subordinating conjunction (not handled)\n","        'SYM': '',   # Symbol (not handled)\n","        'INTJ': '',  # Interjection (not handled)\n","        'ADJ': 'a',    # Adjective\n","        'ADP': '',   # Preposition (not handled)\n","        'PUNCT': '',  # Punctuation (not handled)\n","        'ADV': 'r',    # Adverb\n","        'AUX': 'v',    # Auxiliary verb\n","        'DET': '',   # Determiner (not handled)\n","        'VERB': 'v',   # Verb\n","        'X': '',      # Other (not handled)\n","        'PART': '',   # Particle (not handled)\n","        'PROPN': 'n',   # Proper noun\n","    }\n","    return tag_map.get(nltk_tag)"]},{"cell_type":"markdown","metadata":{},"source":["the `lemmatize_list` function takes the tokens and their respective pos_tags and lemmatizes the tokens"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:04:02.501255Z","iopub.status.busy":"2024-04-22T11:04:02.500473Z","iopub.status.idle":"2024-04-22T11:04:02.507499Z","shell.execute_reply":"2024-04-22T11:04:02.506469Z","shell.execute_reply.started":"2024-04-22T11:04:02.501222Z"},"trusted":true},"outputs":[],"source":["def lemmatize_list(data, pos_tags):\n","    \"\"\"\n","    Lemmatizes a 2D list of tokens using NLTK.\n","\n","    Args:\n","        data: A 2D list of strings (tokens) to be lemmatized.\n","\n","    Returns:\n","        A 2D list containing the lemmatized tokens.\n","    \"\"\"\n","\n","    # Initialize the WordNet lemmatizer\n","    lemmatizer = nltk.WordNetLemmatizer()\n","\n","    pos_tags = [[convert_pos_tag(tag) for tag in row] for row in pos_tags]\n","\n","    data = combine_lists_elementwise(data, pos_tags)\n","\n","\n","    # Lemmatize with part-of-speech information\n","    lemmatized_data = [[token if pos == '' else lemmatizer.lemmatize(token, pos) for token, pos in row] for row in data]\n","\n","    return lemmatized_data"]},{"cell_type":"markdown","metadata":{},"source":["### Pre-Processing Pipeline\n","the `pre_process_data` function applies lemmatization and lowercase to the given data"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:04:14.200954Z","iopub.status.busy":"2024-04-22T11:04:14.200310Z","iopub.status.idle":"2024-04-22T11:04:14.206445Z","shell.execute_reply":"2024-04-22T11:04:14.205105Z","shell.execute_reply.started":"2024-04-22T11:04:14.200923Z"},"trusted":true},"outputs":[],"source":["def pre_process_data(tokens, pos_tags):\n","    # lemmatize the data\n","    data = lemmatize_list(tokens, pos_tags)\n","    # lowercase the data\n","    data = [[string.lower() for string in row] for row in data]\n","    return data"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:04:16.771525Z","iopub.status.busy":"2024-04-22T11:04:16.770829Z","iopub.status.idle":"2024-04-22T11:04:19.449554Z","shell.execute_reply":"2024-04-22T11:04:19.448625Z","shell.execute_reply.started":"2024-04-22T11:04:16.771491Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["original train tokens: ['For', 'this', 'purpose', 'the', 'Gothenburg', 'Young', 'Persons', 'Empowerment', 'Scale', '(', 'GYPES', ')', 'was', 'developed', '.']\n","pre-processed train tokens: ['for', 'this', 'purpose', 'the', 'gothenburg', 'young', 'persons', 'empowerment', 'scale', '(', 'gypes', ')', 'be', 'develop', '.']\n","original val tokens: ['=', 'Manual', 'Ability', 'Classification', 'System', ';', 'QUEST', '=', 'Quest', '-', 'Quality', 'of', 'upper', 'extremity', 'skills', 'test', ';', 'Cont', '=', 'control', ';', 'M', '=', 'male', ',', 'F', '=', 'female', ',', 'V', '=', 'verbal', ',', 'nonV', '=', 'non', '-', 'Verbal', ',', '|Quad', '=', 'quadriplegia', ',', 'Di', '=', 'Diplegia', ',', 'Hemi', '=', 'hemiplegia', '.']\n","pre-processed val tokens: ['=', 'manual', 'ability', 'classification', 'system', ';', 'quest', '=', 'quest', '-', 'quality', 'of', 'upper', 'extremity', 'skill', 'test', ';', 'cont', '=', 'control', ';', 'm', '=', 'male', ',', 'f', '=', 'female', ',', 'v', '=', 'verbal', ',', 'nonv', '=', 'non', '-', 'verbal', ',', '|quad', '=', 'quadriplegia', ',', 'di', '=', 'diplegia', ',', 'hemi', '=', 'hemiplegia', '.']\n"]}],"source":["from datasets import DatasetDict, Dataset\n","train_tokens = pre_process_data(train[\"tokens\"], train[\"pos_tags\"])\n","val_tokens = pre_process_data(val[\"tokens\"], val[\"pos_tags\"])\n","test_tokens = pre_process_data(test[\"tokens\"], test[\"pos_tags\"])\n","original_train_tokens = train[\"tokens\"]\n","original_val_tokens = val[\"tokens\"]\n","print(f\"original train tokens: {original_train_tokens[0]}\\npre-processed train tokens: {train_tokens[0]}\")\n","print(f\"original val tokens: {original_val_tokens[0]}\\npre-processed val tokens: {val_tokens[0]}\")\n","\n","dataset = DatasetDict({\n","    \"train\": Dataset.from_dict({\"tokens\": train_tokens, \"pos_tags\": train[\"pos_tags\"], \"ner_tags\": train[\"ner_tags\"]}),\n","    \"validation\": Dataset.from_dict({\"tokens\": val_tokens, \"pos_tags\": val[\"pos_tags\"], \"ner_tags\": val[\"ner_tags\"]}),\n","    \"test\": Dataset.from_dict({\"tokens\": test_tokens, \"pos_tags\": test[\"pos_tags\"], \"ner_tags\": test[\"ner_tags\"]}),\n","})"]},{"cell_type":"markdown","metadata":{},"source":["### Set Task\n","in this project we are doing Named Entity Recognition so I set the task to \"ner\""]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:04:21.398223Z","iopub.status.busy":"2024-04-22T11:04:21.397378Z","iopub.status.idle":"2024-04-22T11:04:21.402627Z","shell.execute_reply":"2024-04-22T11:04:21.401562Z","shell.execute_reply.started":"2024-04-22T11:04:21.398189Z"},"trusted":true},"outputs":[],"source":["task = \"ner\""]},{"cell_type":"markdown","metadata":{},"source":["# Experiment 1 (Model)\n","HMM vs BERT"]},{"cell_type":"markdown","metadata":{},"source":["## HMM\n","The following is the implementation of an HMM model"]},{"cell_type":"markdown","metadata":{},"source":["### Library Import\n","I am using the nltk library for the HMM implementation"]},{"cell_type":"markdown","metadata":{},"source":["create lists of the sentences and associated tags from the train set"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[],"source":["# sentences = train[:][\"tokens\"]\n","# tags = train[:][\"ner_tags\"]\n","\n","sentences = train_tokens\n","tags = train[:][\"ner_tags\"]"]},{"cell_type":"markdown","metadata":{},"source":["print out an example of the first sentence and its tags"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["sentence: ['for', 'this', 'purpose', 'the', 'gothenburg', 'young', 'persons', 'empowerment', 'scale', '(', 'gypes', ')', 'be', 'develop', '.']\n","tags: ['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O']\n"]}],"source":["print(f\"sentence: {sentences[0]}\")\n","print(f\"tags: {tags[0]}\")"]},{"cell_type":"markdown","metadata":{},"source":["we generate a character set containing all the characters that can be used in the output of the model"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[],"source":["def get_char_set(sentences):\n","    char_set = set()\n","    for sentence in sentences:\n","        for word in sentence:\n","            for char in word:\n","                char_set.add(char)\n","    char_set = list(char_set)\n","    return char_set"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["char_set: ['2', '†', 'è', 'ö', 'y', 'l', 'f', '×', ')', 'ß', '♀', '8', '*', 'μ', '®', '′', 'λ', 'κ', '—', ':', '≥', '-', '”', '→', 'γ', '@', '‘', '?', 'r', '[', 'w', ']', 'ν', '%', 'ó', 'u', 'ε', 'i', 'k', '“', '♂', 'x', '>', \"'\", 'o', '±', '+', 'δ', '\"', '∑', 'q', 'g', '·', 'ü', 'é', '§', 'σ', 'd', 't', '&', 'ú', 'ä', 's', '9', '0', 'θ', '¯', '−', '≤', '5', '7', 'm', 'v', '’', '$', '‒', '´', ';', '3', '•', '…', 'a', 'β', '）', '=', 'c', 'p', 'ï', '}', 'b', '（', '∞', '<', 'α', '4', ',', 'µ', '6', '‡', 'ω', '#', 'å', '(', 'z', '°', '/', 'e', '1', 'j', '.', 'í', 'h', 'ã', '–', 'φ', '_', '{', 'n']\n"]}],"source":["char_set = get_char_set(sentences)\n","print(f\"char_set: {char_set}\")"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[('for', 'B-O'), ('this', 'B-O'), ('purpose', 'B-O'), ('the', 'B-O'), ('gothenburg', 'B-LF'), ('young', 'I-LF'), ('persons', 'I-LF'), ('empowerment', 'I-LF'), ('scale', 'I-LF'), ('(', 'B-O'), ('gypes', 'B-AC'), (')', 'B-O'), ('be', 'B-O'), ('develop', 'B-O'), ('.', 'B-O')]\n"]}],"source":["trainer = nltk.tag.hmm.HiddenMarkovModelTrainer(states=label_list, symbols=char_set)\n","data = combine_lists_elementwise(sentences.copy(), tags.copy())\n","print(data[0])"]},{"cell_type":"markdown","metadata":{},"source":["## BERT\n","\n","The following is the implementation of BERT model"]},{"cell_type":"markdown","metadata":{},"source":["### Tokenizer"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:04:25.570188Z","iopub.status.busy":"2024-04-22T11:04:25.569531Z","iopub.status.idle":"2024-04-22T11:05:00.653321Z","shell.execute_reply":"2024-04-22T11:05:00.652414Z","shell.execute_reply.started":"2024-04-22T11:04:25.570157Z"},"trusted":true},"outputs":[],"source":["model_checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True) # use AutoTokenizer because it defaults to fast tokenizers where as using the BERT Tokenizer does not\n","assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"]},{"cell_type":"markdown","metadata":{},"source":["I need to map the string tokens to numbers"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:05:13.978972Z","iopub.status.busy":"2024-04-22T11:05:13.978588Z","iopub.status.idle":"2024-04-22T11:05:13.985019Z","shell.execute_reply":"2024-04-22T11:05:13.984092Z","shell.execute_reply.started":"2024-04-22T11:05:13.978941Z"},"trusted":true},"outputs":[],"source":["def encode_tags(tag_sequences, possible_tags):\n","    \"\"\"\n","    Encodes a sequence of string tags into a list of corresponding integer tags.\n","\n","    Args:\n","        tag_sequences: A 2d list of strings representing numerical tags.\n","        possible_tags: A list of strings representing the possible textual labels.\n","\n","    Returns:\n","        A list of strings representing the decoded textual tags.\n","    \"\"\"\n","\n","    encoded_tags = [[possible_tags.index(tag) for tag in row] for row in tag_sequences]\n","    return encoded_tags"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:05:16.378799Z","iopub.status.busy":"2024-04-22T11:05:16.377916Z","iopub.status.idle":"2024-04-22T11:05:16.386375Z","shell.execute_reply":"2024-04-22T11:05:16.385412Z","shell.execute_reply.started":"2024-04-22T11:05:16.378763Z"},"trusted":true},"outputs":[],"source":["label_all_tokens = True\n","def tokenize_and_align_labels(data):\n","    tokenized_inputs = tokenizer(data[\"tokens\"], truncation=True, is_split_into_words=True) ## For some models, you may need to set max_length to approximately 500.\n","\n","    labels = []\n","    converted_tags = encode_tags(data[f\"{task}_tags\"], label_list)\n","    for i, label in enumerate(converted_tags):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:\n","            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n","            # ignored in the loss function.\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            # We set the label for the first token of each word.\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(label[word_idx])\n","            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n","            # the label_all_tokens flag.\n","            else:\n","                label_ids.append(label[word_idx] if label_all_tokens else -100)\n","            previous_word_idx = word_idx\n","\n","        labels.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:05:19.255083Z","iopub.status.busy":"2024-04-22T11:05:19.254285Z","iopub.status.idle":"2024-04-22T11:05:39.729806Z","shell.execute_reply":"2024-04-22T11:05:39.728693Z","shell.execute_reply.started":"2024-04-22T11:05:19.255025Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84037aaf2dca43df9712a6b36ba77c89","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1072 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a12e60933fa84825bd1671dc8c7b916d","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/126 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ee7d836284e45a8a44601feba54ce6c","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/153 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n","model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"]},{"cell_type":"code","execution_count":25,"metadata":{"trusted":true},"outputs":[],"source":["model_name = model_checkpoint.split(\"/\")[-1]\n","batch_size = 16\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=1000,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=['none'],\n",")"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:06:33.551012Z","iopub.status.busy":"2024-04-22T11:06:33.550548Z","iopub.status.idle":"2024-04-22T11:06:34.591845Z","shell.execute_reply":"2024-04-22T11:06:34.591004Z","shell.execute_reply.started":"2024-04-22T11:06:33.550980Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\harry\\AppData\\Local\\Temp\\ipykernel_8788\\4079666221.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(\"seqeval\")\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n","  warnings.warn(\n"]}],"source":["data_collator = DataCollatorForTokenClassification(tokenizer)\n","metric = load_metric(\"seqeval\")"]},{"cell_type":"code","execution_count":174,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:06:28.150887Z","iopub.status.busy":"2024-04-22T11:06:28.150458Z","iopub.status.idle":"2024-04-22T11:06:28.160233Z","shell.execute_reply":"2024-04-22T11:06:28.159099Z","shell.execute_reply.started":"2024-04-22T11:06:28.150854Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    \n","    overall_results = metric.compute(predictions=true_predictions, references=true_labels)\n","    \n","    true_labels = [item for sublist in true_labels for item in sublist]\n","    true_predictions = [item for sublist in true_predictions for item in sublist]\n","    \n","    \n","    results = classification_report(true_labels, true_predictions, labels = label_list)\n","    print(results)\n","    return {\n","        \"precision\": overall_results[\"overall_precision\"],\n","        \"recall\": overall_results[\"overall_recall\"],\n","        \"f1\": overall_results[\"overall_f1\"],\n","        \"accuracy\": overall_results[\"overall_accuracy\"],\n","    }"]},{"cell_type":"code","execution_count":28,"metadata":{"trusted":true},"outputs":[],"source":["BERTtrainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"markdown","metadata":{},"source":["#### HMM Training"]},{"cell_type":"markdown","metadata":{},"source":["set up the hmm trainer and combine the tokens with their tags"]},{"cell_type":"markdown","metadata":{},"source":["train the model on the data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = trainer.train_supervised(data)"]},{"cell_type":"markdown","metadata":{},"source":["save the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def save_hmm(model, name):\n","    # Open a file for writing in binary mode\n","    with open(name, 'wb') as f:\n","        # Dill can handle more complex objects than pickle\n","        dill.dump(model, f)\n","\n","    print(f\"Model saved as {name}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save_hmm(model, \"hmm_model.dill\")"]},{"cell_type":"markdown","metadata":{},"source":["load the model"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def load_hmm(name):\n","    # Open the saved model file in binary read mode\n","    with open(name, 'rb') as f:\n","        # Load the model back into a variable using dill.load\n","        model = dill.load(f)\n","        print(\"Model loaded successfully!\")\n","    return model"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model loaded successfully!\n"]}],"source":["model = load_hmm(\"hmm_model.dill\")"]},{"cell_type":"markdown","metadata":{},"source":["#### BERT Training"]},{"cell_type":"markdown","metadata":{},"source":["clear the cuda cache to avoid cuda memory issues"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["train bert"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["BERTtrainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["BERTtrainer.model.save_pretrained(\"model_saves/BERT_save\")"]},{"cell_type":"code","execution_count":31,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",")"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["BERTtrainer.model.from_pretrained(\"model_saves/BERT_save\")"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"markdown","metadata":{},"source":["#### HMM evaluation"]},{"cell_type":"code","execution_count":32,"metadata":{"trusted":true},"outputs":[],"source":["def evaluate_hmm(model, test_sentences):\n","    predicted = []\n","    for sentence in test_sentences:\n","        test_result = model.tag(sentence)\n","        out_tags = []\n","        for word, tag in test_result:\n","            out_tags.append(tag)\n","        predicted.append(out_tags)\n","    return predicted"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tag\\hmm.py:336: RuntimeWarning: overflow encountered in cast\n","  O[i, k] = self._output_logprob(si, self._symbols[k])\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tag\\hmm.py:334: RuntimeWarning: overflow encountered in cast\n","  X[i, j] = self._transitions[si].logprob(self._states[j])\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tag\\hmm.py:332: RuntimeWarning: overflow encountered in cast\n","  P[i] = self._priors.logprob(si)\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tag\\hmm.py:364: RuntimeWarning: overflow encountered in cast\n","  O[i, k] = self._output_logprob(si, self._symbols[k])\n"]}],"source":["test_sentences = dataset[\"validation\"][:][\"tokens\"]\n","test_sentences = pre_process_data(test_sentences, dataset[\"validation\"][:][\"pos_tags\"])\n","correct_tags = dataset[\"validation\"][:][\"ner_tags\"]\n","predicted = evaluate_hmm(model, test_sentences)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["number of predictions: 5000\n","number of correct answers: 5000\n","['B-O', 'B-LF', 'I-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF']\n","['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O']\n"]}],"source":["correct_tags = [item for sublist in correct_tags for item in sublist]\n","predicted = [item for sublist in predicted for item in sublist]\n","print(f\"number of predictions: {len(predicted)}\\nnumber of correct answers: {len(correct_tags)}\")\n","\n","print(correct_tags[:100])\n","print(predicted[:100])"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.87      0.99      0.93      4261\n","        B-AC       0.83      0.11      0.19       263\n","        B-LF       0.54      0.10      0.17       149\n","        I-LF       0.72      0.13      0.22       327\n","\n","    accuracy                           0.86      5000\n","   macro avg       0.74      0.33      0.38      5000\n","weighted avg       0.85      0.86      0.82      5000\n","\n"]}],"source":["print(classification_report(correct_tags, predicted, labels = label_list))"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["def encode_tags(tag_sequences, possible_tags):\n","    \"\"\"\n","    Encodes a sequence of textual labels into a list of corresponding numerical tags.\n","\n","    Args:\n","        tag_sequence: A list of integers representing textual tags.\n","        possible_tags: A list of strings representing the possible textual labels.\n","\n","    Returns:\n","        A list of strings representing the encoded numerical tags.\n","    \"\"\"\n","\n","    encoded_tags = [[possible_tags.index(tag) for tag in row] for row in tag_sequences]\n","    return encoded_tags"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"data":{"text/plain":["<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x26f81e7b490>"]},"execution_count":61,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["encoded_correct_tags = encode_tags([correct_tags], label_list)[0]\n","encoded_predicted = encode_tags([predicted], label_list)[0]\n","\n","\n","cm = confusion_matrix(correct_tags, predicted, labels = label_list)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_list)\n","disp.plot() "]},{"cell_type":"markdown","metadata":{},"source":["#### BERT evaluation"]},{"cell_type":"code","execution_count":177,"metadata":{},"outputs":[],"source":["model = AutoModelForTokenClassification.from_pretrained(\"model_saves\\\\BERT_save\", num_labels=len(label_list))\n","BERTtrainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":178,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"60cb9219c63e429babe0c95713932233","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.93      0.95      0.94      5197\n","        B-AC       0.73      0.74      0.74       563\n","        B-LF       0.64      0.25      0.36       290\n","        I-LF       0.60      0.69      0.64       487\n","\n","    accuracy                           0.88      6537\n","   macro avg       0.73      0.66      0.67      6537\n","weighted avg       0.88      0.88      0.87      6537\n","\n","{'eval_loss': 0.3339511752128601, 'eval_precision': 0.887690044139284, 'eval_recall': 0.8970758301668594, 'eval_f1': 0.8923582580115037, 'eval_accuracy': 0.8811381367599816, 'eval_runtime': 2.2275, 'eval_samples_per_second': 56.567, 'eval_steps_per_second': 56.567}\n"]}],"source":["metrics = BERTtrainer.evaluate()\n","print(metrics)"]},{"cell_type":"code","execution_count":179,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8da062a67e7748ce9e269c1972ade254","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.93      0.95      0.94      5197\n","        B-AC       0.73      0.74      0.74       563\n","        B-LF       0.64      0.25      0.36       290\n","        I-LF       0.60      0.69      0.64       487\n","\n","    accuracy                           0.88      6537\n","   macro avg       0.73      0.66      0.67      6537\n","weighted avg       0.88      0.88      0.87      6537\n","\n"]}],"source":["BERT_predicts = BERTtrainer.predict(tokenized_datasets[\"validation\"])"]},{"cell_type":"code","execution_count":187,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.93      0.95      0.94      5197\n","        B-AC       0.73      0.74      0.74       563\n","        B-LF       0.64      0.25      0.36       290\n","        I-LF       0.60      0.69      0.64       487\n","\n","    accuracy                           0.88      6537\n","   macro avg       0.73      0.66      0.67      6537\n","weighted avg       0.88      0.88      0.87      6537\n","\n"]},{"data":{"text/plain":["<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x26faca8fd00>"]},"execution_count":187,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["predictions = BERT_predicts.predictions.argmax(-1)\n","labels = BERT_predicts.label_ids\n","\n","true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","true_predictions = [item for sublist in true_predictions for item in sublist]\n","labels = [item for sublist in true_labels for item in sublist]\n","\n","cm = confusion_matrix(labels, true_predictions, labels=label_list)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_list)\n","disp.plot()"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment 2 (Loss Functions)\n","cross entropy vs MSE\n","This experiment is on BERT as HMM doesnt use loss functions due to its statistical nature rather than being a neural network"]},{"cell_type":"markdown","metadata":{},"source":["## Creating Custom Trainers"]},{"cell_type":"code","execution_count":148,"metadata":{"trusted":true},"outputs":[],"source":["# cross entropy\n","class CustomBERTTrainerCrossEntropy(Trainer):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.loss_fn = nn.CrossEntropyLoss()\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            labels = inputs.pop(\"labels\")\n","        else:\n","            labels = None\n","        outputs = model(**inputs)\n","        if self.args.past_index >= 0:\n","            self._past = outputs[self.args.past_index]\n","\n","        if labels is not None:\n","            loss = self.label_smoother(outputs, labels)\n","        else:\n","            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n","            loss = loss*loss\n","            loss = loss.mean()\n","\n","        return (loss, outputs) if return_outputs else loss\n","    \n","    \n","# MLSML\n","class CustomBERTTrainerMLSML(Trainer):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.loss_fn = nn.MultiLabelSoftMarginLoss()\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            labels = inputs.pop(\"labels\")\n","        else:\n","            labels = None\n","        outputs = model(**inputs)\n","        if self.args.past_index >= 0:\n","            self._past = outputs[self.args.past_index]\n","\n","        if labels is not None:\n","            loss = self.label_smoother(outputs, labels)\n","        else:\n","            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n","            loss = loss*loss\n","            loss = loss.mean()\n","\n","        return (loss, outputs) if return_outputs else loss\n","    \n","    \n","# KLDivLoss\n","class CustomBERTTrainerKLDiv(Trainer):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.loss_fn = nn.KLDivLoss()\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            labels = inputs.pop(\"labels\")\n","        else:\n","            labels = None\n","        outputs = model(**inputs)\n","        if self.args.past_index >= 0:\n","            self._past = outputs[self.args.past_index]\n","\n","        if labels is not None:\n","            loss = self.label_smoother(outputs, labels)\n","        else:\n","            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n","            loss = loss*loss\n","            loss = loss.mean()\n","\n","        return (loss, outputs) if return_outputs else loss\n","    \n","\n","# MSE\n","class CustomBERTTrainerMSE(Trainer):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.loss_fn = nn.MSELoss()\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            labels = inputs.pop(\"labels\")\n","        else:\n","            labels = None\n","        outputs = model(**inputs)\n","        if self.args.past_index >= 0:\n","            self._past = outputs[self.args.past_index]\n","\n","        if labels is not None:\n","            loss = self.label_smoother(outputs, labels)\n","        else:\n","            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n","            loss = loss*loss\n","            loss = loss.mean()\n","\n","        return (loss, outputs) if return_outputs else loss\n","  "]},{"cell_type":"code","execution_count":150,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["args_CE = TrainingArguments(\n","    f\"{model_name}-finetuned-CE-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 10,\n","    logging_steps = 10,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=5,\n","    weight_decay=0.001,\n","    save_steps=1000,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=['tensorboard'],\n",")\n","\n","args_MLSML = TrainingArguments(\n","    f\"{model_name}-finetuned-MLSML-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 10,\n","    logging_steps = 10,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=5,\n","    weight_decay=0.001,\n","    save_steps=1000,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=['tensorboard'],\n",")\n","\n","args_KLDiv = TrainingArguments(\n","    f\"{model_name}-finetuned-KLDiv-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 10,\n","    logging_steps = 10,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=5,\n","    weight_decay=0.001,\n","    save_steps=1000,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=['tensorboard'],\n",")\n","\n","args_MSE = TrainingArguments(\n","    f\"{model_name}-finetuned-MSE-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 10,\n","    logging_steps = 10,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=5,\n","    weight_decay=0.001,\n","    save_steps=1000,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=['tensorboard'],\n",")\n","model_CE = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","model_MLSML = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","model_KLDiv = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","model_MSE = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","\n","BERTtrainer_CE = CustomBERTTrainerCrossEntropy(\n","    model_CE,\n","    args_CE,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_MLSML = CustomBERTTrainerMLSML(\n","    model_MLSML,\n","    args_MLSML,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_KLDiv = CustomBERTTrainerKLDiv(\n","    model_KLDiv,\n","    args_KLDiv,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_MSE = CustomBERTTrainerMSE(\n","    model_MSE,\n","    args_MSE,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["BERTtrainer_CE.train()\n","BERTtrainer_CE.model.save_pretrained(\"/kaggle/working/BERT_CE_save\")\n","BERTtrainer_CE = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","BERTtrainer_MLSML.train()\n","BERTtrainer_MLSML.model.save_pretrained(\"/kaggle/working/BERT_MLSML_save\")\n","BERTtrainer_MLSML = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","BERTtrainer_KLDiv.train()\n","BERTtrainer_KLDiv.model.save_pretrained(\"/kaggle/working/BERT_KLDiv_save\")\n","BERTtrainer_KLDiv = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","BERTtrainer_MSE.train()\n","BERTtrainer_MSE.model.save_pretrained(\"/kaggle/working/BERT_MSE_save\")\n","BERTtrainer_MSE = None\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":151,"metadata":{"trusted":true},"outputs":[],"source":["model_CE = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_CE_save\", num_labels=len(label_list))\n","model_MLSML = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_MLSML_save\", num_labels=len(label_list))\n","model_KLDiv = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_KLDiv_save\", num_labels=len(label_list))\n","model_MSE = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_MSE_save\", num_labels=len(label_list))\n","\n","BERTtrainer_CE = CustomBERTTrainerCrossEntropy(\n","    model_CE,\n","    args_CE,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_MLSML = CustomBERTTrainerMLSML(\n","    model_MLSML,\n","    args_MLSML,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_KLDiv = CustomBERTTrainerKLDiv(\n","    model_KLDiv,\n","    args_KLDiv,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_MSE = CustomBERTTrainerMSE(\n","    model_MSE,\n","    args_MSE,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"code","execution_count":152,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7f71e4236f642e3898ac6b0696a982b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.95      0.96      5197\n","        B-AC       0.81      0.85      0.83       563\n","        B-LF       0.75      0.71      0.73       290\n","        I-LF       0.75      0.85      0.80       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.82      0.84      0.83      6537\n","weighted avg       0.93      0.92      0.92      6537\n","\n","{'eval_loss': 0.11299525201320648, 'eval_precision': 0.9310058187863675, 'eval_recall': 0.9251610771518255, 'eval_f1': 0.9280742459396751, 'eval_accuracy': 0.9229004130335016, 'eval_runtime': 2.2405, 'eval_samples_per_second': 56.237, 'eval_steps_per_second': 56.237}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"987ab65409754571966b5280e1e67a4a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.95      0.96      5197\n","        B-AC       0.81      0.84      0.82       563\n","        B-LF       0.73      0.76      0.74       290\n","        I-LF       0.77      0.84      0.80       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.82      0.85      0.83      6537\n","weighted avg       0.93      0.92      0.92      6537\n","\n","{'eval_loss': 0.10830678045749664, 'eval_precision': 0.93033670592138, 'eval_recall': 0.9266479431686767, 'eval_f1': 0.9284886608177454, 'eval_accuracy': 0.9229004130335016, 'eval_runtime': 2.3795, 'eval_samples_per_second': 52.951, 'eval_steps_per_second': 52.951}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e1c7bc7b90741c39a426cd42b3984a4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.94      0.95      5197\n","        B-AC       0.78      0.85      0.81       563\n","        B-LF       0.72      0.75      0.74       290\n","        I-LF       0.76      0.85      0.80       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.81      0.85      0.83      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","{'eval_loss': 0.11526535451412201, 'eval_precision': 0.9276567437219359, 'eval_recall': 0.9215265157773005, 'eval_f1': 0.9245814685894249, 'eval_accuracy': 0.9196879302432308, 'eval_runtime': 3.9611, 'eval_samples_per_second': 31.81, 'eval_steps_per_second': 31.81}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8f3c551c2954d5a9ed59b696fa1067b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.81      0.85      0.83       563\n","        B-LF       0.76      0.74      0.75       290\n","        I-LF       0.77      0.86      0.81       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.85      0.84      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","{'eval_loss': 0.1157521978020668, 'eval_precision': 0.9345514950166113, 'eval_recall': 0.9294564678671733, 'eval_f1': 0.9319970181396504, 'eval_accuracy': 0.9267247973076335, 'eval_runtime': 2.3528, 'eval_samples_per_second': 53.554, 'eval_steps_per_second': 53.554}\n"]}],"source":["CE_eval = BERTtrainer_CE.evaluate()\n","print(CE_eval)\n","MLSML_eval = BERTtrainer_MLSML.evaluate()\n","print(MLSML_eval)\n","KLDiv_eval = BERTtrainer_KLDiv.evaluate()\n","print(KLDiv_eval)\n","MSE_eval = BERTtrainer_MSE.evaluate()\n","print(MSE_eval)"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment 3 (Additional Training Samples from Optional Dataset)"]},{"cell_type":"markdown","metadata":{},"source":["lemmatization with Bag of Words VS Word2Vec"]},{"cell_type":"markdown","metadata":{},"source":["## Collecting Dataset"]},{"cell_type":"code","execution_count":189,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:06:43.863339Z","iopub.status.busy":"2024-04-22T11:06:43.862946Z","iopub.status.idle":"2024-04-22T11:08:44.588017Z","shell.execute_reply":"2024-04-22T11:08:44.587088Z","shell.execute_reply.started":"2024-04-22T11:06:43.863306Z"},"trusted":true},"outputs":[],"source":["filtered_dataset = load_dataset(\"surrey-nlp/PLOD-filtered\")"]},{"cell_type":"code","execution_count":190,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:11:10.932307Z","iopub.status.busy":"2024-04-22T11:11:10.931366Z","iopub.status.idle":"2024-04-22T11:11:10.938337Z","shell.execute_reply":"2024-04-22T11:11:10.937188Z","shell.execute_reply.started":"2024-04-22T11:11:10.932273Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["train size: 112652\n","val size: 24140\n","test size: 24140\n"]}],"source":["filtered_train = filtered_dataset[\"train\"]\n","print(f\"train size: {len(filtered_train)}\")\n","filtered_val = filtered_dataset[\"validation\"]\n","print(f\"val size: {len(filtered_val)}\")\n","filtered_test = filtered_dataset[\"test\"]\n","print(f\"test size: {len(filtered_test)}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Extracting Data to Use\n","I will have three tests, using three sizes of data acquired from the filtered dataset.\n","\n","small: 1072 extra samples to double the dataset size  \n","medium: 10000 extra samples  \n","large: 50000 extra samples"]},{"cell_type":"code","execution_count":191,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:11:22.171865Z","iopub.status.busy":"2024-04-22T11:11:22.170943Z","iopub.status.idle":"2024-04-22T11:11:22.182548Z","shell.execute_reply":"2024-04-22T11:11:22.181597Z","shell.execute_reply.started":"2024-04-22T11:11:22.171820Z"},"trusted":true},"outputs":[],"source":["def decode_tags(tag_sequences, possible_tags):\n","    \"\"\"\n","    Decodes a sequence of numerical tags into a list of corresponding textual labels.\n","\n","    Args:\n","        tag_sequence: A list of integers representing numerical tags.\n","        possible_tags: A list of strings representing the possible textual labels.\n","\n","    Returns:\n","        A list of strings representing the decoded textual tags.\n","    \"\"\"\n","\n","    decoded_tags = [[possible_tags[tag] for tag in row] for row in tag_sequences]\n","    return decoded_tags\n","\n","\n","def build_dataset(filtered_set, cw_set, num_of_samples):\n","    \"\"\"\n","    Merges a specified number of rows from a larger list to a smaller list, ensuring no duplicates.\n","\n","    Args:\n","        filtered_set: a split of the filtered dataset\n","        cw_set: a split of the cw dataset\n","        num_of_samples: The number of rows to add from the filtered set.\n","\n","    Returns:\n","        new tokens, pos_tags and ner_tags lists\n","    \"\"\"\n","    # set up the initial lists\n","    tokens = cw_set[\"tokens\"]\n","    pos_tags = cw_set[\"pos_tags\"]\n","    ner_tags = cw_set[\"ner_tags\"]\n","     \n","    # set up the filtered lists\n","    # tokens\n","    filtered_tokens = filtered_set[\"tokens\"]\n","    # pos_tags\n","    filtered_label_list = filtered_set.features[f\"pos_tags\"].feature.names\n","    filtered_pos_tags = decode_tags(filtered_set[\"pos_tags\"], filtered_label_list)\n","    # ner_tags\n","    filtered_label_list = filtered_set.features[f\"ner_tags\"].feature.names\n","    filtered_ner_tags = decode_tags(filtered_set[\"ner_tags\"], filtered_label_list)\n","\n","    # convert the tokens list to sets for efficient duplicate checking\n","    tokens_set = set(tuple(row) for row in tokens)\n","    filtered_tokens_set = set(tuple(row) for row in filtered_tokens)\n","\n","    # find rows to add\n","    rows_to_add = []\n","    for index, row in enumerate(filtered_tokens_set):\n","        if tuple(row) not in tokens_set and len(rows_to_add) < num_of_samples:\n","            rows_to_add.append(index)\n","\n","    # Merge and return the lists\n","    tokens = tokens + [filtered_tokens[i] for i in rows_to_add]\n","    pos_tags = pos_tags + [filtered_pos_tags[i] for i in rows_to_add]\n","    ner_tags = ner_tags + [filtered_ner_tags[i] for i in rows_to_add]\n","\n","    return tokens, pos_tags, ner_tags\n","\n","\n"]},{"cell_type":"code","execution_count":192,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:11:28.422163Z","iopub.status.busy":"2024-04-22T11:11:28.421392Z","iopub.status.idle":"2024-04-22T11:12:35.684269Z","shell.execute_reply":"2024-04-22T11:12:35.683333Z","shell.execute_reply.started":"2024-04-22T11:11:28.422131Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["num of small samples: 2144\n","num of medium samples: 11072\n","num of large samples: 51072\n"]}],"source":["small = 1072\n","medium = 10000\n","large = 50000\n","tokens_small, pos_tags_small, ner_tags_small = build_dataset(filtered_train, train, small)\n","tokens_small = pre_process_data(tokens_small, pos_tags_small)\n","tokens_medium, pos_tags_medium, ner_tags_medium = build_dataset(filtered_train, train, medium)\n","tokens_medium = pre_process_data(tokens_medium, pos_tags_medium)\n","tokens_large, pos_tags_large, ner_tags_large = build_dataset(filtered_train, train, large)\n","tokens_large = pre_process_data(tokens_large, pos_tags_large)\n","print(f\"num of small samples: {len(tokens_small)}\\nnum of medium samples: {len(tokens_medium)}\\nnum of large samples: {len(tokens_large)}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"markdown","metadata":{},"source":["### HMM Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# create the character sets\n","char_set_small = get_char_set(tokens_small)\n","char_set_medium = get_char_set(tokens_medium)\n","char_set_large = get_char_set(tokens_large)\n","\n","# create trainers\n","#small\n","trainer_small = nltk.tag.hmm.HiddenMarkovModelTrainer(states=label_list, symbols=char_set_small)\n","data_small = combine_lists_elementwise(tokens_small.copy(), ner_tags_small.copy())\n","# medium\n","trainer_medium = nltk.tag.hmm.HiddenMarkovModelTrainer(states=label_list, symbols=char_set_medium)\n","data_medium = combine_lists_elementwise(tokens_medium.copy(), ner_tags_medium.copy())\n","# large\n","trainer_large = nltk.tag.hmm.HiddenMarkovModelTrainer(states=label_list, symbols=char_set_large)\n","data_large = combine_lists_elementwise(tokens_large.copy(), ner_tags_large.copy())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_small = trainer_small.train_supervised(data_small)\n","model_medium = trainer_medium.train_supervised(data_medium)\n","model_large = trainer_large.train_supervised(data_large)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["save_hmm(model_small, \"hmm_model_small.dill\")\n","save_hmm(model_medium, \"hmm_model_medium.dill\")\n","save_hmm(model_large, \"hmm_model_large.dill\")"]},{"cell_type":"code","execution_count":193,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model loaded successfully!\n","Model loaded successfully!\n","Model loaded successfully!\n"]}],"source":["model_small = load_hmm(\"hmm_model_small.dill\")\n","model_medium = load_hmm(\"hmm_model_medium.dill\")\n","model_large = load_hmm(\"hmm_model_large.dill\")"]},{"cell_type":"markdown","metadata":{},"source":["### BERT Training"]},{"cell_type":"code","execution_count":203,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:12:42.604039Z","iopub.status.busy":"2024-04-22T11:12:42.603347Z","iopub.status.idle":"2024-04-22T11:12:43.778557Z","shell.execute_reply":"2024-04-22T11:12:43.777487Z","shell.execute_reply.started":"2024-04-22T11:12:42.604004Z"},"trusted":true},"outputs":[],"source":["# create 3 datasets\n","\n","small_datasets_dict = {\n","    \"train\": Dataset.from_dict({\"tokens\": tokens_small, \"pos_tags\": pos_tags_small, \"ner_tags\": ner_tags_small})\n","}\n","medium_datasets_dict = {\n","    \"train\": Dataset.from_dict({\"tokens\": tokens_medium, \"pos_tags\": pos_tags_medium, \"ner_tags\": ner_tags_medium})\n","}\n","large_datasets_dict = {\n","    \"train\": Dataset.from_dict({\"tokens\": tokens_large, \"pos_tags\": pos_tags_large, \"ner_tags\": ner_tags_large})\n","}\n","\n","small_dataset = DatasetDict(small_datasets_dict)\n","medium_dataset = DatasetDict(medium_datasets_dict)\n","large_dataset = DatasetDict(large_datasets_dict)"]},{"cell_type":"code","execution_count":204,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:13:59.828462Z","iopub.status.busy":"2024-04-22T11:13:59.828037Z","iopub.status.idle":"2024-04-22T11:14:31.021982Z","shell.execute_reply":"2024-04-22T11:14:31.020974Z","shell.execute_reply.started":"2024-04-22T11:13:59.828433Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd618f48875c4876896d2a6d697ab912","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2144 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9cb0cde4523488e8b14792df1c82ee5","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/11072 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b1e0e3a873e4ed4bc3d6a04ae4475f9","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/51072 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["small_tokenized_dataset = small_dataset.map(tokenize_and_align_labels, batched=True)\n","medium_tokenized_dataset = medium_dataset.map(tokenize_and_align_labels, batched=True)\n","large_tokenized_dataset = large_dataset.map(tokenize_and_align_labels, batched=True)"]},{"cell_type":"code","execution_count":205,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["args_small = TrainingArguments(\n","    f\"{model_name}-finetuned-small-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 20,\n","    logging_steps = 20,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","args_medium = TrainingArguments(\n","    f\"{model_name}-finetuned-medium-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 50,\n","    logging_steps = 50,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","args_large = TrainingArguments(\n","    f\"{model_name}-finetuned-large-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 300,\n","    logging_steps = 300,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","model_small = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","model_medium = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","model_large = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","\n","\n","BERTtrainer_small = Trainer(\n","    model_small,\n","    args_small,\n","    train_dataset=small_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_medium = Trainer(\n","    model_medium,\n","    args_medium,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_large = Trainer(\n","    model_large,\n","    args_large,\n","    train_dataset=large_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# BERTtrainer_small = None\n","# BERTtrainer_medium = None\n","# BERTtrainer_large = None\n","# BERTtrainer = None\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["BERTtrainer_small.train()\n","BERTtrainer_small.model.save_pretrained(\"/kaggle/working/BERT_small_save\")\n","BERTtrainer_small = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","BERTtrainer_medium.train()\n","BERTtrainer_medium.model.save_pretrained(\"/kaggle/working/BERT_medium_save\")\n","BERTtrainer_medium = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","BERTtrainer_large.train()\n","BERTtrainer_large.model.save_pretrained(\"/kaggle/working/BERT_large_save\")\n","BERTtrainer_large = None\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## evaluation"]},{"cell_type":"markdown","metadata":{},"source":["### HMM Evaluation"]},{"cell_type":"code","execution_count":194,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model loaded successfully!\n","Model loaded successfully!\n","Model loaded successfully!\n"]}],"source":["model_small = load_hmm(\"hmm_model_small.dill\")\n","model_medium = load_hmm(\"hmm_model_medium.dill\")\n","model_large = load_hmm(\"hmm_model_large.dill\")"]},{"cell_type":"code","execution_count":195,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tag\\hmm.py:336: RuntimeWarning: overflow encountered in cast\n","  O[i, k] = self._output_logprob(si, self._symbols[k])\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tag\\hmm.py:334: RuntimeWarning: overflow encountered in cast\n","  X[i, j] = self._transitions[si].logprob(self._states[j])\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tag\\hmm.py:332: RuntimeWarning: overflow encountered in cast\n","  P[i] = self._priors.logprob(si)\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tag\\hmm.py:364: RuntimeWarning: overflow encountered in cast\n","  O[i, k] = self._output_logprob(si, self._symbols[k])\n"]}],"source":["predicted_small = evaluate_hmm(model_small, test_sentences)\n","predicted_medium = evaluate_hmm(model_medium, test_sentences)\n","predicted_large = evaluate_hmm(model_large, test_sentences)"]},{"cell_type":"code","execution_count":196,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["5000\n","5000\n","5000\n","5000\n"]}],"source":["predicted_small = [item for sublist in predicted_small for item in sublist]\n","predicted_medium = [item for sublist in predicted_medium for item in sublist]\n","predicted_large = [item for sublist in predicted_large for item in sublist]\n","\n","print(len(correct_tags))\n","print(len(predicted_small))\n","print(len(predicted_medium))\n","print(len(predicted_large))\n"]},{"cell_type":"code","execution_count":197,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.87      0.98      0.92      4261\n","        B-AC       0.84      0.18      0.29       263\n","        B-LF       0.51      0.15      0.24       149\n","        I-LF       0.58      0.16      0.25       327\n","\n","    accuracy                           0.86      5000\n","   macro avg       0.70      0.37      0.43      5000\n","weighted avg       0.84      0.86      0.83      5000\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.90      0.97      0.94      4261\n","        B-AC       0.86      0.37      0.51       263\n","        B-LF       0.57      0.32      0.41       149\n","        I-LF       0.65      0.39      0.49       327\n","\n","    accuracy                           0.88      5000\n","   macro avg       0.74      0.51      0.58      5000\n","weighted avg       0.87      0.88      0.87      5000\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.94      0.97      0.96      4261\n","        B-AC       0.86      0.69      0.77       263\n","        B-LF       0.66      0.54      0.59       149\n","        I-LF       0.74      0.65      0.69       327\n","\n","    accuracy                           0.92      5000\n","   macro avg       0.80      0.71      0.75      5000\n","weighted avg       0.92      0.92      0.92      5000\n","\n"]}],"source":["print(classification_report(correct_tags, predicted_small, labels = label_list))\n","print(classification_report(correct_tags, predicted_medium, labels = label_list))\n","print(classification_report(correct_tags, predicted_large, labels = label_list))"]},{"cell_type":"markdown","metadata":{},"source":["#### Calculate MCC"]},{"cell_type":"code","execution_count":201,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.29521559592637375\n","0.4727518631532203\n","0.681932947075382\n"]}],"source":["print(matthews_corrcoef(correct_tags, predicted_small))\n","print(matthews_corrcoef(correct_tags, predicted_medium))\n","print(matthews_corrcoef(correct_tags, predicted_large))"]},{"cell_type":"markdown","metadata":{},"source":["### BERT Evaluation"]},{"cell_type":"code","execution_count":206,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a11f3acd73974c54b2e98c40f55c2c26","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.94      0.95      5197\n","        B-AC       0.74      0.82      0.78       563\n","        B-LF       0.66      0.66      0.66       290\n","        I-LF       0.72      0.83      0.77       487\n","\n","    accuracy                           0.91      6537\n","   macro avg       0.77      0.81      0.79      6537\n","weighted avg       0.91      0.91      0.91      6537\n","\n","{'eval_loss': 0.26455628871917725, 'eval_precision': 0.9140572951365756, 'eval_recall': 0.906657855608789, 'eval_f1': 0.9103425396035498, 'eval_accuracy': 0.9062260975982867, 'eval_runtime': 2.3886, 'eval_samples_per_second': 52.751, 'eval_steps_per_second': 52.751}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee024e04a1274106922df5556fd33d93","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.82      0.87      0.84       563\n","        B-LF       0.76      0.83      0.79       290\n","        I-LF       0.78      0.89      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.88      0.86      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","{'eval_loss': 0.1811598688364029, 'eval_precision': 0.9424904150691782, 'eval_recall': 0.9340822732529325, 'eval_f1': 0.9382675074676402, 'eval_accuracy': 0.933149762888175, 'eval_runtime': 2.3391, 'eval_samples_per_second': 53.866, 'eval_steps_per_second': 53.866}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f40e032caed94bb3945de2739db5c79c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.96      0.97      5197\n","        B-AC       0.88      0.90      0.89       563\n","        B-LF       0.78      0.87      0.82       290\n","        I-LF       0.80      0.92      0.86       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.91      0.88      6537\n","weighted avg       0.95      0.94      0.95      6537\n","\n","{'eval_loss': 0.1521802544593811, 'eval_precision': 0.9544924154025671, 'eval_recall': 0.9459772013877417, 'eval_f1': 0.9502157318287422, 'eval_accuracy': 0.9444699403396053, 'eval_runtime': 2.2633, 'eval_samples_per_second': 55.671, 'eval_steps_per_second': 55.671}\n"]}],"source":["model_small = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_small_save\", num_labels=len(label_list))\n","model_medium = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_medium_save\", num_labels=len(label_list))\n","model_large = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_large_save\", num_labels=len(label_list))\n","\n","\n","BERTtrainer_small = Trainer(\n","    model_small,\n","    args_small,\n","    train_dataset=small_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_medium = Trainer(\n","    model_medium,\n","    args_medium,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_large = Trainer(\n","    model_large,\n","    args_large,\n","    train_dataset=large_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# BERTtrainer_small.model.from_pretrained(\"model_saves/BERT_small_save\")\n","# BERTtrainer_medium.model.from_pretrained(\"model_saves/BERT_medium_save\")\n","# BERTtrainer_large.model.from_pretrained(\"model_saves/BERT_large_save\")\n","\n","small_metrics = BERTtrainer_small.evaluate()\n","print(small_metrics)\n","medium_metrics = BERTtrainer_medium.evaluate()\n","print(medium_metrics)\n","large_metrics = BERTtrainer_large.evaluate()\n","print(large_metrics)"]},{"cell_type":"markdown","metadata":{},"source":["#### Calculate MCC"]},{"cell_type":"code","execution_count":208,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"784984294aa445d2868a22e0f111bee6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.94      0.95      5197\n","        B-AC       0.74      0.82      0.78       563\n","        B-LF       0.66      0.66      0.66       290\n","        I-LF       0.72      0.83      0.77       487\n","\n","    accuracy                           0.91      6537\n","   macro avg       0.77      0.81      0.79      6537\n","weighted avg       0.91      0.91      0.91      6537\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ec82724a5094940b2e5ad9693029f96","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.82      0.87      0.84       563\n","        B-LF       0.76      0.83      0.79       290\n","        I-LF       0.78      0.89      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.88      0.86      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"36b6cdd4835b4d51ba5e76748641c6af","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.96      0.97      5197\n","        B-AC       0.88      0.90      0.89       563\n","        B-LF       0.78      0.87      0.82       290\n","        I-LF       0.80      0.92      0.86       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.91      0.88      6537\n","weighted avg       0.95      0.94      0.95      6537\n","\n","0.7459767729923106\n","0.8192710779366021\n","0.8490004655748122\n"]}],"source":["small_predictions = BERTtrainer_small.predict(tokenized_datasets[\"validation\"])\n","medium_predictions = BERTtrainer_medium.predict(tokenized_datasets[\"validation\"])\n","large_predictions = BERTtrainer_large.predict(tokenized_datasets[\"validation\"])\n","\n","labels = small_predictions.label_ids\n","\n","small_predictions = small_predictions.predictions.argmax(-1)\n","medium_predictions = medium_predictions.predictions.argmax(-1)\n","large_predictions = large_predictions.predictions.argmax(-1)\n","\n","\n","true_predictions_small = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(small_predictions, labels)\n","    ]\n","true_predictions_medium = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(medium_predictions, labels)\n","    ]\n","true_predictions_large = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(large_predictions, labels)\n","    ]\n","\n","\n","true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(small_predictions, labels)\n","    ]\n","\n","true_predictions_small = [item for sublist in true_predictions_small for item in sublist]\n","true_predictions_medium = [item for sublist in true_predictions_medium for item in sublist]\n","true_predictions_large = [item for sublist in true_predictions_large for item in sublist]\n","true_labels = [item for sublist in true_labels for item in sublist]\n","\n","\n","print(matthews_corrcoef(true_labels, true_predictions_small))\n","print(matthews_corrcoef(true_labels, true_predictions_medium))\n","print(matthews_corrcoef(true_labels, true_predictions_large))"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment 4 (Hyperparameters)"]},{"cell_type":"markdown","metadata":{},"source":["## Arguments For Each Test"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["args_one = TrainingArguments(\n","    f\"{model_name}-finetuned-one-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.01,\n","    adam_beta1=0.9,\n","    adam_beta2=0.999,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","args_two = TrainingArguments(\n","    f\"{model_name}-finetuned-two-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.001,\n","    adam_beta1=0.9,\n","    adam_beta2=0.999,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","args_three = TrainingArguments(\n","    f\"{model_name}-finetuned-three-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.0001,\n","    adam_beta1=0.9,\n","    adam_beta2=0.999,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","args_four = TrainingArguments(\n","    f\"{model_name}-finetuned-four-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.00001,\n","    adam_beta1=0.9,\n","    adam_beta2=0.999,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","\n","args_five = TrainingArguments(\n","    f\"{model_name}-finetuned-five-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.000001,\n","    adam_beta1=0.9,\n","    adam_beta2=0.999,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","args_six = TrainingArguments(\n","    f\"{model_name}-finetuned-six-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.00001,\n","    adam_beta1=0.9,\n","    adam_beta2=0.999,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=5,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","args_seven = TrainingArguments(\n","    f\"{model_name}-finetuned-seven-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.00001,\n","    adam_beta1=0.5,\n","    adam_beta2=0.5,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=3,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","args_eight = TrainingArguments(\n","    f\"{model_name}-finetuned-eight-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.00001,\n","    adam_beta1=0.2,\n","    adam_beta2=0.2,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=3,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","args_nine = TrainingArguments(\n","    f\"{model_name}-finetuned-nine-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.00001,\n","    adam_beta1=0.1,\n","    adam_beta2=0.9,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=3,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","\n","args_ten = TrainingArguments(\n","    f\"{model_name}-finetuned-ten-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.00001,\n","    adam_beta1=0.9,\n","    adam_beta2=0.1,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=5,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["BERTtrainer_ex_4 = Trainer(\n","    model_ex_4,\n","    args_one,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4.train()\n","BERTtrainer_ex_4.model.save_pretrained(\"model_saves/BERT_ex4_one_save\")\n","BERTtrainer_ex_4 = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","BERTtrainer_ex_4 = Trainer(\n","    model_ex_4,\n","    args_two,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4.train()\n","BERTtrainer_ex_4.model.save_pretrained(\"model_saves/BERT_ex4_two_save\")\n","BERTtrainer_ex_4 = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","BERTtrainer_ex_4 = Trainer(\n","    model_ex_4,\n","    args_four,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4.train()\n","BERTtrainer_ex_4.model.save_pretrained(\"model_saves/BERT_ex4_four_save\")\n","BERTtrainer_ex_4 = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","\n","model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","BERTtrainer_ex_4 = Trainer(\n","    model_ex_4,\n","    args_five,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4.train()\n","BERTtrainer_ex_4.model.save_pretrained(\"model_saves/BERT_ex4_five_save\")\n","BERTtrainer_ex_4 = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","BERTtrainer_ex_4 = Trainer(\n","    model_ex_4,\n","    args_seven,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4.train()\n","BERTtrainer_ex_4.model.save_pretrained(\"model_saves/BERT_ex4_seven_save\")\n","BERTtrainer_ex_4 = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","BERTtrainer_ex_4 = Trainer(\n","    model_ex_4,\n","    args_eight,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4.train()\n","BERTtrainer_ex_4.model.save_pretrained(\"model_saves/BERT_ex4_eight_save\")\n","BERTtrainer_ex_4 = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","BERTtrainer_ex_4 = Trainer(\n","    model_ex_4,\n","    args_nine,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4.train()\n","BERTtrainer_ex_4.model.save_pretrained(\"model_saves/BERT_ex4_nine_save\")\n","BERTtrainer_ex_4 = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","BERTtrainer_ex_4 = Trainer(\n","    model_ex_4,\n","    args_ten,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4.train()\n","BERTtrainer_ex_4.model.save_pretrained(\"model_saves/BERT_ex4_ten_save\")\n","BERTtrainer_ex_4 = None\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_ex_4_one = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_one_save\", num_labels=len(label_list))\n","model_ex_4_two = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_two_save\", num_labels=len(label_list))\n","model_ex_4_three = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_three_save\", num_labels=len(label_list))\n","model_ex_4_four = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_four_save\", num_labels=len(label_list))\n","model_ex_4_five = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_five_save\", num_labels=len(label_list))\n","model_ex_4_six = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_six_save\", num_labels=len(label_list))\n","model_ex_4_seven = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_seven_save\", num_labels=len(label_list))\n","model_ex_4_eight = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_eight_save\", num_labels=len(label_list))\n","model_ex_4_nine = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_nine_save\", num_labels=len(label_list))\n","model_ex_4_ten = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_ten_save\", num_labels=len(label_list))\n","\n","BERTtrainer_ex_4_one = Trainer(\n","    model_ex_4_one,\n","    args_one,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_two = Trainer(\n","    model_ex_4_two,\n","    args_two,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_three = Trainer(\n","    model_ex_4_three,\n","    args_three,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_four = Trainer(\n","    model_ex_4_four,\n","    args_four,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_five = Trainer(\n","    model_ex_4_five,\n","    args_five,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_six = Trainer(\n","    model_ex_4_six,\n","    args_six,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_seven = Trainer(\n","    model_ex_4_seven,\n","    args_seven,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_eight = Trainer(\n","    model_ex_4_eight,\n","    args_eight,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_nine = Trainer(\n","    model_ex_4_nine,\n","    args_nine,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_ten = Trainer(\n","    model_ex_4_ten,\n","    args_ten,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = BERTtrainer_ex_4_one.evaluate()\n","print(metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = BERTtrainer_ex_4_two.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = BERTtrainer_ex_4_three.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = BERTtrainer_ex_4_four.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = BERTtrainer_ex_4_five.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = BERTtrainer_ex_4_six.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = BERTtrainer_ex_4_seven.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = BERTtrainer_ex_4_eight.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = BERTtrainer_ex_4_nine.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = BERTtrainer_ex_4_ten.evaluate()\n","print(metrics)"]},{"cell_type":"markdown","metadata":{},"source":["## Automatic Hyperparameter Tuning"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:14:39.462279Z","iopub.status.busy":"2024-04-22T11:14:39.461506Z","iopub.status.idle":"2024-04-22T11:14:39.466721Z","shell.execute_reply":"2024-04-22T11:14:39.465692Z","shell.execute_reply.started":"2024-04-22T11:14:39.462242Z"},"trusted":true},"outputs":[],"source":["model_name = \"bert-base-uncased\""]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:14:59.916605Z","iopub.status.busy":"2024-04-22T11:14:59.915825Z","iopub.status.idle":"2024-04-22T11:14:59.922579Z","shell.execute_reply":"2024-04-22T11:14:59.921624Z","shell.execute_reply.started":"2024-04-22T11:14:59.916571Z"},"trusted":true},"outputs":[],"source":["def optuna_hp_space(trial):\n","    return {\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 3e-5, log=True),\n","        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16]),\n","        \"num_train_epochs\": trial.suggest_int(\"epochs\", 1, 3),\n","        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0005, 0.01),\n","        \"adam_beta1\": trial.suggest_float(\"adam_beta1\", 0.1, 0.9),\n","        \"adam_beta2\": trial.suggest_float(\"adam_beta2\", 0.1, 0.9)\n","}"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:15:06.092020Z","iopub.status.busy":"2024-04-22T11:15:06.091636Z","iopub.status.idle":"2024-04-22T11:15:06.097262Z","shell.execute_reply":"2024-04-22T11:15:06.096085Z","shell.execute_reply.started":"2024-04-22T11:15:06.091991Z"},"trusted":true},"outputs":[],"source":["def model_init(trial):\n","    return BertForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:15:09.554585Z","iopub.status.busy":"2024-04-22T11:15:09.553958Z","iopub.status.idle":"2024-04-22T11:15:09.610122Z","shell.execute_reply":"2024-04-22T11:15:09.609076Z","shell.execute_reply.started":"2024-04-22T11:15:09.554552Z"},"trusted":true},"outputs":[],"source":["train_args = TrainingArguments(\n","        f\"{model_name}-finetuned-optuna-{task}\",\n","        evaluation_strategy ='steps',\n","        # eval_steps = 10,\n","        # logging_steps = 10,\n","        save_total_limit = 1, \n","        adam_epsilon=1e-8,\n","        max_grad_norm=1,\n","        per_device_eval_batch_size=1,\n","        save_steps=0,\n","        metric_for_best_model = 'f1',\n","        load_best_model_at_end=True,\n","        report_to=[\"none\"],\n","    )"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:15:15.300952Z","iopub.status.busy":"2024-04-22T11:15:15.300180Z","iopub.status.idle":"2024-04-22T11:15:17.483604Z","shell.execute_reply":"2024-04-22T11:15:17.482747Z","shell.execute_reply.started":"2024-04-22T11:15:15.300919Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["trainer = Trainer(\n","    model=None,\n","    args=train_args,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    model_init=model_init,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:15:20.740491Z","iopub.status.busy":"2024-04-22T11:15:20.739772Z","iopub.status.idle":"2024-04-22T11:15:21.297544Z","shell.execute_reply":"2024-04-22T11:15:21.296570Z","shell.execute_reply.started":"2024-04-22T11:15:20.740457Z"},"trusted":true},"outputs":[{"data":{"text/plain":["61"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:18:23.411277Z","iopub.status.busy":"2024-04-22T11:18:23.410473Z","iopub.status.idle":"2024-04-22T11:18:23.416287Z","shell.execute_reply":"2024-04-22T11:18:23.415217Z","shell.execute_reply.started":"2024-04-22T11:18:23.411241Z"},"trusted":true},"outputs":[],"source":["def custom_objective(eval_result):\n","  f1 = eval_result[\"eval_f1\"]\n","  loss = eval_result[\"eval_loss\"]\n","  return f1, loss\n"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T11:18:25.567361Z","iopub.status.busy":"2024-04-22T11:18:25.566486Z","iopub.status.idle":"2024-04-22T13:54:15.038440Z","shell.execute_reply":"2024-04-22T13:54:15.037631Z","shell.execute_reply.started":"2024-04-22T11:18:25.567329Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 11:18:25,569] A new study created in memory with name: no-name-f8da4f76-b586-4596-bed1-44bddb9646ce\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4152/4152 09:25, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.309600</td>\n","      <td>0.197858</td>\n","      <td>0.940979</td>\n","      <td>0.927144</td>\n","      <td>0.934010</td>\n","      <td>0.928255</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.228400</td>\n","      <td>0.185127</td>\n","      <td>0.945914</td>\n","      <td>0.933256</td>\n","      <td>0.939543</td>\n","      <td>0.933915</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.203900</td>\n","      <td>0.173009</td>\n","      <td>0.947719</td>\n","      <td>0.940360</td>\n","      <td>0.944025</td>\n","      <td>0.938810</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.175700</td>\n","      <td>0.167916</td>\n","      <td>0.949784</td>\n","      <td>0.943664</td>\n","      <td>0.946714</td>\n","      <td>0.940952</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.183600</td>\n","      <td>0.164975</td>\n","      <td>0.949119</td>\n","      <td>0.943003</td>\n","      <td>0.946051</td>\n","      <td>0.940187</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.167600</td>\n","      <td>0.171188</td>\n","      <td>0.949900</td>\n","      <td>0.942838</td>\n","      <td>0.946356</td>\n","      <td>0.940187</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.159200</td>\n","      <td>0.164691</td>\n","      <td>0.947953</td>\n","      <td>0.944821</td>\n","      <td>0.946384</td>\n","      <td>0.941410</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.152300</td>\n","      <td>0.165060</td>\n","      <td>0.950100</td>\n","      <td>0.943664</td>\n","      <td>0.946871</td>\n","      <td>0.940799</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.82      0.85      0.83       563\n","        B-LF       0.76      0.77      0.76       290\n","        I-LF       0.74      0.91      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.82      0.87      0.84      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.94      0.96      5197\n","        B-AC       0.82      0.90      0.86       563\n","        B-LF       0.76      0.82      0.79       290\n","        I-LF       0.76      0.92      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.90      0.86      6537\n","weighted avg       0.94      0.93      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.84      0.86       563\n","        B-LF       0.78      0.82      0.80       290\n","        I-LF       0.79      0.90      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.86      0.89      0.88       563\n","        B-LF       0.79      0.82      0.81       290\n","        I-LF       0.79      0.88      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.88      0.87       563\n","        B-LF       0.79      0.81      0.80       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.87      0.87       563\n","        B-LF       0.79      0.83      0.81       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.85      0.90      0.87       563\n","        B-LF       0.80      0.84      0.82       290\n","        I-LF       0.81      0.87      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.87      0.87       563\n","        B-LF       0.80      0.83      0.81       290\n","        I-LF       0.79      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 11:27:54,487] Trial 0 finished with values: [0.9468711147948612, 0.16505979001522064] and parameters: {'learning_rate': 1.4862333383535989e-05, 'per_device_train_batch_size': 8, 'epochs': 3, 'weight_decay': 0.007632839440960023, 'adam_beta1': 0.2374782514693039, 'adam_beta2': 0.6740412716573023}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5536' max='5536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5536/5536 07:10, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.318600</td>\n","      <td>0.211133</td>\n","      <td>0.929394</td>\n","      <td>0.932926</td>\n","      <td>0.931157</td>\n","      <td>0.925501</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.236600</td>\n","      <td>0.190024</td>\n","      <td>0.946470</td>\n","      <td>0.934743</td>\n","      <td>0.940570</td>\n","      <td>0.933915</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.222600</td>\n","      <td>0.193186</td>\n","      <td>0.940199</td>\n","      <td>0.935074</td>\n","      <td>0.937629</td>\n","      <td>0.932385</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.216300</td>\n","      <td>0.199294</td>\n","      <td>0.945787</td>\n","      <td>0.930943</td>\n","      <td>0.938307</td>\n","      <td>0.931467</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.198000</td>\n","      <td>0.170724</td>\n","      <td>0.949141</td>\n","      <td>0.940360</td>\n","      <td>0.944730</td>\n","      <td>0.938351</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.190200</td>\n","      <td>0.183322</td>\n","      <td>0.948633</td>\n","      <td>0.945812</td>\n","      <td>0.947220</td>\n","      <td>0.940799</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.158300</td>\n","      <td>0.182057</td>\n","      <td>0.948957</td>\n","      <td>0.939865</td>\n","      <td>0.944389</td>\n","      <td>0.938351</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.165800</td>\n","      <td>0.178504</td>\n","      <td>0.949161</td>\n","      <td>0.943830</td>\n","      <td>0.946488</td>\n","      <td>0.940034</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.171800</td>\n","      <td>0.182520</td>\n","      <td>0.948880</td>\n","      <td>0.944490</td>\n","      <td>0.946680</td>\n","      <td>0.940646</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.163600</td>\n","      <td>0.180918</td>\n","      <td>0.950964</td>\n","      <td>0.945151</td>\n","      <td>0.948049</td>\n","      <td>0.941869</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.162800</td>\n","      <td>0.177528</td>\n","      <td>0.950457</td>\n","      <td>0.944490</td>\n","      <td>0.947464</td>\n","      <td>0.941410</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.95      0.96      0.96      5197\n","        B-AC       0.88      0.75      0.81       563\n","        B-LF       0.71      0.81      0.76       290\n","        I-LF       0.82      0.80      0.81       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.84      0.83      0.83      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.87      0.83      0.85       563\n","        B-LF       0.76      0.76      0.76       290\n","        I-LF       0.76      0.90      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.84      0.86      0.85      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.78      0.91      0.84       563\n","        B-LF       0.80      0.79      0.79       290\n","        I-LF       0.79      0.87      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.84      0.88      0.86      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.94      0.96      5197\n","        B-AC       0.84      0.89      0.87       563\n","        B-LF       0.75      0.80      0.77       290\n","        I-LF       0.74      0.92      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.89      0.85      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.89      0.83      0.86       563\n","        B-LF       0.77      0.81      0.79       290\n","        I-LF       0.78      0.90      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.88      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.97      0.97      5197\n","        B-AC       0.90      0.83      0.87       563\n","        B-LF       0.80      0.81      0.80       290\n","        I-LF       0.81      0.87      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.87      0.87      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.84      0.91      0.88       563\n","        B-LF       0.75      0.81      0.78       290\n","        I-LF       0.79      0.91      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.84      0.90      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.85      0.91      0.88       563\n","        B-LF       0.78      0.80      0.79       290\n","        I-LF       0.80      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.90      0.85      0.87       563\n","        B-LF       0.80      0.79      0.80       290\n","        I-LF       0.80      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.87      0.87      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.89      0.88       563\n","        B-LF       0.79      0.81      0.80       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.89      0.88       563\n","        B-LF       0.78      0.81      0.80       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 11:35:08,182] Trial 1 finished with values: [0.9474643685780577, 0.1775275617837906] and parameters: {'learning_rate': 1.5201145492997306e-05, 'per_device_train_batch_size': 4, 'epochs': 2, 'weight_decay': 0.0013132813377010506, 'adam_beta1': 0.34055734532514564, 'adam_beta2': 0.10271393282928454}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1384' max='1384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1384/1384 06:31, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.296500</td>\n","      <td>0.201248</td>\n","      <td>0.939069</td>\n","      <td>0.926813</td>\n","      <td>0.932901</td>\n","      <td>0.928102</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.205400</td>\n","      <td>0.170630</td>\n","      <td>0.945137</td>\n","      <td>0.939204</td>\n","      <td>0.942161</td>\n","      <td>0.936515</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.94      0.96      5197\n","        B-AC       0.78      0.90      0.84       563\n","        B-LF       0.75      0.80      0.78       290\n","        I-LF       0.75      0.91      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.82      0.89      0.85      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.86      0.86      0.86       563\n","        B-LF       0.77      0.81      0.79       290\n","        I-LF       0.79      0.87      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.88      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 11:41:43,796] Trial 2 finished with values: [0.9421610871726881, 0.17062970995903015] and parameters: {'learning_rate': 1.4225203287307486e-05, 'per_device_train_batch_size': 16, 'epochs': 2, 'weight_decay': 0.009441913650384852, 'adam_beta1': 0.1007033905579739, 'adam_beta2': 0.10660561316440083}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1384' max='1384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1384/1384 03:08, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.322400</td>\n","      <td>0.204400</td>\n","      <td>0.938735</td>\n","      <td>0.926483</td>\n","      <td>0.932568</td>\n","      <td>0.925807</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.234100</td>\n","      <td>0.189254</td>\n","      <td>0.940902</td>\n","      <td>0.931109</td>\n","      <td>0.935979</td>\n","      <td>0.930855</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.81      0.85      0.83       563\n","        B-LF       0.74      0.75      0.75       290\n","        I-LF       0.74      0.90      0.81       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.82      0.86      0.84      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.80      0.89      0.84       563\n","        B-LF       0.75      0.80      0.78       290\n","        I-LF       0.77      0.90      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.82      0.88      0.85      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 11:44:55,014] Trial 3 finished with values: [0.9359794071244706, 0.18925434350967407] and parameters: {'learning_rate': 1.1666504209998573e-05, 'per_device_train_batch_size': 8, 'epochs': 1, 'weight_decay': 0.009348845981118455, 'adam_beta1': 0.46219664966634755, 'adam_beta2': 0.7363835231366819}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='692' max='692' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [692/692 03:15, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.317400</td>\n","      <td>0.201056</td>\n","      <td>0.932945</td>\n","      <td>0.926318</td>\n","      <td>0.929619</td>\n","      <td>0.925807</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.94      0.96      5197\n","        B-AC       0.79      0.87      0.83       563\n","        B-LF       0.72      0.79      0.76       290\n","        I-LF       0.77      0.87      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.81      0.87      0.84      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 11:48:13,086] Trial 4 finished with values: [0.9296194976374036, 0.20105579495429993] and parameters: {'learning_rate': 1.0495334892765539e-05, 'per_device_train_batch_size': 16, 'epochs': 1, 'weight_decay': 0.004177094768599791, 'adam_beta1': 0.5947944115007203, 'adam_beta2': 0.7119370854788608}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2076' max='2076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2076/2076 09:49, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.306500</td>\n","      <td>0.192577</td>\n","      <td>0.940393</td>\n","      <td>0.933091</td>\n","      <td>0.936728</td>\n","      <td>0.932538</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.205500</td>\n","      <td>0.169847</td>\n","      <td>0.946440</td>\n","      <td>0.940030</td>\n","      <td>0.943224</td>\n","      <td>0.937739</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.194200</td>\n","      <td>0.175756</td>\n","      <td>0.947263</td>\n","      <td>0.937717</td>\n","      <td>0.942466</td>\n","      <td>0.938045</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.178100</td>\n","      <td>0.169647</td>\n","      <td>0.948097</td>\n","      <td>0.938543</td>\n","      <td>0.943296</td>\n","      <td>0.938351</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.80      0.90      0.85       563\n","        B-LF       0.76      0.82      0.79       290\n","        I-LF       0.78      0.89      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.89      0.86      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.85      0.87      0.86       563\n","        B-LF       0.77      0.81      0.79       290\n","        I-LF       0.79      0.88      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.88      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.97      5197\n","        B-AC       0.84      0.88      0.86       563\n","        B-LF       0.75      0.82      0.78       290\n","        I-LF       0.78      0.91      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.84      0.89      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.96      0.97      5197\n","        B-AC       0.85      0.87      0.86       563\n","        B-LF       0.77      0.83      0.80       290\n","        I-LF       0.78      0.91      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.84      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 11:58:05,625] Trial 5 finished with values: [0.9432959734329597, 0.16964735090732574] and parameters: {'learning_rate': 1.1314749336651628e-05, 'per_device_train_batch_size': 16, 'epochs': 3, 'weight_decay': 0.0011584680785346278, 'adam_beta1': 0.5034383028375595, 'adam_beta2': 0.5419724118157591}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4152/4152 09:31, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.316300</td>\n","      <td>0.199075</td>\n","      <td>0.940684</td>\n","      <td>0.927474</td>\n","      <td>0.934032</td>\n","      <td>0.927796</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.231300</td>\n","      <td>0.189801</td>\n","      <td>0.944444</td>\n","      <td>0.932430</td>\n","      <td>0.938399</td>\n","      <td>0.932844</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.207400</td>\n","      <td>0.173616</td>\n","      <td>0.947062</td>\n","      <td>0.939865</td>\n","      <td>0.943449</td>\n","      <td>0.938657</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.180200</td>\n","      <td>0.169473</td>\n","      <td>0.948663</td>\n","      <td>0.943334</td>\n","      <td>0.945991</td>\n","      <td>0.940646</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.188300</td>\n","      <td>0.167097</td>\n","      <td>0.949093</td>\n","      <td>0.942508</td>\n","      <td>0.945789</td>\n","      <td>0.940340</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.173500</td>\n","      <td>0.172104</td>\n","      <td>0.949883</td>\n","      <td>0.942508</td>\n","      <td>0.946181</td>\n","      <td>0.940340</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.165700</td>\n","      <td>0.166612</td>\n","      <td>0.947098</td>\n","      <td>0.943499</td>\n","      <td>0.945295</td>\n","      <td>0.940034</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.159000</td>\n","      <td>0.166092</td>\n","      <td>0.951057</td>\n","      <td>0.943830</td>\n","      <td>0.947430</td>\n","      <td>0.941257</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.82      0.85      0.84       563\n","        B-LF       0.75      0.78      0.76       290\n","        I-LF       0.74      0.90      0.81       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.82      0.87      0.84      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.82      0.90      0.86       563\n","        B-LF       0.75      0.82      0.78       290\n","        I-LF       0.76      0.91      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.89      0.86      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.83      0.85       563\n","        B-LF       0.78      0.82      0.80       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.86      0.89      0.88       563\n","        B-LF       0.79      0.82      0.80       290\n","        I-LF       0.80      0.87      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.88      0.87       563\n","        B-LF       0.79      0.81      0.80       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.86      0.87      0.87       563\n","        B-LF       0.79      0.83      0.81       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.84      0.91      0.87       563\n","        B-LF       0.79      0.83      0.81       290\n","        I-LF       0.80      0.87      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.88      0.88       563\n","        B-LF       0.79      0.82      0.81       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 12:07:39,987] Trial 6 finished with values: [0.9474295190713101, 0.1660916656255722] and parameters: {'learning_rate': 1.3399762647745683e-05, 'per_device_train_batch_size': 8, 'epochs': 3, 'weight_decay': 0.007269802275169065, 'adam_beta1': 0.149874201367758, 'adam_beta2': 0.19821008957090014}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2768' max='2768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2768/2768 06:21, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.302300</td>\n","      <td>0.194392</td>\n","      <td>0.943466</td>\n","      <td>0.929126</td>\n","      <td>0.936241</td>\n","      <td>0.930090</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.225900</td>\n","      <td>0.181027</td>\n","      <td>0.947008</td>\n","      <td>0.935900</td>\n","      <td>0.941421</td>\n","      <td>0.935903</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.200300</td>\n","      <td>0.168838</td>\n","      <td>0.945986</td>\n","      <td>0.940360</td>\n","      <td>0.943165</td>\n","      <td>0.937739</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.171100</td>\n","      <td>0.166799</td>\n","      <td>0.949917</td>\n","      <td>0.943169</td>\n","      <td>0.946531</td>\n","      <td>0.940187</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.178500</td>\n","      <td>0.164056</td>\n","      <td>0.948709</td>\n","      <td>0.941186</td>\n","      <td>0.944933</td>\n","      <td>0.939881</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.83      0.85      0.84       563\n","        B-LF       0.76      0.77      0.76       290\n","        I-LF       0.75      0.92      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.87      0.85      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.82      0.90      0.86       563\n","        B-LF       0.77      0.83      0.80       290\n","        I-LF       0.77      0.91      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.83      0.90      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.89      0.81      0.85       563\n","        B-LF       0.79      0.82      0.81       290\n","        I-LF       0.80      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.87      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.86      0.89      0.88       563\n","        B-LF       0.79      0.82      0.80       290\n","        I-LF       0.79      0.88      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.96      0.97      5197\n","        B-AC       0.86      0.88      0.87       563\n","        B-LF       0.78      0.82      0.80       290\n","        I-LF       0.79      0.90      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 12:14:22,881] Trial 7 finished with values: [0.9449328246807099, 0.16405589878559113] and parameters: {'learning_rate': 1.5409178867839693e-05, 'per_device_train_batch_size': 8, 'epochs': 2, 'weight_decay': 0.008213818647612382, 'adam_beta1': 0.5535504813003239, 'adam_beta2': 0.3698941353184395}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='8304' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [8304/8304 10:56, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.360100</td>\n","      <td>0.227067</td>\n","      <td>0.922407</td>\n","      <td>0.926978</td>\n","      <td>0.924687</td>\n","      <td>0.920300</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.247800</td>\n","      <td>0.201302</td>\n","      <td>0.942002</td>\n","      <td>0.931109</td>\n","      <td>0.936524</td>\n","      <td>0.930090</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.232000</td>\n","      <td>0.197683</td>\n","      <td>0.936576</td>\n","      <td>0.931935</td>\n","      <td>0.934250</td>\n","      <td>0.928713</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.223000</td>\n","      <td>0.206536</td>\n","      <td>0.945143</td>\n","      <td>0.930778</td>\n","      <td>0.937906</td>\n","      <td>0.931773</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.208000</td>\n","      <td>0.180025</td>\n","      <td>0.944204</td>\n","      <td>0.936560</td>\n","      <td>0.940367</td>\n","      <td>0.934832</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.201700</td>\n","      <td>0.183984</td>\n","      <td>0.946503</td>\n","      <td>0.941186</td>\n","      <td>0.943837</td>\n","      <td>0.937892</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.173800</td>\n","      <td>0.195228</td>\n","      <td>0.944946</td>\n","      <td>0.932926</td>\n","      <td>0.938898</td>\n","      <td>0.933609</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.182500</td>\n","      <td>0.181368</td>\n","      <td>0.949525</td>\n","      <td>0.941682</td>\n","      <td>0.945587</td>\n","      <td>0.939881</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.189800</td>\n","      <td>0.184901</td>\n","      <td>0.946817</td>\n","      <td>0.941186</td>\n","      <td>0.943993</td>\n","      <td>0.937892</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.181400</td>\n","      <td>0.182158</td>\n","      <td>0.949567</td>\n","      <td>0.942508</td>\n","      <td>0.946024</td>\n","      <td>0.940799</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.181500</td>\n","      <td>0.181307</td>\n","      <td>0.950773</td>\n","      <td>0.944490</td>\n","      <td>0.947621</td>\n","      <td>0.940952</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.162900</td>\n","      <td>0.188151</td>\n","      <td>0.948709</td>\n","      <td>0.941186</td>\n","      <td>0.944933</td>\n","      <td>0.938504</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.164800</td>\n","      <td>0.184391</td>\n","      <td>0.948517</td>\n","      <td>0.940525</td>\n","      <td>0.944504</td>\n","      <td>0.938198</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.161500</td>\n","      <td>0.179575</td>\n","      <td>0.947237</td>\n","      <td>0.943169</td>\n","      <td>0.945199</td>\n","      <td>0.938963</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.153400</td>\n","      <td>0.181222</td>\n","      <td>0.947998</td>\n","      <td>0.942673</td>\n","      <td>0.945328</td>\n","      <td>0.938963</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.164200</td>\n","      <td>0.179604</td>\n","      <td>0.948164</td>\n","      <td>0.942838</td>\n","      <td>0.945494</td>\n","      <td>0.939116</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.95      0.96      0.96      5197\n","        B-AC       0.86      0.74      0.80       563\n","        B-LF       0.69      0.79      0.74       290\n","        I-LF       0.80      0.79      0.79       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.83      0.82      0.82      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.85      0.83      0.84       563\n","        B-LF       0.77      0.77      0.77       290\n","        I-LF       0.74      0.89      0.81       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.86      0.84      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.77      0.91      0.83       563\n","        B-LF       0.77      0.79      0.78       290\n","        I-LF       0.79      0.86      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.88      0.85      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.94      0.96      5197\n","        B-AC       0.84      0.89      0.86       563\n","        B-LF       0.74      0.80      0.77       290\n","        I-LF       0.74      0.92      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.82      0.89      0.85      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.88      0.80      0.84       563\n","        B-LF       0.78      0.81      0.79       290\n","        I-LF       0.79      0.89      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.85      0.86      0.86      6537\n","weighted avg       0.94      0.93      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.89      0.82      0.85       563\n","        B-LF       0.79      0.82      0.81       290\n","        I-LF       0.80      0.87      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.87      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.94      0.96      5197\n","        B-AC       0.81      0.92      0.86       563\n","        B-LF       0.74      0.83      0.78       290\n","        I-LF       0.77      0.92      0.84       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.82      0.90      0.86      6537\n","weighted avg       0.94      0.93      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.97      5197\n","        B-AC       0.84      0.90      0.87       563\n","        B-LF       0.78      0.81      0.80       290\n","        I-LF       0.79      0.90      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.96      0.96      5197\n","        B-AC       0.89      0.81      0.85       563\n","        B-LF       0.79      0.80      0.80       290\n","        I-LF       0.80      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.87      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.87      0.87       563\n","        B-LF       0.78      0.82      0.80       290\n","        I-LF       0.79      0.90      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.87      0.87       563\n","        B-LF       0.79      0.82      0.81       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.87      0.88      0.87       563\n","        B-LF       0.76      0.83      0.79       290\n","        I-LF       0.79      0.89      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.87      0.88      0.87       563\n","        B-LF       0.76      0.81      0.79       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.89      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.86      0.89      0.88       563\n","        B-LF       0.78      0.81      0.79       290\n","        I-LF       0.80      0.86      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.87      0.88      0.88       563\n","        B-LF       0.77      0.82      0.80       290\n","        I-LF       0.79      0.87      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.87      0.88      0.88       563\n","        B-LF       0.78      0.81      0.79       290\n","        I-LF       0.79      0.87      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 12:25:41,160] Trial 8 finished with values: [0.9454937044400266, 0.17960412800312042] and parameters: {'learning_rate': 1.0249444777628393e-05, 'per_device_train_batch_size': 4, 'epochs': 3, 'weight_decay': 0.004295765732187533, 'adam_beta1': 0.15317764726933403, 'adam_beta2': 0.603950742546258}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5536' max='5536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5536/5536 07:18, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.311500</td>\n","      <td>0.208569</td>\n","      <td>0.931091</td>\n","      <td>0.933091</td>\n","      <td>0.932090</td>\n","      <td>0.927184</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.234600</td>\n","      <td>0.186045</td>\n","      <td>0.946841</td>\n","      <td>0.935734</td>\n","      <td>0.941255</td>\n","      <td>0.935291</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.220400</td>\n","      <td>0.187701</td>\n","      <td>0.944334</td>\n","      <td>0.938873</td>\n","      <td>0.941596</td>\n","      <td>0.936209</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.215600</td>\n","      <td>0.199774</td>\n","      <td>0.945610</td>\n","      <td>0.930613</td>\n","      <td>0.938052</td>\n","      <td>0.931008</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.195600</td>\n","      <td>0.168157</td>\n","      <td>0.950008</td>\n","      <td>0.941847</td>\n","      <td>0.945910</td>\n","      <td>0.940340</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.188200</td>\n","      <td>0.181541</td>\n","      <td>0.949487</td>\n","      <td>0.947134</td>\n","      <td>0.948309</td>\n","      <td>0.942328</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.155300</td>\n","      <td>0.182210</td>\n","      <td>0.950284</td>\n","      <td>0.941021</td>\n","      <td>0.945630</td>\n","      <td>0.939881</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.163300</td>\n","      <td>0.176552</td>\n","      <td>0.950839</td>\n","      <td>0.945812</td>\n","      <td>0.948319</td>\n","      <td>0.942022</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.167900</td>\n","      <td>0.180659</td>\n","      <td>0.948216</td>\n","      <td>0.943830</td>\n","      <td>0.946018</td>\n","      <td>0.939881</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.160700</td>\n","      <td>0.179706</td>\n","      <td>0.950899</td>\n","      <td>0.943830</td>\n","      <td>0.947351</td>\n","      <td>0.940952</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.158700</td>\n","      <td>0.175805</td>\n","      <td>0.951382</td>\n","      <td>0.943995</td>\n","      <td>0.947674</td>\n","      <td>0.940952</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.96      0.96      5197\n","        B-AC       0.88      0.76      0.81       563\n","        B-LF       0.72      0.83      0.77       290\n","        I-LF       0.81      0.81      0.81       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.84      0.84      0.84      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.87      0.84      0.85       563\n","        B-LF       0.76      0.78      0.77       290\n","        I-LF       0.77      0.90      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.84      0.87      0.85      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.79      0.91      0.85       563\n","        B-LF       0.80      0.80      0.80       290\n","        I-LF       0.81      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.84      0.89      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.94      0.96      5197\n","        B-AC       0.84      0.90      0.87       563\n","        B-LF       0.74      0.80      0.77       290\n","        I-LF       0.73      0.92      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.82      0.89      0.85      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.89      0.83      0.86       563\n","        B-LF       0.78      0.82      0.80       290\n","        I-LF       0.79      0.90      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.97      0.97      5197\n","        B-AC       0.90      0.83      0.87       563\n","        B-LF       0.81      0.81      0.81       290\n","        I-LF       0.82      0.87      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.87      0.87      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.97      5197\n","        B-AC       0.84      0.92      0.88       563\n","        B-LF       0.76      0.82      0.79       290\n","        I-LF       0.79      0.91      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.84      0.90      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.86      0.91      0.89       563\n","        B-LF       0.79      0.81      0.80       290\n","        I-LF       0.81      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.90      0.85      0.87       563\n","        B-LF       0.80      0.78      0.79       290\n","        I-LF       0.80      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.87      0.87      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.88      0.88       563\n","        B-LF       0.78      0.82      0.80       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.89      0.88       563\n","        B-LF       0.78      0.81      0.80       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 12:33:21,132] Trial 9 finished with values: [0.9476739364789784, 0.17580465972423553] and parameters: {'learning_rate': 1.827772538829445e-05, 'per_device_train_batch_size': 4, 'epochs': 2, 'weight_decay': 0.007513860111426424, 'adam_beta1': 0.407210540866042, 'adam_beta2': 0.21499684396083066}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2076' max='2076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2076/2076 09:50, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.413800</td>\n","      <td>0.362308</td>\n","      <td>0.855737</td>\n","      <td>0.890798</td>\n","      <td>0.872916</td>\n","      <td>0.867217</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.364400</td>\n","      <td>0.329907</td>\n","      <td>0.862402</td>\n","      <td>0.886337</td>\n","      <td>0.874206</td>\n","      <td>0.871348</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.328900</td>\n","      <td>0.327370</td>\n","      <td>0.872009</td>\n","      <td>0.897076</td>\n","      <td>0.884365</td>\n","      <td>0.882821</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.276700</td>\n","      <td>0.308086</td>\n","      <td>0.878268</td>\n","      <td>0.904675</td>\n","      <td>0.891276</td>\n","      <td>0.889552</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.92      0.93      0.93      5197\n","        B-AC       0.68      0.81      0.74       563\n","        B-LF       0.55      0.42      0.48       290\n","        I-LF       0.66      0.51      0.58       487\n","\n","    accuracy                           0.87      6537\n","   macro avg       0.70      0.67      0.68      6537\n","weighted avg       0.86      0.87      0.86      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.94      0.92      0.93      5197\n","        B-AC       0.68      0.82      0.75       563\n","        B-LF       0.52      0.46      0.49       290\n","        I-LF       0.62      0.62      0.62       487\n","\n","    accuracy                           0.87      6537\n","   macro avg       0.69      0.71      0.70      6537\n","weighted avg       0.87      0.87      0.87      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.94      0.93      0.94      5197\n","        B-AC       0.71      0.83      0.76       563\n","        B-LF       0.54      0.54      0.54       290\n","        I-LF       0.68      0.63      0.65       487\n","\n","    accuracy                           0.88      6537\n","   macro avg       0.72      0.73      0.72      6537\n","weighted avg       0.89      0.88      0.88      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.94      0.95      0.94      5197\n","        B-AC       0.76      0.79      0.77       563\n","        B-LF       0.58      0.50      0.53       290\n","        I-LF       0.70      0.64      0.67       487\n","\n","    accuracy                           0.89      6537\n","   macro avg       0.74      0.72      0.73      6537\n","weighted avg       0.89      0.89      0.89      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 12:43:31,982] Trial 10 finished with values: [0.8912760416666667, 0.30808576941490173] and parameters: {'learning_rate': 2.013578678866205e-05, 'per_device_train_batch_size': 16, 'epochs': 3, 'weight_decay': 0.0018976159397410437, 'adam_beta1': 0.8437793353524975, 'adam_beta2': 0.269862505957472}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='8304' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [8304/8304 10:53, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.302500</td>\n","      <td>0.209972</td>\n","      <td>0.932638</td>\n","      <td>0.930943</td>\n","      <td>0.931790</td>\n","      <td>0.927184</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.234500</td>\n","      <td>0.190260</td>\n","      <td>0.947069</td>\n","      <td>0.934082</td>\n","      <td>0.940531</td>\n","      <td>0.934527</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.221000</td>\n","      <td>0.188993</td>\n","      <td>0.944176</td>\n","      <td>0.936065</td>\n","      <td>0.940103</td>\n","      <td>0.934374</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.214200</td>\n","      <td>0.191973</td>\n","      <td>0.945955</td>\n","      <td>0.931109</td>\n","      <td>0.938473</td>\n","      <td>0.931008</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.192700</td>\n","      <td>0.163898</td>\n","      <td>0.952023</td>\n","      <td>0.940856</td>\n","      <td>0.946406</td>\n","      <td>0.939575</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.182700</td>\n","      <td>0.181438</td>\n","      <td>0.950058</td>\n","      <td>0.949116</td>\n","      <td>0.949587</td>\n","      <td>0.943093</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.148700</td>\n","      <td>0.165883</td>\n","      <td>0.952206</td>\n","      <td>0.944656</td>\n","      <td>0.948416</td>\n","      <td>0.942328</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.156200</td>\n","      <td>0.181623</td>\n","      <td>0.949338</td>\n","      <td>0.947299</td>\n","      <td>0.948317</td>\n","      <td>0.940646</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.162900</td>\n","      <td>0.196692</td>\n","      <td>0.945861</td>\n","      <td>0.943830</td>\n","      <td>0.944844</td>\n","      <td>0.936974</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.156000</td>\n","      <td>0.170212</td>\n","      <td>0.950613</td>\n","      <td>0.947629</td>\n","      <td>0.949119</td>\n","      <td>0.942481</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.150800</td>\n","      <td>0.182385</td>\n","      <td>0.955211</td>\n","      <td>0.947794</td>\n","      <td>0.951489</td>\n","      <td>0.944317</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.121500</td>\n","      <td>0.185553</td>\n","      <td>0.951430</td>\n","      <td>0.944986</td>\n","      <td>0.948197</td>\n","      <td>0.941410</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.119100</td>\n","      <td>0.184829</td>\n","      <td>0.952681</td>\n","      <td>0.947960</td>\n","      <td>0.950315</td>\n","      <td>0.944164</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.119100</td>\n","      <td>0.184354</td>\n","      <td>0.947742</td>\n","      <td>0.949777</td>\n","      <td>0.948758</td>\n","      <td>0.942634</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.111100</td>\n","      <td>0.192993</td>\n","      <td>0.949868</td>\n","      <td>0.948455</td>\n","      <td>0.949161</td>\n","      <td>0.942481</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.116700</td>\n","      <td>0.198606</td>\n","      <td>0.951993</td>\n","      <td>0.946803</td>\n","      <td>0.949391</td>\n","      <td>0.942022</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.86      0.81      0.83       563\n","        B-LF       0.68      0.84      0.75       290\n","        I-LF       0.79      0.84      0.81       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.82      0.86      0.84      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.88      0.84      0.86       563\n","        B-LF       0.76      0.78      0.77       290\n","        I-LF       0.76      0.92      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.84      0.87      0.85      6537\n","weighted avg       0.94      0.93      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.94      0.96      5197\n","        B-AC       0.78      0.93      0.85       563\n","        B-LF       0.79      0.81      0.80       290\n","        I-LF       0.79      0.90      0.84       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.84      0.90      0.86      6537\n","weighted avg       0.94      0.93      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.94      0.96      5197\n","        B-AC       0.86      0.90      0.88       563\n","        B-LF       0.74      0.82      0.78       290\n","        I-LF       0.72      0.92      0.81       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.82      0.90      0.86      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.90      0.84      0.87       563\n","        B-LF       0.78      0.82      0.80       290\n","        I-LF       0.78      0.92      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.97      0.97      5197\n","        B-AC       0.90      0.85      0.87       563\n","        B-LF       0.81      0.81      0.81       290\n","        I-LF       0.83      0.85      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.88      0.87      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.96      0.97      5197\n","        B-AC       0.85      0.92      0.89       563\n","        B-LF       0.78      0.83      0.80       290\n","        I-LF       0.80      0.90      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.90      0.88      6537\n","weighted avg       0.95      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.91      0.89       563\n","        B-LF       0.78      0.80      0.79       290\n","        I-LF       0.81      0.85      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.97      0.96      5197\n","        B-AC       0.90      0.83      0.87       563\n","        B-LF       0.81      0.77      0.79       290\n","        I-LF       0.80      0.84      0.82       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.87      0.85      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.89      0.88       563\n","        B-LF       0.78      0.81      0.79       290\n","        I-LF       0.83      0.87      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.96      0.97      5197\n","        B-AC       0.89      0.89      0.89       563\n","        B-LF       0.79      0.85      0.82       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.90      0.88      6537\n","weighted avg       0.95      0.94      0.95      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.90      0.89       563\n","        B-LF       0.77      0.81      0.79       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.90      0.89       563\n","        B-LF       0.78      0.86      0.82       290\n","        I-LF       0.82      0.89      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.90      0.88      6537\n","weighted avg       0.95      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.91      0.89       563\n","        B-LF       0.80      0.81      0.81       290\n","        I-LF       0.84      0.83      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.87      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.89      0.89      0.89       563\n","        B-LF       0.78      0.80      0.79       290\n","        I-LF       0.82      0.86      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.89      0.89       563\n","        B-LF       0.78      0.82      0.80       290\n","        I-LF       0.80      0.87      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 12:54:46,767] Trial 11 finished with values: [0.9493912035119688, 0.19860602915287018] and parameters: {'learning_rate': 2.3559867008430577e-05, 'per_device_train_batch_size': 4, 'epochs': 3, 'weight_decay': 0.004443194177945597, 'adam_beta1': 0.705204700300799, 'adam_beta2': 0.5365038647556237}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5536' max='5536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5536/5536 07:15, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.317300</td>\n","      <td>0.212878</td>\n","      <td>0.930613</td>\n","      <td>0.930613</td>\n","      <td>0.930613</td>\n","      <td>0.925807</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.236200</td>\n","      <td>0.190022</td>\n","      <td>0.945625</td>\n","      <td>0.933752</td>\n","      <td>0.939651</td>\n","      <td>0.932997</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.223200</td>\n","      <td>0.191983</td>\n","      <td>0.942311</td>\n","      <td>0.936395</td>\n","      <td>0.939344</td>\n","      <td>0.934068</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.216900</td>\n","      <td>0.195794</td>\n","      <td>0.945488</td>\n","      <td>0.931274</td>\n","      <td>0.938327</td>\n","      <td>0.931620</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.196900</td>\n","      <td>0.169544</td>\n","      <td>0.947184</td>\n","      <td>0.939204</td>\n","      <td>0.943177</td>\n","      <td>0.937280</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.189100</td>\n","      <td>0.177075</td>\n","      <td>0.948739</td>\n","      <td>0.944821</td>\n","      <td>0.946776</td>\n","      <td>0.940952</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.157600</td>\n","      <td>0.175883</td>\n","      <td>0.950275</td>\n","      <td>0.940856</td>\n","      <td>0.945542</td>\n","      <td>0.939575</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.165400</td>\n","      <td>0.172438</td>\n","      <td>0.951739</td>\n","      <td>0.944821</td>\n","      <td>0.948267</td>\n","      <td>0.941869</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.170200</td>\n","      <td>0.176630</td>\n","      <td>0.950166</td>\n","      <td>0.944986</td>\n","      <td>0.947569</td>\n","      <td>0.941104</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.162000</td>\n","      <td>0.178073</td>\n","      <td>0.951390</td>\n","      <td>0.944160</td>\n","      <td>0.947761</td>\n","      <td>0.941410</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.161600</td>\n","      <td>0.174818</td>\n","      <td>0.951248</td>\n","      <td>0.944490</td>\n","      <td>0.947857</td>\n","      <td>0.941563</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.96      0.96      5197\n","        B-AC       0.87      0.77      0.82       563\n","        B-LF       0.70      0.83      0.76       290\n","        I-LF       0.79      0.83      0.81       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.85      0.84      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.87      0.83      0.85       563\n","        B-LF       0.76      0.75      0.76       290\n","        I-LF       0.76      0.90      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.84      0.86      0.85      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.79      0.91      0.84       563\n","        B-LF       0.79      0.80      0.80       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.84      0.89      0.86      6537\n","weighted avg       0.94      0.93      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.94      0.96      5197\n","        B-AC       0.84      0.89      0.87       563\n","        B-LF       0.73      0.79      0.76       290\n","        I-LF       0.75      0.92      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.82      0.89      0.85      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.89      0.82      0.86       563\n","        B-LF       0.77      0.81      0.79       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.87      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.97      0.97      5197\n","        B-AC       0.90      0.83      0.87       563\n","        B-LF       0.80      0.81      0.80       290\n","        I-LF       0.81      0.87      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.87      0.87      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.85      0.91      0.88       563\n","        B-LF       0.76      0.82      0.79       290\n","        I-LF       0.79      0.91      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.84      0.90      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.96      0.97      5197\n","        B-AC       0.86      0.91      0.88       563\n","        B-LF       0.79      0.81      0.80       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.90      0.85      0.87       563\n","        B-LF       0.80      0.80      0.80       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.87      0.87      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.89      0.88       563\n","        B-LF       0.78      0.81      0.80       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.89      0.88       563\n","        B-LF       0.78      0.81      0.80       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 13:02:03,337] Trial 12 finished with values: [0.9478570836442013, 0.17481805384159088] and parameters: {'learning_rate': 1.698913795368728e-05, 'per_device_train_batch_size': 4, 'epochs': 2, 'weight_decay': 0.0027984591554001785, 'adam_beta1': 0.6102149262820477, 'adam_beta2': 0.5560883112575032}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5536' max='5536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5536/5536 07:15, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.349100</td>\n","      <td>0.225178</td>\n","      <td>0.922419</td>\n","      <td>0.927144</td>\n","      <td>0.924775</td>\n","      <td>0.919841</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.245600</td>\n","      <td>0.199913</td>\n","      <td>0.941353</td>\n","      <td>0.930778</td>\n","      <td>0.936036</td>\n","      <td>0.929478</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.230600</td>\n","      <td>0.197187</td>\n","      <td>0.937095</td>\n","      <td>0.932761</td>\n","      <td>0.934923</td>\n","      <td>0.929019</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.222600</td>\n","      <td>0.205801</td>\n","      <td>0.943365</td>\n","      <td>0.930117</td>\n","      <td>0.936694</td>\n","      <td>0.930855</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.207300</td>\n","      <td>0.180488</td>\n","      <td>0.944195</td>\n","      <td>0.936395</td>\n","      <td>0.940279</td>\n","      <td>0.934832</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.201100</td>\n","      <td>0.183661</td>\n","      <td>0.945847</td>\n","      <td>0.940691</td>\n","      <td>0.943262</td>\n","      <td>0.937586</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.173400</td>\n","      <td>0.189598</td>\n","      <td>0.947746</td>\n","      <td>0.937882</td>\n","      <td>0.942788</td>\n","      <td>0.937586</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.182000</td>\n","      <td>0.178989</td>\n","      <td>0.950391</td>\n","      <td>0.943169</td>\n","      <td>0.946766</td>\n","      <td>0.941257</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.188600</td>\n","      <td>0.178859</td>\n","      <td>0.947333</td>\n","      <td>0.942012</td>\n","      <td>0.944665</td>\n","      <td>0.938351</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.180400</td>\n","      <td>0.178429</td>\n","      <td>0.950216</td>\n","      <td>0.942838</td>\n","      <td>0.946513</td>\n","      <td>0.940493</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.180400</td>\n","      <td>0.175636</td>\n","      <td>0.948313</td>\n","      <td>0.942673</td>\n","      <td>0.945485</td>\n","      <td>0.939728</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.95      0.96      0.95      5197\n","        B-AC       0.86      0.73      0.79       563\n","        B-LF       0.70      0.79      0.74       290\n","        I-LF       0.80      0.78      0.79       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.83      0.82      0.82      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.85      0.83      0.84       563\n","        B-LF       0.76      0.76      0.76       290\n","        I-LF       0.74      0.89      0.81       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.86      0.84      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.78      0.91      0.84       563\n","        B-LF       0.77      0.79      0.78       290\n","        I-LF       0.79      0.86      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.87      0.85      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.94      0.96      5197\n","        B-AC       0.83      0.89      0.86       563\n","        B-LF       0.74      0.80      0.77       290\n","        I-LF       0.74      0.91      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.82      0.89      0.85      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.96      0.96      5197\n","        B-AC       0.89      0.79      0.84       563\n","        B-LF       0.78      0.81      0.79       290\n","        I-LF       0.79      0.89      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.85      0.86      0.86      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.88      0.81      0.85       563\n","        B-LF       0.80      0.82      0.81       290\n","        I-LF       0.80      0.87      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.87      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.82      0.91      0.86       563\n","        B-LF       0.77      0.83      0.80       290\n","        I-LF       0.79      0.91      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.84      0.90      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.96      0.97      5197\n","        B-AC       0.85      0.89      0.87       563\n","        B-LF       0.79      0.81      0.80       290\n","        I-LF       0.80      0.90      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.89      0.82      0.86       563\n","        B-LF       0.80      0.80      0.80       290\n","        I-LF       0.80      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.87      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.87      0.87       563\n","        B-LF       0.78      0.82      0.80       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.87      0.87       563\n","        B-LF       0.79      0.81      0.80       290\n","        I-LF       0.79      0.88      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 13:09:20,252] Trial 13 finished with values: [0.9454846727423363, 0.17563563585281372] and parameters: {'learning_rate': 1.150669206895087e-05, 'per_device_train_batch_size': 4, 'epochs': 2, 'weight_decay': 0.006281367410989544, 'adam_beta1': 0.1871750780467215, 'adam_beta2': 0.5061683427840943}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='692' max='692' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [692/692 03:15, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.265100</td>\n","      <td>0.178269</td>\n","      <td>0.945630</td>\n","      <td>0.936726</td>\n","      <td>0.941157</td>\n","      <td>0.935750</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.83      0.89      0.86       563\n","        B-LF       0.77      0.80      0.78       290\n","        I-LF       0.77      0.90      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.84      0.89      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 13:12:36,774] Trial 14 finished with values: [0.9411569424848536, 0.17826907336711884] and parameters: {'learning_rate': 2.9541073025632464e-05, 'per_device_train_batch_size': 16, 'epochs': 1, 'weight_decay': 0.0024850622156338503, 'adam_beta1': 0.1759041389275743, 'adam_beta2': 0.43487899400270924}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='8304' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [8304/8304 10:48, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.308800</td>\n","      <td>0.208085</td>\n","      <td>0.928854</td>\n","      <td>0.931769</td>\n","      <td>0.930309</td>\n","      <td>0.925807</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.234400</td>\n","      <td>0.182524</td>\n","      <td>0.947703</td>\n","      <td>0.937056</td>\n","      <td>0.942349</td>\n","      <td>0.936209</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.219800</td>\n","      <td>0.187624</td>\n","      <td>0.942301</td>\n","      <td>0.936230</td>\n","      <td>0.939256</td>\n","      <td>0.934527</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.215700</td>\n","      <td>0.195393</td>\n","      <td>0.946599</td>\n","      <td>0.931274</td>\n","      <td>0.938874</td>\n","      <td>0.931314</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.195300</td>\n","      <td>0.165651</td>\n","      <td>0.950342</td>\n","      <td>0.942177</td>\n","      <td>0.946242</td>\n","      <td>0.940340</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.187700</td>\n","      <td>0.178310</td>\n","      <td>0.949801</td>\n","      <td>0.947134</td>\n","      <td>0.948466</td>\n","      <td>0.942634</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.154000</td>\n","      <td>0.184699</td>\n","      <td>0.948572</td>\n","      <td>0.938543</td>\n","      <td>0.943531</td>\n","      <td>0.936821</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.162900</td>\n","      <td>0.175512</td>\n","      <td>0.950548</td>\n","      <td>0.946308</td>\n","      <td>0.948423</td>\n","      <td>0.942328</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.168500</td>\n","      <td>0.187739</td>\n","      <td>0.947028</td>\n","      <td>0.942177</td>\n","      <td>0.944596</td>\n","      <td>0.938351</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.162100</td>\n","      <td>0.176960</td>\n","      <td>0.950863</td>\n","      <td>0.946308</td>\n","      <td>0.948580</td>\n","      <td>0.942940</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.158500</td>\n","      <td>0.186488</td>\n","      <td>0.953887</td>\n","      <td>0.946638</td>\n","      <td>0.950249</td>\n","      <td>0.943552</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.133600</td>\n","      <td>0.184158</td>\n","      <td>0.950525</td>\n","      <td>0.942673</td>\n","      <td>0.946583</td>\n","      <td>0.939422</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.132600</td>\n","      <td>0.186464</td>\n","      <td>0.951114</td>\n","      <td>0.944986</td>\n","      <td>0.948040</td>\n","      <td>0.941410</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.131600</td>\n","      <td>0.179799</td>\n","      <td>0.949105</td>\n","      <td>0.945812</td>\n","      <td>0.947456</td>\n","      <td>0.941104</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.120500</td>\n","      <td>0.184355</td>\n","      <td>0.949088</td>\n","      <td>0.945482</td>\n","      <td>0.947281</td>\n","      <td>0.939728</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.129600</td>\n","      <td>0.189555</td>\n","      <td>0.950980</td>\n","      <td>0.945482</td>\n","      <td>0.948223</td>\n","      <td>0.940952</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.96      0.96      5197\n","        B-AC       0.88      0.76      0.81       563\n","        B-LF       0.69      0.82      0.75       290\n","        I-LF       0.81      0.80      0.81       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.84      0.83      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.87      0.85      0.86       563\n","        B-LF       0.77      0.78      0.77       290\n","        I-LF       0.77      0.90      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.84      0.87      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.79      0.92      0.85       563\n","        B-LF       0.79      0.79      0.79       290\n","        I-LF       0.80      0.89      0.85       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.84      0.89      0.86      6537\n","weighted avg       0.94      0.93      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.94      0.96      5197\n","        B-AC       0.84      0.90      0.87       563\n","        B-LF       0.74      0.82      0.78       290\n","        I-LF       0.73      0.92      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.82      0.90      0.86      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.90      0.84      0.87       563\n","        B-LF       0.79      0.81      0.80       290\n","        I-LF       0.79      0.90      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.97      0.97      5197\n","        B-AC       0.91      0.84      0.87       563\n","        B-LF       0.80      0.81      0.80       290\n","        I-LF       0.82      0.87      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.87      0.87      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.83      0.93      0.88       563\n","        B-LF       0.74      0.82      0.78       290\n","        I-LF       0.78      0.90      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.83      0.90      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.86      0.92      0.89       563\n","        B-LF       0.79      0.80      0.79       290\n","        I-LF       0.81      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.96      0.96      5197\n","        B-AC       0.90      0.83      0.87       563\n","        B-LF       0.79      0.77      0.78       290\n","        I-LF       0.80      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.86      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.89      0.88       563\n","        B-LF       0.78      0.83      0.80       290\n","        I-LF       0.82      0.89      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.88      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.89      0.88      0.89       563\n","        B-LF       0.80      0.83      0.81       290\n","        I-LF       0.80      0.90      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.88      6537\n","weighted avg       0.95      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.87      0.89      0.88       563\n","        B-LF       0.76      0.81      0.79       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.89      0.89       563\n","        B-LF       0.78      0.82      0.80       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.86      0.91      0.88       563\n","        B-LF       0.77      0.81      0.79       290\n","        I-LF       0.82      0.87      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.88      0.90      0.89       563\n","        B-LF       0.77      0.80      0.79       290\n","        I-LF       0.80      0.86      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.90      0.89       563\n","        B-LF       0.77      0.80      0.79       290\n","        I-LF       0.80      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 13:23:26,680] Trial 15 finished with values: [0.9482230138348107, 0.18955543637275696] and parameters: {'learning_rate': 2.029574429072882e-05, 'per_device_train_batch_size': 4, 'epochs': 3, 'weight_decay': 0.0012902936554091212, 'adam_beta1': 0.5425236973348518, 'adam_beta2': 0.5898902029449955}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='692' max='692' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [692/692 03:14, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.293800</td>\n","      <td>0.193286</td>\n","      <td>0.939041</td>\n","      <td>0.931439</td>\n","      <td>0.935224</td>\n","      <td>0.931008</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.81      0.88      0.84       563\n","        B-LF       0.75      0.81      0.78       290\n","        I-LF       0.77      0.89      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.88      0.85      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 13:26:42,835] Trial 16 finished with values: [0.9352243509994194, 0.19328567385673523] and parameters: {'learning_rate': 1.4809355913082218e-05, 'per_device_train_batch_size': 16, 'epochs': 1, 'weight_decay': 0.0050545147021264225, 'adam_beta1': 0.44291032955181664, 'adam_beta2': 0.7297754228448994}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4152/4152 09:26, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.302600</td>\n","      <td>0.196806</td>\n","      <td>0.940398</td>\n","      <td>0.927970</td>\n","      <td>0.934143</td>\n","      <td>0.928255</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.228500</td>\n","      <td>0.185794</td>\n","      <td>0.946417</td>\n","      <td>0.933752</td>\n","      <td>0.940042</td>\n","      <td>0.934374</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.203300</td>\n","      <td>0.174054</td>\n","      <td>0.948043</td>\n","      <td>0.940525</td>\n","      <td>0.944269</td>\n","      <td>0.938810</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.173900</td>\n","      <td>0.167455</td>\n","      <td>0.949834</td>\n","      <td>0.944656</td>\n","      <td>0.947238</td>\n","      <td>0.941716</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.182100</td>\n","      <td>0.167364</td>\n","      <td>0.950332</td>\n","      <td>0.945151</td>\n","      <td>0.947735</td>\n","      <td>0.941869</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.165500</td>\n","      <td>0.171006</td>\n","      <td>0.951248</td>\n","      <td>0.944490</td>\n","      <td>0.947857</td>\n","      <td>0.942022</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.156400</td>\n","      <td>0.165482</td>\n","      <td>0.948518</td>\n","      <td>0.946638</td>\n","      <td>0.947577</td>\n","      <td>0.942175</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.149200</td>\n","      <td>0.167220</td>\n","      <td>0.952072</td>\n","      <td>0.945151</td>\n","      <td>0.948599</td>\n","      <td>0.941869</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.82      0.85      0.84       563\n","        B-LF       0.75      0.76      0.76       290\n","        I-LF       0.75      0.90      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.82      0.86      0.84      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.83      0.89      0.86       563\n","        B-LF       0.77      0.82      0.79       290\n","        I-LF       0.75      0.92      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.89      0.86      6537\n","weighted avg       0.94      0.93      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.84      0.86       563\n","        B-LF       0.78      0.83      0.81       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.86      0.90      0.88       563\n","        B-LF       0.79      0.81      0.80       290\n","        I-LF       0.80      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.88      0.88       563\n","        B-LF       0.80      0.81      0.80       290\n","        I-LF       0.80      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.87      0.87       563\n","        B-LF       0.79      0.84      0.82       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.85      0.91      0.88       563\n","        B-LF       0.80      0.83      0.81       290\n","        I-LF       0.82      0.86      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.88      0.88       563\n","        B-LF       0.80      0.82      0.81       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 13:36:11,161] Trial 17 finished with values: [0.9485989056541204, 0.16721966862678528] and parameters: {'learning_rate': 1.723364186510463e-05, 'per_device_train_batch_size': 8, 'epochs': 3, 'weight_decay': 0.0017073697673903134, 'adam_beta1': 0.1625004708363173, 'adam_beta2': 0.21453363300338024}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='8304' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [8304/8304 10:48, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.496100</td>\n","      <td>0.452719</td>\n","      <td>0.827874</td>\n","      <td>0.868495</td>\n","      <td>0.847698</td>\n","      <td>0.839070</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.482400</td>\n","      <td>0.421618</td>\n","      <td>0.836878</td>\n","      <td>0.884024</td>\n","      <td>0.859806</td>\n","      <td>0.846413</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.476500</td>\n","      <td>0.407417</td>\n","      <td>0.840748</td>\n","      <td>0.883529</td>\n","      <td>0.861608</td>\n","      <td>0.852073</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.457200</td>\n","      <td>0.395494</td>\n","      <td>0.837571</td>\n","      <td>0.881712</td>\n","      <td>0.859074</td>\n","      <td>0.850696</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.445100</td>\n","      <td>0.382363</td>\n","      <td>0.844100</td>\n","      <td>0.882868</td>\n","      <td>0.863049</td>\n","      <td>0.855285</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.407900</td>\n","      <td>0.375195</td>\n","      <td>0.846445</td>\n","      <td>0.886998</td>\n","      <td>0.866247</td>\n","      <td>0.856662</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.376400</td>\n","      <td>0.368541</td>\n","      <td>0.840395</td>\n","      <td>0.884685</td>\n","      <td>0.861972</td>\n","      <td>0.852685</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.388700</td>\n","      <td>0.355951</td>\n","      <td>0.849218</td>\n","      <td>0.887659</td>\n","      <td>0.868013</td>\n","      <td>0.860486</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.390700</td>\n","      <td>0.355951</td>\n","      <td>0.851969</td>\n","      <td>0.893772</td>\n","      <td>0.872370</td>\n","      <td>0.862781</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.387100</td>\n","      <td>0.349839</td>\n","      <td>0.854834</td>\n","      <td>0.891128</td>\n","      <td>0.872604</td>\n","      <td>0.865076</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.370800</td>\n","      <td>0.344777</td>\n","      <td>0.854344</td>\n","      <td>0.893441</td>\n","      <td>0.873456</td>\n","      <td>0.865841</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.329100</td>\n","      <td>0.360944</td>\n","      <td>0.848331</td>\n","      <td>0.881546</td>\n","      <td>0.864620</td>\n","      <td>0.857427</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.319400</td>\n","      <td>0.355182</td>\n","      <td>0.854462</td>\n","      <td>0.887494</td>\n","      <td>0.870665</td>\n","      <td>0.863852</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.318500</td>\n","      <td>0.358893</td>\n","      <td>0.852425</td>\n","      <td>0.891294</td>\n","      <td>0.871426</td>\n","      <td>0.865535</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.302800</td>\n","      <td>0.346420</td>\n","      <td>0.855134</td>\n","      <td>0.894267</td>\n","      <td>0.874263</td>\n","      <td>0.868288</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.313200</td>\n","      <td>0.347200</td>\n","      <td>0.854335</td>\n","      <td>0.890468</td>\n","      <td>0.872027</td>\n","      <td>0.866605</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.90      0.93      0.91      5197\n","        B-AC       0.63      0.67      0.65       563\n","        B-LF       0.36      0.29      0.32       290\n","        I-LF       0.56      0.42      0.48       487\n","\n","    accuracy                           0.84      6537\n","   macro avg       0.61      0.58      0.59      6537\n","weighted avg       0.83      0.84      0.83      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.87      0.96      0.91      5197\n","        B-AC       0.76      0.53      0.63       563\n","        B-LF       0.59      0.10      0.18       290\n","        I-LF       0.60      0.40      0.48       487\n","\n","    accuracy                           0.85      6537\n","   macro avg       0.70      0.50      0.55      6537\n","weighted avg       0.83      0.85      0.82      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.90      0.94      0.92      5197\n","        B-AC       0.65      0.73      0.69       563\n","        B-LF       0.52      0.24      0.33       290\n","        I-LF       0.62      0.44      0.51       487\n","\n","    accuracy                           0.85      6537\n","   macro avg       0.67      0.59      0.61      6537\n","weighted avg       0.84      0.85      0.84      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.90      0.94      0.92      5197\n","        B-AC       0.67      0.71      0.69       563\n","        B-LF       0.48      0.31      0.38       290\n","        I-LF       0.58      0.43      0.49       487\n","\n","    accuracy                           0.85      6537\n","   macro avg       0.66      0.60      0.62      6537\n","weighted avg       0.84      0.85      0.84      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.89      0.95      0.92      5197\n","        B-AC       0.71      0.62      0.66       563\n","        B-LF       0.60      0.26      0.37       290\n","        I-LF       0.60      0.50      0.54       487\n","\n","    accuracy                           0.86      6537\n","   macro avg       0.70      0.58      0.62      6537\n","weighted avg       0.84      0.86      0.84      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.90      0.94      0.92      5197\n","        B-AC       0.71      0.69      0.70       563\n","        B-LF       0.59      0.33      0.43       290\n","        I-LF       0.59      0.45      0.51       487\n","\n","    accuracy                           0.86      6537\n","   macro avg       0.70      0.60      0.64      6537\n","weighted avg       0.84      0.86      0.85      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.90      0.94      0.92      5197\n","        B-AC       0.67      0.73      0.70       563\n","        B-LF       0.52      0.34      0.42       290\n","        I-LF       0.61      0.40      0.49       487\n","\n","    accuracy                           0.85      6537\n","   macro avg       0.68      0.60      0.63      6537\n","weighted avg       0.84      0.85      0.84      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.91      0.94      0.92      5197\n","        B-AC       0.70      0.72      0.71       563\n","        B-LF       0.49      0.41      0.44       290\n","        I-LF       0.64      0.47      0.54       487\n","\n","    accuracy                           0.86      6537\n","   macro avg       0.69      0.63      0.65      6537\n","weighted avg       0.85      0.86      0.86      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.90      0.95      0.92      5197\n","        B-AC       0.73      0.69      0.71       563\n","        B-LF       0.57      0.32      0.41       290\n","        I-LF       0.65      0.45      0.53       487\n","\n","    accuracy                           0.86      6537\n","   macro avg       0.71      0.60      0.64      6537\n","weighted avg       0.85      0.86      0.85      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.91      0.94      0.92      5197\n","        B-AC       0.74      0.73      0.73       563\n","        B-LF       0.55      0.35      0.43       290\n","        I-LF       0.63      0.50      0.56       487\n","\n","    accuracy                           0.87      6537\n","   macro avg       0.71      0.63      0.66      6537\n","weighted avg       0.86      0.87      0.86      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.91      0.94      0.92      5197\n","        B-AC       0.73      0.74      0.74       563\n","        B-LF       0.58      0.39      0.46       290\n","        I-LF       0.64      0.48      0.55       487\n","\n","    accuracy                           0.87      6537\n","   macro avg       0.71      0.64      0.67      6537\n","weighted avg       0.86      0.87      0.86      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.92      0.92      0.92      5197\n","        B-AC       0.69      0.79      0.74       563\n","        B-LF       0.52      0.43      0.47       290\n","        I-LF       0.57      0.52      0.54       487\n","\n","    accuracy                           0.86      6537\n","   macro avg       0.68      0.66      0.67      6537\n","weighted avg       0.85      0.86      0.86      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.91      0.93      0.92      5197\n","        B-AC       0.74      0.74      0.74       563\n","        B-LF       0.57      0.42      0.49       290\n","        I-LF       0.59      0.52      0.55       487\n","\n","    accuracy                           0.86      6537\n","   macro avg       0.70      0.66      0.68      6537\n","weighted avg       0.86      0.86      0.86      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.91      0.93      0.92      5197\n","        B-AC       0.71      0.78      0.74       563\n","        B-LF       0.60      0.43      0.50       290\n","        I-LF       0.62      0.50      0.55       487\n","\n","    accuracy                           0.87      6537\n","   macro avg       0.71      0.66      0.68      6537\n","weighted avg       0.86      0.87      0.86      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.91      0.94      0.93      5197\n","        B-AC       0.73      0.76      0.74       563\n","        B-LF       0.60      0.42      0.49       290\n","        I-LF       0.63      0.49      0.55       487\n","\n","    accuracy                           0.87      6537\n","   macro avg       0.72      0.65      0.68      6537\n","weighted avg       0.86      0.87      0.86      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.92      0.93      0.92      5197\n","        B-AC       0.72      0.78      0.75       563\n","        B-LF       0.58      0.45      0.51       290\n","        I-LF       0.61      0.52      0.56       487\n","\n","    accuracy                           0.87      6537\n","   macro avg       0.71      0.67      0.68      6537\n","weighted avg       0.86      0.87      0.86      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 13:47:01,277] Trial 18 finished with values: [0.8720271800679502, 0.3471996784210205] and parameters: {'learning_rate': 1.3370912455129997e-05, 'per_device_train_batch_size': 4, 'epochs': 3, 'weight_decay': 0.0074114268772986665, 'adam_beta1': 0.8963563183856276, 'adam_beta2': 0.4387426559131742}. \n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5536' max='5536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5536/5536 07:12, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.298500</td>\n","      <td>0.222004</td>\n","      <td>0.921066</td>\n","      <td>0.931109</td>\n","      <td>0.926060</td>\n","      <td>0.921218</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.236000</td>\n","      <td>0.189011</td>\n","      <td>0.948178</td>\n","      <td>0.937056</td>\n","      <td>0.942584</td>\n","      <td>0.936515</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.220100</td>\n","      <td>0.189335</td>\n","      <td>0.940844</td>\n","      <td>0.935404</td>\n","      <td>0.938116</td>\n","      <td>0.932079</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.211700</td>\n","      <td>0.193782</td>\n","      <td>0.946291</td>\n","      <td>0.931439</td>\n","      <td>0.938806</td>\n","      <td>0.931008</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.191400</td>\n","      <td>0.169646</td>\n","      <td>0.949059</td>\n","      <td>0.941847</td>\n","      <td>0.945439</td>\n","      <td>0.940187</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.183400</td>\n","      <td>0.175379</td>\n","      <td>0.949983</td>\n","      <td>0.947629</td>\n","      <td>0.948805</td>\n","      <td>0.942787</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.146900</td>\n","      <td>0.170882</td>\n","      <td>0.950683</td>\n","      <td>0.942673</td>\n","      <td>0.946661</td>\n","      <td>0.939728</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.154900</td>\n","      <td>0.173223</td>\n","      <td>0.952507</td>\n","      <td>0.947629</td>\n","      <td>0.950062</td>\n","      <td>0.943399</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.160000</td>\n","      <td>0.174321</td>\n","      <td>0.949220</td>\n","      <td>0.944986</td>\n","      <td>0.947098</td>\n","      <td>0.940952</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.150400</td>\n","      <td>0.171833</td>\n","      <td>0.951739</td>\n","      <td>0.944821</td>\n","      <td>0.948267</td>\n","      <td>0.941410</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.146500</td>\n","      <td>0.170141</td>\n","      <td>0.953554</td>\n","      <td>0.946308</td>\n","      <td>0.949917</td>\n","      <td>0.942940</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.94      0.97      0.96      5197\n","        B-AC       0.90      0.67      0.77       563\n","        B-LF       0.75      0.74      0.75       290\n","        I-LF       0.84      0.77      0.80       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.86      0.79      0.82      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.86      0.87      0.86       563\n","        B-LF       0.76      0.79      0.77       290\n","        I-LF       0.77      0.91      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.84      0.88      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.78      0.92      0.84       563\n","        B-LF       0.79      0.79      0.79       290\n","        I-LF       0.79      0.87      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.88      0.86      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.86      0.87      0.87       563\n","        B-LF       0.74      0.81      0.77       290\n","        I-LF       0.73      0.92      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.89      0.85      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.90      0.83      0.86       563\n","        B-LF       0.78      0.82      0.80       290\n","        I-LF       0.80      0.90      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.97      0.97      5197\n","        B-AC       0.91      0.85      0.88       563\n","        B-LF       0.81      0.81      0.81       290\n","        I-LF       0.82      0.86      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.87      0.87      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.86      0.92      0.89       563\n","        B-LF       0.76      0.83      0.79       290\n","        I-LF       0.78      0.89      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.84      0.90      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.91      0.90       563\n","        B-LF       0.77      0.82      0.80       290\n","        I-LF       0.80      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.88      6537\n","weighted avg       0.95      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.90      0.85      0.88       563\n","        B-LF       0.79      0.80      0.80       290\n","        I-LF       0.81      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.87      0.87      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.89      0.88       563\n","        B-LF       0.78      0.81      0.80       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.89      0.89      0.89       563\n","        B-LF       0.79      0.82      0.80       290\n","        I-LF       0.80      0.89      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.88      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2024-04-22 13:54:15,031] Trial 19 finished with values: [0.9499170812603648, 0.17014054954051971] and parameters: {'learning_rate': 2.7372546773241383e-05, 'per_device_train_batch_size': 4, 'epochs': 2, 'weight_decay': 0.007683180956122042, 'adam_beta1': 0.1251657644168966, 'adam_beta2': 0.8712430313955226}. \n"]}],"source":["best_trials = trainer.hyperparameter_search(\n","    direction=[\"maximize\", \"minimize\"],\n","    backend=\"optuna\",\n","    hp_space=optuna_hp_space,\n","    n_trials=20,\n","    compute_objective=custom_objective,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T14:00:55.821416Z","iopub.status.busy":"2024-04-22T14:00:55.820580Z","iopub.status.idle":"2024-04-22T14:00:55.826839Z","shell.execute_reply":"2024-04-22T14:00:55.825606Z","shell.execute_reply.started":"2024-04-22T14:00:55.821378Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["BestRun(run_id='19', objective=[0.9499170812603648, 0.17014054954051971], hyperparameters={'learning_rate': 2.7372546773241383e-05, 'per_device_train_batch_size': 4, 'epochs': 2, 'weight_decay': 0.007683180956122042, 'adam_beta1': 0.1251657644168966, 'adam_beta2': 0.8712430313955226}, run_summary=None)\n"]}],"source":["print(best_trials[4])"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T14:01:21.881938Z","iopub.status.busy":"2024-04-22T14:01:21.881207Z","iopub.status.idle":"2024-04-22T14:01:21.887932Z","shell.execute_reply":"2024-04-22T14:01:21.886871Z","shell.execute_reply.started":"2024-04-22T14:01:21.881901Z"},"trusted":true},"outputs":[],"source":["best_learning_rate = best_trials[4].hyperparameters[\"learning_rate\"] # 2.7372546773241383e-05\n","best_per_device_train_batch_size = best_trials[4].hyperparameters['per_device_train_batch_size'] # 4\n","best_epochs = best_trials[4].hyperparameters['epochs'] # 2\n","best_weight_decay = best_trials[4].hyperparameters['weight_decay'] # 0.007683180956122042\n","best_adam_beta1 = best_trials[4].hyperparameters['adam_beta1'] # 0.1251657644168966\n","best_adam_beta2 = best_trials[4].hyperparameters['adam_beta2'] # 0.8712430313955226"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T14:01:24.213686Z","iopub.status.busy":"2024-04-22T14:01:24.213314Z","iopub.status.idle":"2024-04-22T14:01:24.219159Z","shell.execute_reply":"2024-04-22T14:01:24.218356Z","shell.execute_reply.started":"2024-04-22T14:01:24.213659Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["best_learning_rate: 2.7372546773241383e-05\n","best_per_device_train_batch_size: 4\n","best_epochs: 2\n","best_weight_decay: 0.007683180956122042\n","best_adam_beta1: 0.1251657644168966\n","best_adam_beta2: 0.8712430313955226\n"]}],"source":["print(f\"best_learning_rate: {best_learning_rate}\")\n","print(f\"best_per_device_train_batch_size: {best_per_device_train_batch_size}\")\n","print(f\"best_epochs: {best_epochs}\")\n","print(f\"best_weight_decay: {best_weight_decay}\")\n","print(f\"best_adam_beta1: {best_adam_beta1}\")\n","print(f\"best_adam_beta2: {best_adam_beta2}\")"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T14:01:28.406236Z","iopub.status.busy":"2024-04-22T14:01:28.405828Z","iopub.status.idle":"2024-04-22T14:01:28.432349Z","shell.execute_reply":"2024-04-22T14:01:28.431400Z","shell.execute_reply.started":"2024-04-22T14:01:28.406205Z"},"trusted":true},"outputs":[],"source":["best_train_args = TrainingArguments(\n","        f\"{model_name}-finetuned-best-{task}\",\n","        evaluation_strategy ='steps',\n","        eval_steps = 100,\n","        logging_steps = 100,\n","        save_total_limit = 1, \n","        adam_epsilon=1e-8,\n","        max_grad_norm=1,\n","        per_device_eval_batch_size=1,\n","        save_steps=0,\n","        metric_for_best_model = 'f1',\n","        learning_rate = best_learning_rate,\n","        per_device_train_batch_size = best_per_device_train_batch_size,\n","        num_train_epochs = best_epochs,\n","        weight_decay = best_weight_decay,\n","        adam_beta1 = best_adam_beta1,\n","        adam_beta2 = best_adam_beta2,\n","        load_best_model_at_end=True,\n","        report_to=[\"tensorboard\"],\n","    )"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T14:01:33.718254Z","iopub.status.busy":"2024-04-22T14:01:33.717848Z","iopub.status.idle":"2024-04-22T14:01:34.500010Z","shell.execute_reply":"2024-04-22T14:01:34.499072Z","shell.execute_reply.started":"2024-04-22T14:01:33.718223Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:409: FutureWarning: `Trainer` requires either a `model` or `model_init` argument, but not both. `model_init` will overwrite your model when calling the `train` method. This will become a fatal error in the next release.\n","  warnings.warn(\n"]}],"source":["model = BertForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n","best_trainer = Trainer(\n","    model=model,\n","    args=best_train_args,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    model_init=model_init,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T14:01:42.751151Z","iopub.status.busy":"2024-04-22T14:01:42.750500Z","iopub.status.idle":"2024-04-22T14:10:17.105463Z","shell.execute_reply":"2024-04-22T14:10:17.104495Z","shell.execute_reply.started":"2024-04-22T14:01:42.751111Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5536' max='5536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5536/5536 08:32, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.452500</td>\n","      <td>0.283832</td>\n","      <td>0.900033</td>\n","      <td>0.904345</td>\n","      <td>0.902184</td>\n","      <td>0.899036</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.279600</td>\n","      <td>0.240837</td>\n","      <td>0.923562</td>\n","      <td>0.920205</td>\n","      <td>0.921880</td>\n","      <td>0.915099</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.250000</td>\n","      <td>0.257836</td>\n","      <td>0.923798</td>\n","      <td>0.911284</td>\n","      <td>0.917498</td>\n","      <td>0.911274</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.252900</td>\n","      <td>0.235453</td>\n","      <td>0.935159</td>\n","      <td>0.919709</td>\n","      <td>0.927370</td>\n","      <td>0.919994</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.257700</td>\n","      <td>0.222004</td>\n","      <td>0.921066</td>\n","      <td>0.931109</td>\n","      <td>0.926060</td>\n","      <td>0.921218</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.248000</td>\n","      <td>0.224666</td>\n","      <td>0.929561</td>\n","      <td>0.920040</td>\n","      <td>0.924776</td>\n","      <td>0.920912</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.251300</td>\n","      <td>0.187811</td>\n","      <td>0.942848</td>\n","      <td>0.937552</td>\n","      <td>0.940192</td>\n","      <td>0.937280</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.231100</td>\n","      <td>0.189980</td>\n","      <td>0.943815</td>\n","      <td>0.935239</td>\n","      <td>0.939507</td>\n","      <td>0.935138</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.234100</td>\n","      <td>0.184323</td>\n","      <td>0.945482</td>\n","      <td>0.936891</td>\n","      <td>0.941167</td>\n","      <td>0.934985</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.215400</td>\n","      <td>0.189011</td>\n","      <td>0.948178</td>\n","      <td>0.937056</td>\n","      <td>0.942584</td>\n","      <td>0.936515</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.212600</td>\n","      <td>0.211461</td>\n","      <td>0.935296</td>\n","      <td>0.928961</td>\n","      <td>0.932118</td>\n","      <td>0.926419</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.210800</td>\n","      <td>0.196647</td>\n","      <td>0.946383</td>\n","      <td>0.927309</td>\n","      <td>0.936749</td>\n","      <td>0.930090</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.241500</td>\n","      <td>0.190835</td>\n","      <td>0.943415</td>\n","      <td>0.933752</td>\n","      <td>0.938559</td>\n","      <td>0.932079</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.214000</td>\n","      <td>0.185232</td>\n","      <td>0.940737</td>\n","      <td>0.936230</td>\n","      <td>0.938478</td>\n","      <td>0.931161</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.221400</td>\n","      <td>0.189335</td>\n","      <td>0.940844</td>\n","      <td>0.935404</td>\n","      <td>0.938116</td>\n","      <td>0.932079</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.223900</td>\n","      <td>0.190142</td>\n","      <td>0.948893</td>\n","      <td>0.941682</td>\n","      <td>0.945274</td>\n","      <td>0.939728</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.230600</td>\n","      <td>0.175565</td>\n","      <td>0.944233</td>\n","      <td>0.942673</td>\n","      <td>0.943452</td>\n","      <td>0.936668</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.195100</td>\n","      <td>0.181100</td>\n","      <td>0.945403</td>\n","      <td>0.941186</td>\n","      <td>0.943290</td>\n","      <td>0.936056</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.204400</td>\n","      <td>0.178343</td>\n","      <td>0.950158</td>\n","      <td>0.941682</td>\n","      <td>0.945901</td>\n","      <td>0.939269</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.204400</td>\n","      <td>0.193782</td>\n","      <td>0.946291</td>\n","      <td>0.931439</td>\n","      <td>0.938806</td>\n","      <td>0.931008</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.185500</td>\n","      <td>0.169228</td>\n","      <td>0.949409</td>\n","      <td>0.942508</td>\n","      <td>0.945946</td>\n","      <td>0.939116</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.192100</td>\n","      <td>0.171833</td>\n","      <td>0.940441</td>\n","      <td>0.944325</td>\n","      <td>0.942379</td>\n","      <td>0.936515</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.210500</td>\n","      <td>0.164060</td>\n","      <td>0.939563</td>\n","      <td>0.945151</td>\n","      <td>0.942349</td>\n","      <td>0.937739</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.185400</td>\n","      <td>0.163746</td>\n","      <td>0.951130</td>\n","      <td>0.945316</td>\n","      <td>0.948214</td>\n","      <td>0.942481</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.183500</td>\n","      <td>0.169646</td>\n","      <td>0.949059</td>\n","      <td>0.941847</td>\n","      <td>0.945439</td>\n","      <td>0.940187</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.199200</td>\n","      <td>0.163052</td>\n","      <td>0.951677</td>\n","      <td>0.946803</td>\n","      <td>0.949234</td>\n","      <td>0.943399</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.197500</td>\n","      <td>0.165654</td>\n","      <td>0.951565</td>\n","      <td>0.944490</td>\n","      <td>0.948014</td>\n","      <td>0.941104</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.203200</td>\n","      <td>0.162650</td>\n","      <td>0.952405</td>\n","      <td>0.945482</td>\n","      <td>0.948931</td>\n","      <td>0.943093</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>0.165100</td>\n","      <td>0.174253</td>\n","      <td>0.949352</td>\n","      <td>0.944490</td>\n","      <td>0.946915</td>\n","      <td>0.940799</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.152000</td>\n","      <td>0.175379</td>\n","      <td>0.949983</td>\n","      <td>0.947629</td>\n","      <td>0.948805</td>\n","      <td>0.942787</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>0.139200</td>\n","      <td>0.188689</td>\n","      <td>0.951696</td>\n","      <td>0.940691</td>\n","      <td>0.946162</td>\n","      <td>0.939881</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.155100</td>\n","      <td>0.169818</td>\n","      <td>0.951236</td>\n","      <td>0.947464</td>\n","      <td>0.949346</td>\n","      <td>0.942634</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>0.150200</td>\n","      <td>0.177386</td>\n","      <td>0.949057</td>\n","      <td>0.947960</td>\n","      <td>0.948508</td>\n","      <td>0.941410</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.131300</td>\n","      <td>0.181350</td>\n","      <td>0.949209</td>\n","      <td>0.941682</td>\n","      <td>0.945430</td>\n","      <td>0.938810</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.158900</td>\n","      <td>0.170882</td>\n","      <td>0.950683</td>\n","      <td>0.942673</td>\n","      <td>0.946661</td>\n","      <td>0.939728</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.156000</td>\n","      <td>0.176310</td>\n","      <td>0.953954</td>\n","      <td>0.944656</td>\n","      <td>0.949282</td>\n","      <td>0.942634</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>0.152000</td>\n","      <td>0.167823</td>\n","      <td>0.950349</td>\n","      <td>0.945482</td>\n","      <td>0.947909</td>\n","      <td>0.940646</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.157600</td>\n","      <td>0.165629</td>\n","      <td>0.951410</td>\n","      <td>0.947794</td>\n","      <td>0.949599</td>\n","      <td>0.942328</td>\n","    </tr>\n","    <tr>\n","      <td>3900</td>\n","      <td>0.152800</td>\n","      <td>0.175131</td>\n","      <td>0.953008</td>\n","      <td>0.944821</td>\n","      <td>0.948897</td>\n","      <td>0.942634</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.156200</td>\n","      <td>0.173223</td>\n","      <td>0.952507</td>\n","      <td>0.947629</td>\n","      <td>0.950062</td>\n","      <td>0.943399</td>\n","    </tr>\n","    <tr>\n","      <td>4100</td>\n","      <td>0.159800</td>\n","      <td>0.162766</td>\n","      <td>0.951005</td>\n","      <td>0.945977</td>\n","      <td>0.948484</td>\n","      <td>0.942481</td>\n","    </tr>\n","    <tr>\n","      <td>4200</td>\n","      <td>0.167800</td>\n","      <td>0.169783</td>\n","      <td>0.952555</td>\n","      <td>0.945316</td>\n","      <td>0.948922</td>\n","      <td>0.942787</td>\n","    </tr>\n","    <tr>\n","      <td>4300</td>\n","      <td>0.154900</td>\n","      <td>0.163940</td>\n","      <td>0.955515</td>\n","      <td>0.947464</td>\n","      <td>0.951472</td>\n","      <td>0.945388</td>\n","    </tr>\n","    <tr>\n","      <td>4400</td>\n","      <td>0.162200</td>\n","      <td>0.164661</td>\n","      <td>0.949710</td>\n","      <td>0.945316</td>\n","      <td>0.947508</td>\n","      <td>0.941104</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.155300</td>\n","      <td>0.174321</td>\n","      <td>0.949220</td>\n","      <td>0.944986</td>\n","      <td>0.947098</td>\n","      <td>0.940952</td>\n","    </tr>\n","    <tr>\n","      <td>4600</td>\n","      <td>0.160800</td>\n","      <td>0.170351</td>\n","      <td>0.953151</td>\n","      <td>0.944490</td>\n","      <td>0.948801</td>\n","      <td>0.941410</td>\n","    </tr>\n","    <tr>\n","      <td>4700</td>\n","      <td>0.136100</td>\n","      <td>0.170407</td>\n","      <td>0.951374</td>\n","      <td>0.943830</td>\n","      <td>0.947587</td>\n","      <td>0.940493</td>\n","    </tr>\n","    <tr>\n","      <td>4800</td>\n","      <td>0.155400</td>\n","      <td>0.166329</td>\n","      <td>0.948935</td>\n","      <td>0.942508</td>\n","      <td>0.945711</td>\n","      <td>0.939269</td>\n","    </tr>\n","    <tr>\n","      <td>4900</td>\n","      <td>0.160100</td>\n","      <td>0.167926</td>\n","      <td>0.950250</td>\n","      <td>0.943499</td>\n","      <td>0.946862</td>\n","      <td>0.940187</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.139700</td>\n","      <td>0.171833</td>\n","      <td>0.951739</td>\n","      <td>0.944821</td>\n","      <td>0.948267</td>\n","      <td>0.941410</td>\n","    </tr>\n","    <tr>\n","      <td>5100</td>\n","      <td>0.154600</td>\n","      <td>0.167274</td>\n","      <td>0.951795</td>\n","      <td>0.945977</td>\n","      <td>0.948877</td>\n","      <td>0.942328</td>\n","    </tr>\n","    <tr>\n","      <td>5200</td>\n","      <td>0.147500</td>\n","      <td>0.168243</td>\n","      <td>0.952603</td>\n","      <td>0.946308</td>\n","      <td>0.949445</td>\n","      <td>0.942634</td>\n","    </tr>\n","    <tr>\n","      <td>5300</td>\n","      <td>0.143000</td>\n","      <td>0.171045</td>\n","      <td>0.952563</td>\n","      <td>0.945482</td>\n","      <td>0.949009</td>\n","      <td>0.942175</td>\n","    </tr>\n","    <tr>\n","      <td>5400</td>\n","      <td>0.144400</td>\n","      <td>0.171684</td>\n","      <td>0.953546</td>\n","      <td>0.946142</td>\n","      <td>0.949830</td>\n","      <td>0.942787</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.143200</td>\n","      <td>0.170141</td>\n","      <td>0.953554</td>\n","      <td>0.946308</td>\n","      <td>0.949917</td>\n","      <td>0.942940</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.93      0.94      5197\n","        B-AC       0.66      0.87      0.75       563\n","        B-LF       0.70      0.64      0.67       290\n","        I-LF       0.76      0.76      0.76       487\n","\n","    accuracy                           0.90      6537\n","   macro avg       0.77      0.80      0.78      6537\n","weighted avg       0.91      0.90      0.90      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.94      0.95      5197\n","        B-AC       0.78      0.83      0.80       563\n","        B-LF       0.71      0.75      0.73       290\n","        I-LF       0.75      0.82      0.78       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.80      0.84      0.82      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.93      0.95      5197\n","        B-AC       0.78      0.82      0.80       563\n","        B-LF       0.68      0.79      0.73       290\n","        I-LF       0.70      0.87      0.78       487\n","\n","    accuracy                           0.91      6537\n","   macro avg       0.78      0.85      0.81      6537\n","weighted avg       0.92      0.91      0.91      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.94      0.95      5197\n","        B-AC       0.82      0.82      0.82       563\n","        B-LF       0.74      0.76      0.75       290\n","        I-LF       0.71      0.89      0.79       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.81      0.85      0.83      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.94      0.97      0.96      5197\n","        B-AC       0.90      0.67      0.77       563\n","        B-LF       0.75      0.74      0.75       290\n","        I-LF       0.84      0.77      0.80       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.86      0.79      0.82      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.93      0.95      5197\n","        B-AC       0.74      0.90      0.82       563\n","        B-LF       0.68      0.84      0.76       290\n","        I-LF       0.77      0.89      0.83       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.79      0.89      0.84      6537\n","weighted avg       0.93      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.85      0.85      0.85       563\n","        B-LF       0.75      0.84      0.79       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.84      0.88      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.86      0.84      0.85       563\n","        B-LF       0.74      0.83      0.78       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.84      0.88      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.87      0.85      0.86       563\n","        B-LF       0.76      0.81      0.79       290\n","        I-LF       0.77      0.88      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.84      0.88      0.86      6537\n","weighted avg       0.94      0.93      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.86      0.87      0.86       563\n","        B-LF       0.76      0.79      0.77       290\n","        I-LF       0.77      0.91      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.84      0.88      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.95      0.96      0.96      5197\n","        B-AC       0.89      0.72      0.80       563\n","        B-LF       0.77      0.81      0.79       290\n","        I-LF       0.77      0.86      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.85      0.84      0.84      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.87      0.80      0.83       563\n","        B-LF       0.77      0.80      0.78       290\n","        I-LF       0.73      0.94      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.84      0.87      0.85      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.85      0.85      0.85       563\n","        B-LF       0.77      0.79      0.78       290\n","        I-LF       0.76      0.89      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.84      0.87      0.85      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.84      0.88      0.86       563\n","        B-LF       0.76      0.78      0.77       290\n","        I-LF       0.77      0.85      0.81       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.87      0.85      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.78      0.92      0.84       563\n","        B-LF       0.79      0.79      0.79       290\n","        I-LF       0.79      0.87      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.88      0.86      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.84      0.85       563\n","        B-LF       0.79      0.84      0.82       290\n","        I-LF       0.80      0.90      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.97      0.96      5197\n","        B-AC       0.89      0.83      0.86       563\n","        B-LF       0.81      0.78      0.79       290\n","        I-LF       0.79      0.84      0.82       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.85      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.82      0.91      0.86       563\n","        B-LF       0.80      0.80      0.80       290\n","        I-LF       0.79      0.86      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.88      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.97      5197\n","        B-AC       0.84      0.91      0.87       563\n","        B-LF       0.78      0.83      0.81       290\n","        I-LF       0.78      0.89      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.86      0.87      0.87       563\n","        B-LF       0.74      0.81      0.77       290\n","        I-LF       0.73      0.92      0.82       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.89      0.85      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.90      0.84      0.87       563\n","        B-LF       0.78      0.81      0.79       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.87      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.97      0.96      5197\n","        B-AC       0.91      0.79      0.85       563\n","        B-LF       0.84      0.76      0.80       290\n","        I-LF       0.81      0.82      0.82       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.88      0.84      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.85      0.89      0.87       563\n","        B-LF       0.79      0.79      0.79       290\n","        I-LF       0.83      0.81      0.82       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.86      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.89      0.88       563\n","        B-LF       0.79      0.81      0.80       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.90      0.83      0.86       563\n","        B-LF       0.78      0.82      0.80       290\n","        I-LF       0.80      0.90      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.90      0.88       563\n","        B-LF       0.81      0.81      0.81       290\n","        I-LF       0.80      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.87      0.89      0.88      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.85      0.87       563\n","        B-LF       0.82      0.82      0.82       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.87      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.96      0.97      5197\n","        B-AC       0.85      0.91      0.88       563\n","        B-LF       0.82      0.82      0.82       290\n","        I-LF       0.79      0.91      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.90      0.88      6537\n","weighted avg       0.95      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.89      0.87      0.88       563\n","        B-LF       0.78      0.83      0.80       290\n","        I-LF       0.80      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.97      0.97      5197\n","        B-AC       0.91      0.85      0.88       563\n","        B-LF       0.81      0.81      0.81       290\n","        I-LF       0.82      0.86      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.87      0.87      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.97      5197\n","        B-AC       0.89      0.87      0.88       563\n","        B-LF       0.74      0.84      0.79       290\n","        I-LF       0.79      0.91      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.90      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.91      0.89       563\n","        B-LF       0.77      0.83      0.80       290\n","        I-LF       0.81      0.87      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.90      0.89       563\n","        B-LF       0.80      0.81      0.80       290\n","        I-LF       0.81      0.85      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.83      0.93      0.88       563\n","        B-LF       0.79      0.80      0.80       290\n","        I-LF       0.79      0.90      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.90      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.86      0.92      0.89       563\n","        B-LF       0.76      0.83      0.79       290\n","        I-LF       0.78      0.89      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.84      0.90      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.90      0.87      0.88       563\n","        B-LF       0.77      0.81      0.79       290\n","        I-LF       0.79      0.91      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.89      0.89      0.89       563\n","        B-LF       0.77      0.80      0.79       290\n","        I-LF       0.79      0.87      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.90      0.88      0.89       563\n","        B-LF       0.78      0.83      0.80       290\n","        I-LF       0.81      0.86      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.88      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.90      0.87      0.88       563\n","        B-LF       0.79      0.81      0.80       290\n","        I-LF       0.79      0.90      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.91      0.90       563\n","        B-LF       0.77      0.82      0.80       290\n","        I-LF       0.80      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.88      6537\n","weighted avg       0.95      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.90      0.88       563\n","        B-LF       0.78      0.83      0.81       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.96      0.97      5197\n","        B-AC       0.87      0.90      0.89       563\n","        B-LF       0.79      0.84      0.81       290\n","        I-LF       0.80      0.90      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.90      0.88      6537\n","weighted avg       0.95      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.96      0.97      5197\n","        B-AC       0.89      0.89      0.89       563\n","        B-LF       0.80      0.83      0.81       290\n","        I-LF       0.81      0.91      0.86       487\n","\n","    accuracy                           0.95      6537\n","   macro avg       0.87      0.90      0.88      6537\n","weighted avg       0.95      0.95      0.95      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.90      0.88       563\n","        B-LF       0.79      0.82      0.80       290\n","        I-LF       0.80      0.87      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.90      0.85      0.88       563\n","        B-LF       0.79      0.80      0.80       290\n","        I-LF       0.81      0.88      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.87      0.87      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.89      0.88       563\n","        B-LF       0.79      0.82      0.81       290\n","        I-LF       0.79      0.90      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.89      0.88       563\n","        B-LF       0.78      0.81      0.79       290\n","        I-LF       0.79      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.86      0.89      0.88       563\n","        B-LF       0.77      0.82      0.80       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.87      0.89      0.88       563\n","        B-LF       0.76      0.82      0.79       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.89      0.88       563\n","        B-LF       0.78      0.81      0.80       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.89      0.88       563\n","        B-LF       0.79      0.81      0.80       290\n","        I-LF       0.81      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.89      0.89       563\n","        B-LF       0.79      0.81      0.80       290\n","        I-LF       0.81      0.89      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.89      0.89       563\n","        B-LF       0.78      0.82      0.80       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.89      0.89      0.89       563\n","        B-LF       0.78      0.82      0.80       290\n","        I-LF       0.80      0.89      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.89      0.89      0.89       563\n","        B-LF       0.79      0.82      0.80       290\n","        I-LF       0.80      0.89      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.88      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=5536, training_loss=0.19065164322453426, metrics={'train_runtime': 512.8642, 'train_samples_per_second': 43.177, 'train_steps_per_second': 10.794, 'total_flos': 1206512638087008.0, 'train_loss': 0.19065164322453426, 'epoch': 2.0})"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["best_trainer.train()"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T14:23:06.489868Z","iopub.status.busy":"2024-04-22T14:23:06.489428Z","iopub.status.idle":"2024-04-22T14:23:07.215909Z","shell.execute_reply":"2024-04-22T14:23:07.214815Z","shell.execute_reply.started":"2024-04-22T14:23:06.489835Z"},"trusted":true},"outputs":[],"source":["best_trainer.model.save_pretrained(\"model_saves/BERT_best_save\")"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T14:23:08.610272Z","iopub.status.busy":"2024-04-22T14:23:08.609902Z","iopub.status.idle":"2024-04-22T14:23:10.444858Z","shell.execute_reply":"2024-04-22T14:23:10.443946Z","shell.execute_reply.started":"2024-04-22T14:23:08.610245Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [126/126 00:01]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.89      0.89      0.89       563\n","        B-LF       0.79      0.82      0.80       290\n","        I-LF       0.80      0.89      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.89      0.88      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","{'eval_loss': 0.1702021062374115, 'eval_precision': 0.9535541867820876, 'eval_recall': 0.946307616058153, 'eval_f1': 0.9499170812603648, 'eval_accuracy': 0.9429401866299526, 'eval_runtime': 1.8242, 'eval_samples_per_second': 69.07, 'eval_steps_per_second': 69.07, 'epoch': 2.0}\n"]}],"source":["best_metrics = best_trainer.evaluate()\n","print(best_metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import shutil\n","shutil.rmtree(\"/kaggle/working/bert-base-uncased-finetuned-best-ner\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
