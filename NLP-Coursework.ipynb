{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Install dependencies\n","# %pip install -q -U ipywidgets transformers tqdm\n","# %pip install -q -U seqeval\n","# %pip install -q -U accelerate\n","# %pip install -q -U transformers[torch]\n","# %pip install -q --upgrade -U torch torchvision torchaudio torchtext\n","# %pip install -q dill==0.3.1.1\n","# %pip install -q numpy==1.14.3\n","# %pip install -q pyarrow==0.3.8\n","# %pip install -q multiprocess==0.70.16\n","# %pip install -q -U datasets==2.6.0\n","# %pip install fsspec==2023.9.2"]},{"cell_type":"markdown","metadata":{},"source":["## Set Seed and CUDA"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:25:04.658490Z","iopub.status.busy":"2024-04-02T10:25:04.657907Z","iopub.status.idle":"2024-04-02T10:25:05.890120Z","shell.execute_reply":"2024-04-02T10:25:05.889164Z","shell.execute_reply.started":"2024-04-02T10:25:04.658456Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.18.0\n"]}],"source":["import datasets\n","print(datasets.__version__)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:26:05.425613Z","iopub.status.busy":"2024-04-02T10:26:05.425058Z","iopub.status.idle":"2024-04-02T10:26:07.032554Z","shell.execute_reply":"2024-04-02T10:26:07.031596Z","shell.execute_reply.started":"2024-04-02T10:26:05.425584Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["PyTorch Version:  2.2.1+cu121\n","torchtext Version:  0.17.1+cpu\n","Using GPU.\n"]}],"source":["import torch\n","import torchtext\n","\n","SEED = 1234\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","print(\"PyTorch Version: \", torch.__version__)\n","print(\"torchtext Version: \", torchtext.__version__)\n","print(f\"Using {'GPU' if str(DEVICE) == 'cuda' else 'CPU'}.\")"]},{"cell_type":"markdown","metadata":{},"source":["## Data Prep"]},{"cell_type":"markdown","metadata":{},"source":["### Download the Dataset\n","this will download the huggingface dataset ready for use"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:26:09.813478Z","iopub.status.busy":"2024-04-02T10:26:09.812648Z","iopub.status.idle":"2024-04-02T10:26:12.826948Z","shell.execute_reply":"2024-04-02T10:26:12.825763Z","shell.execute_reply.started":"2024-04-02T10:26:09.813442Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f32114b681994d9fbfdc547007f605d3","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/8.37k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188k/188k [00:00<00:00, 534kB/s]\n","Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.4k/28.4k [00:00<00:00, 118kB/s]\n","Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.7k/28.7k [00:00<00:00, 119kB/s]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd104c0ff5654dcea8cf05f95b10afe2","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/1072 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3713658374264580bb6fccb44a5018d1","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/126 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19a6c2488c3d4841803f9687157fbfd0","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/153 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset, Features, Value\n","# import pyarrow as pa\n","# data_type = pa.list_(pa.string())\n","# context_feat = Features({'tokens': Value(dtype=\"string\"), 'pos_tags':Value(dtype=\"string\"), 'ner_tags':Value(dtype=\"string\")})\n","# dataset = load_dataset(\"surrey-nlp/PLOD-CW\", features=context_feat)\n","dataset = load_dataset(\"surrey-nlp/PLOD-CW\", cache_dir=None, download_mode=\"force_redownload\")"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'datasets.dataset_dict.DatasetDict'>\n"]}],"source":["print(type(dataset))"]},{"cell_type":"markdown","metadata":{},"source":["### Get Label List\n","this gets the list of labels for the dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:26:16.391989Z","iopub.status.busy":"2024-04-02T10:26:16.391299Z","iopub.status.idle":"2024-04-02T10:26:16.396576Z","shell.execute_reply":"2024-04-02T10:26:16.395672Z","shell.execute_reply.started":"2024-04-02T10:26:16.391959Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['B-O', 'B-AC', 'B-LF', 'I-LF']\n"]}],"source":["label_list = ['B-O', 'B-AC', 'B-LF', 'I-LF']\n","print(label_list)"]},{"cell_type":"markdown","metadata":{},"source":["### Split The Set Into Train, Val and Test Sets"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:26:26.998689Z","iopub.status.busy":"2024-04-02T10:26:26.997814Z","iopub.status.idle":"2024-04-02T10:26:27.004386Z","shell.execute_reply":"2024-04-02T10:26:27.003236Z","shell.execute_reply.started":"2024-04-02T10:26:26.998657Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["train size: 1072\n","val size: 126\n","test size: 153\n","Counter({'B-O': 32971, 'I-LF': 3231, 'B-AC': 2336, 'B-LF': 1462})\n"]}],"source":["train = dataset['train']\n","print(f\"train size: {len(train)}\")\n","val = dataset['validation']\n","print(f\"val size: {len(val)}\")\n","test = dataset['test']\n","print(f\"test size: {len(test)}\")\n","\n","def flatten(A):\n","    rt = []\n","    for i in A:\n","        if isinstance(i,list): rt.extend(flatten(i))\n","        else: rt.append(i)\n","    return rt\n","\n","from collections import Counter\n","flat = flatten(train[\"ner_tags\"])\n","print(Counter(flat))"]},{"cell_type":"markdown","metadata":{},"source":["## Data visualization\n","Here I visualize the dataset to be used for this course work and analyse its features"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from collections import Counter\n","import os\n","\n","def analyze_nlp_dataset(data, output_folder):\n","  \"\"\"\n","  Analyzes and visualizes an NLP dataset with tokens, POS tags, and NER tags.\n","\n","  Args:\n","      data: A dictionary containing separate lists for tokens, POS tags, and NER tags.\n","          - data[\"tokens\"]: A list of lists of tokens.\n","          - data[\"pos_tags\"]: A list of lists of POS tags.\n","          - data[\"ner_tags\"]: A list of lists of NER tags.\n","      output_folder: The folder path to save generated plots.\n","  \"\"\"\n","\n","  try:\n","    os.mkdir(output_folder)\n","  except FileExistsError:\n","    pass  # Folder already exists, continue\n","\n","  # POS Tag Analysis\n","  all_pos_tags = [pos_tag for row in data[\"pos_tags\"] for pos_tag in row]\n","  pos_tag_counts = Counter(all_pos_tags)\n","\n","  # Plot POS tag distribution\n","  plt.figure(figsize=(8, 6))\n","  plt.pie(pos_tag_counts.values(), labels=pos_tag_counts.keys(), autopct=\"%1.1f%%\")\n","  plt.title(\"POS Tag Distribution\")\n","  plt.savefig(f\"{output_folder}/pos_tag_distribution.png\")\n","  plt.close()\n","\n","  # NER Tag Analysis\n","  all_ner_tags = [ner_tag for row in data[\"ner_tags\"] for ner_tag in row]\n","  ner_tag_counts = Counter(all_ner_tags)\n","\n","  # Plot NER tag distribution (if any named entities exist)\n","  if ner_tag_counts:\n","    plt.figure(figsize=(8, 6))\n","    plt.bar(ner_tag_counts.keys(), ner_tag_counts.values())\n","    plt.xlabel(\"NER Tag\")\n","    plt.ylabel(\"Frequency\")\n","    plt.title(\"NER Tag Distribution\")\n","    plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels for better readability\n","    plt.tight_layout()\n","    plt.savefig(f\"{output_folder}/ner_tag_distribution.png\")\n","    plt.close()\n","  else:\n","    print(\"No named entity tags found in the data for NER tag analysis.\")\n","\n","  # Analysis of POS tags within NER tags\n","  pos_in_ner_tags = {}\n","  for tokens, pos_tags, ner_tags in zip(data[\"tokens\"], data[\"pos_tags\"], data[\"ner_tags\"]):\n","    for token, pos_tag, ner_tag in zip(tokens, pos_tags, ner_tags):\n","      if ner_tag and ner_tag != \"O\":  # Consider only named entity tags (excluding \"O\")\n","        pos_in_ner_tags.setdefault(ner_tag, []).append(pos_tag)\n","\n","  # Calculate POS tag proportions within each NER tag (if data exists)\n","  if pos_in_ner_tags:\n","    for ner_tag, pos_tag_list in pos_in_ner_tags.items():\n","      pos_tag_counts_in_ner = Counter(pos_tag_list)\n","      total_count = sum(pos_tag_counts_in_ner.values())\n","      pos_in_ner_tags[ner_tag] = {tag: count / total_count for tag, count in pos_tag_counts_in_ner.items()}\n","\n","  # Print insights from POS tags within NER tags analysis (optional)\n","  if pos_in_ner_tags:\n","    print(\"\\nInsights from POS tags within NER tags:\")\n","    for ner_tag, pos_tag_proportions in pos_in_ner_tags.items():\n","      print(f\"- NER Tag: {ner_tag}\")\n","      for pos_tag, proportion in pos_tag_proportions.items():\n","        print(f\"  - Proportion of {pos_tag}: {proportion:.2f}\")\n","\n","\n","    # Visualize POS tags within NER tags (if data exists)\n","  if pos_in_ner_tags:\n","    for ner_tag, pos_tag_proportions in pos_in_ner_tags.items():\n","      plt.figure(figsize=(8, 6))\n","      plt.bar(pos_tag_proportions.keys(), pos_tag_proportions.values())\n","      plt.xlabel(\"POS Tag\")\n","      plt.ylabel(\"Proportion\")\n","      plt.title(f\"POS Tag Proportions within NER Tag: {ner_tag}\")\n","      plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels for better readability\n","      plt.tight_layout()\n","      plt.savefig(f\"{output_folder}/pos_in_ner_{ner_tag}.png\")\n","      plt.close()\n","\n","  print(\"Analysis complete. Plots saved to\", output_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["analyze_nlp_dataset(train, \"train_set_analysis\")\n","analyze_nlp_dataset(val, \"val_set_analysis\")\n","analyze_nlp_dataset(test, \"test_set_analysis\")"]},{"cell_type":"markdown","metadata":{},"source":["## Data Pre-Processing"]},{"cell_type":"markdown","metadata":{},"source":["### Lemmatization"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:26:45.591526Z","iopub.status.busy":"2024-04-02T10:26:45.590891Z","iopub.status.idle":"2024-04-02T10:26:47.220720Z","shell.execute_reply":"2024-04-02T10:26:47.219728Z","shell.execute_reply.started":"2024-04-02T10:26:45.591491Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\harry\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('wordnet')\n","\n","# import subprocess\n","\n","# # Download and unzip wordnet\n","# try:\n","#     nltk.data.find('wordnet.zip')\n","# except:\n","#     nltk.download('wordnet', download_dir='/kaggle/working/')\n","#     command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n","#     subprocess.run(command.split())\n","#     nltk.data.path.append('/kaggle/working/')\n","\n","# # Now you can import the NLTK resources as usual\n","# from nltk.corpus import wordnet"]},{"cell_type":"markdown","metadata":{},"source":["the `combine_lists_elementwise` function turns two lists into one, pairing each element elementwise, maintaining the shape of the original list"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:26:52.905869Z","iopub.status.busy":"2024-04-02T10:26:52.904741Z","iopub.status.idle":"2024-04-02T10:26:52.912053Z","shell.execute_reply":"2024-04-02T10:26:52.911095Z","shell.execute_reply.started":"2024-04-02T10:26:52.905835Z"},"trusted":true},"outputs":[],"source":["def combine_lists_elementwise(list_A, list_B):\n","  \"\"\"\n","  Combines two 2D lists of strings element-wise into a 2D list of tuples.\n","\n","  Args:\n","      list_A: A 2D list of strings (e.g., [['A', 'A', 'A'], ['A', 'A', 'A']]).\n","      list_B: Another 2D list of strings with the same dimensions as list_A.\n","\n","  Returns:\n","      A 2D list of tuples, where each tuple combines corresponding elements from list_A and list_B.\n","\n","  Raises:\n","      ValueError: If the dimensions of list_A and list_B don't match.\n","  \"\"\"\n","\n","  # Check if dimensions match\n","  if len(list_A) != len(list_B) or len(list_A[0]) != len(list_B[0]):\n","    raise ValueError(\"Dimensions of lists A and B must be equal.\")\n","\n","  # Create the resulting list using list comprehension\n","  return [[(a, b) for a, b in zip(row_a, row_b)] for row_a, row_b in zip(list_A, list_B)]"]},{"cell_type":"markdown","metadata":{},"source":["the nltk lemmatize function takes a certain format for POS_tags so the `convert_pos_tag` maps a POS_tag from the dataset, to one in the required format. Its important to note that alot of data is lost due to the simplicity of the nltk lemmatize function"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:26:59.984459Z","iopub.status.busy":"2024-04-02T10:26:59.984062Z","iopub.status.idle":"2024-04-02T10:26:59.991410Z","shell.execute_reply":"2024-04-02T10:26:59.990335Z","shell.execute_reply.started":"2024-04-02T10:26:59.984427Z"},"trusted":true},"outputs":[],"source":["def convert_pos_tag(nltk_tag):\n","    \"\"\"\n","    Converts NLTK POS tags to the format expected by the lemmatizer.\n","\n","    Args:\n","        nltk_tag: The POS tag in NLTK format (e.g., VBG, NNS).\n","\n","    Returns:\n","        The corresponding POS tag for the lemmatizer (n, v, a, r, or s) or None if no match.\n","    \"\"\"\n","\n","    tag_map = {\n","        'NUM': '',  # Number (not handled by lemmatizer)\n","        'CCONJ': '',  # Coordinating conjunction (not handled)\n","        'PRON': '',  # Pronoun (not handled)\n","        'NOUN': 'n',   # Noun\n","        'SCONJ': '',  # Subordinating conjunction (not handled)\n","        'SYM': '',   # Symbol (not handled)\n","        'INTJ': '',  # Interjection (not handled)\n","        'ADJ': 'a',    # Adjective\n","        'ADP': '',   # Preposition (not handled)\n","        'PUNCT': '',  # Punctuation (not handled)\n","        'ADV': 'r',    # Adverb\n","        'AUX': 'v',    # Auxiliary verb\n","        'DET': '',   # Determiner (not handled)\n","        'VERB': 'v',   # Verb\n","        'X': '',      # Other (not handled)\n","        'PART': '',   # Particle (not handled)\n","        'PROPN': 'n',   # Proper noun\n","    }\n","    return tag_map.get(nltk_tag)"]},{"cell_type":"markdown","metadata":{},"source":["the `lemmatize_list` function takes the tokens and their respective pos_tags and lemmatizes the tokens"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:27:02.490545Z","iopub.status.busy":"2024-04-02T10:27:02.490099Z","iopub.status.idle":"2024-04-02T10:27:02.496755Z","shell.execute_reply":"2024-04-02T10:27:02.495832Z","shell.execute_reply.started":"2024-04-02T10:27:02.490502Z"},"trusted":true},"outputs":[],"source":["def lemmatize_list(data, pos_tags):\n","    \"\"\"\n","    Lemmatizes a 2D list of tokens using NLTK.\n","\n","    Args:\n","        data: A 2D list of strings (tokens) to be lemmatized.\n","\n","    Returns:\n","        A 2D list containing the lemmatized tokens.\n","    \"\"\"\n","\n","    # Initialize the WordNet lemmatizer\n","    lemmatizer = nltk.WordNetLemmatizer()\n","\n","    pos_tags = [[convert_pos_tag(tag) for tag in row] for row in pos_tags]\n","\n","    data = combine_lists_elementwise(data, pos_tags)\n","\n","\n","    # Lemmatize with part-of-speech information\n","    lemmatized_data = [[token if pos == '' else lemmatizer.lemmatize(token, pos) for token, pos in row] for row in data]\n","\n","    return lemmatized_data"]},{"cell_type":"markdown","metadata":{},"source":["### Pre-Processing Pipeline\n","the `pre_process_data` function applies lemmatization and lowercase to the given data"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:27:11.007638Z","iopub.status.busy":"2024-04-02T10:27:11.006864Z","iopub.status.idle":"2024-04-02T10:27:11.012529Z","shell.execute_reply":"2024-04-02T10:27:11.011522Z","shell.execute_reply.started":"2024-04-02T10:27:11.007603Z"},"trusted":true},"outputs":[],"source":["def pre_process_data(tokens, pos_tags):\n","    # lemmatize the data\n","    data = lemmatize_list(tokens, pos_tags)\n","    # lowercase the data\n","    data = [[string.lower() for string in row] for row in data]\n","    return data"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:27:14.440435Z","iopub.status.busy":"2024-04-02T10:27:14.440043Z","iopub.status.idle":"2024-04-02T10:27:17.187226Z","shell.execute_reply":"2024-04-02T10:27:17.186322Z","shell.execute_reply.started":"2024-04-02T10:27:14.440402Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["original train tokens: ['For', 'this', 'purpose', 'the', 'Gothenburg', 'Young', 'Persons', 'Empowerment', 'Scale', '(', 'GYPES', ')', 'was', 'developed', '.']\n","pre-processed train tokens: ['for', 'this', 'purpose', 'the', 'gothenburg', 'young', 'persons', 'empowerment', 'scale', '(', 'gypes', ')', 'be', 'develop', '.']\n","original val tokens: ['=', 'Manual', 'Ability', 'Classification', 'System', ';', 'QUEST', '=', 'Quest', '-', 'Quality', 'of', 'upper', 'extremity', 'skills', 'test', ';', 'Cont', '=', 'control', ';', 'M', '=', 'male', ',', 'F', '=', 'female', ',', 'V', '=', 'verbal', ',', 'nonV', '=', 'non', '-', 'Verbal', ',', '|Quad', '=', 'quadriplegia', ',', 'Di', '=', 'Diplegia', ',', 'Hemi', '=', 'hemiplegia', '.']\n","pre-processed val tokens: ['=', 'manual', 'ability', 'classification', 'system', ';', 'quest', '=', 'quest', '-', 'quality', 'of', 'upper', 'extremity', 'skill', 'test', ';', 'cont', '=', 'control', ';', 'm', '=', 'male', ',', 'f', '=', 'female', ',', 'v', '=', 'verbal', ',', 'nonv', '=', 'non', '-', 'verbal', ',', '|quad', '=', 'quadriplegia', ',', 'di', '=', 'diplegia', ',', 'hemi', '=', 'hemiplegia', '.']\n"]}],"source":["from datasets import DatasetDict, Dataset\n","train_tokens = pre_process_data(train[\"tokens\"], train[\"pos_tags\"])\n","val_tokens = pre_process_data(val[\"tokens\"], val[\"pos_tags\"])\n","test_tokens = pre_process_data(test[\"tokens\"], test[\"pos_tags\"])\n","original_train_tokens = train[\"tokens\"]\n","original_val_tokens = val[\"tokens\"]\n","print(f\"original train tokens: {original_train_tokens[0]}\\npre-processed train tokens: {train_tokens[0]}\")\n","print(f\"original val tokens: {original_val_tokens[0]}\\npre-processed val tokens: {val_tokens[0]}\")\n","\n","dataset = DatasetDict({\n","    \"train\": Dataset.from_dict({\"tokens\": train_tokens, \"pos_tags\": train[\"pos_tags\"], \"ner_tags\": train[\"ner_tags\"]}),\n","    \"validation\": Dataset.from_dict({\"tokens\": val_tokens, \"pos_tags\": val[\"pos_tags\"], \"ner_tags\": val[\"ner_tags\"]}),\n","    \"test\": Dataset.from_dict({\"tokens\": test_tokens, \"pos_tags\": test[\"pos_tags\"], \"ner_tags\": test[\"ner_tags\"]}),\n","})"]},{"cell_type":"markdown","metadata":{},"source":["### Set Task\n","in this project we are doing Named Entity Recognition so I set the task to \"ner\""]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:27:24.381494Z","iopub.status.busy":"2024-04-02T10:27:24.381066Z","iopub.status.idle":"2024-04-02T10:27:24.386402Z","shell.execute_reply":"2024-04-02T10:27:24.385369Z","shell.execute_reply.started":"2024-04-02T10:27:24.381462Z"},"trusted":true},"outputs":[],"source":["task = \"ner\""]},{"cell_type":"markdown","metadata":{},"source":["# Experiment 1 (Model)\n","HMM vs BERT"]},{"cell_type":"markdown","metadata":{},"source":["## HMM\n","The following is the implementation of an HMM model"]},{"cell_type":"markdown","metadata":{},"source":["### Library Import\n","I am using the nltk library for the HMM implementation"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:27:26.822691Z","iopub.status.busy":"2024-04-02T10:27:26.822060Z","iopub.status.idle":"2024-04-02T10:27:26.826575Z","shell.execute_reply":"2024-04-02T10:27:26.825670Z","shell.execute_reply.started":"2024-04-02T10:27:26.822660Z"},"trusted":true},"outputs":[],"source":["import nltk\n","from sklearn.metrics import classification_report"]},{"cell_type":"markdown","metadata":{},"source":["create lists of the sentences and associated tags from the train set"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[],"source":["# sentences = train[:][\"tokens\"]\n","# tags = train[:][\"ner_tags\"]\n","\n","sentences = train_tokens\n","tags = train[:][\"ner_tags\"]"]},{"cell_type":"markdown","metadata":{},"source":["print out an example of the first sentence and its tags"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["sentence: ['for', 'this', 'purpose', 'the', 'gothenburg', 'young', 'persons', 'empowerment', 'scale', '(', 'gypes', ')', 'be', 'develop', '.']\n","tags: ['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O']\n"]}],"source":["print(f\"sentence: {sentences[0]}\")\n","print(f\"tags: {tags[0]}\")"]},{"cell_type":"markdown","metadata":{},"source":["we generate a character set containing all the characters that can be used in the output of the model"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[],"source":["def get_char_set(sentences):\n","    char_set = set()\n","    for sentence in sentences:\n","        for word in sentence:\n","            for char in word:\n","                char_set.add(char)\n","    char_set = list(char_set)\n","    return char_set"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["char_set: ['[', 'âˆž', 'i', 'y', ')', 'â€¦', 'Î½', 'p', 'Â¯', '+', 'Îº', '}', 'Â§', 'Î³', 'x', 'â™‚', '{', 'â€¡', 'ï¼ˆ', ',', 'â€', '3', '=', 'g', '/', 'e', 'â€¢', 'âˆ’', '%', 'â€²', 'Ã¤', 'Î¼', ':', 's', 'Ã¼', '\"', 'Ïƒ', 'Â´', 'q', 'Â°', '5', 'Â±', '7', 'Ï‰', '&', 'k', 'b', \"'\", 'Ã—', 'Ã£', 'o', 'Ï†', 'â‰¤', 'Î²', 'h', '1', '4', 'â€“', '.', '8', 'w', 'â€œ', ';', 'd', 'ÃŸ', 'a', '@', 'â™€', '>', 'Â®', 'â‰¥', '9', 'Ã¶', 'l', 't', 'Ã¥', 'Ãº', '$', 'Ã©', '(', 'â†’', 'f', 'Âµ', 'â€˜', 'â€”', 'm', 'Îµ', '2', '_', '0', 'Î»', 'u', '-', 'j', 'â€™', 'Ã³', 'Î¸', '*', 'Ã¨', 'Ã¯', 'â€ ', 'c', '6', 'ï¼‰', 'Î±', '<', 'Â·', 'v', ']', 'Ã­', 'â€’', 'z', '#', 'âˆ‘', '?', 'n', 'Î´', 'r']\n"]}],"source":["char_set = get_char_set(sentences)\n","print(f\"char_set: {char_set}\")"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[('for', 'B-O'), ('this', 'B-O'), ('purpose', 'B-O'), ('the', 'B-O'), ('gothenburg', 'B-LF'), ('young', 'I-LF'), ('persons', 'I-LF'), ('empowerment', 'I-LF'), ('scale', 'I-LF'), ('(', 'B-O'), ('gypes', 'B-AC'), (')', 'B-O'), ('be', 'B-O'), ('develop', 'B-O'), ('.', 'B-O')]\n"]}],"source":["trainer = nltk.tag.hmm.HiddenMarkovModelTrainer(states=label_list, symbols=char_set)\n","data = combine_lists_elementwise(sentences.copy(), tags.copy())\n","print(data[0])"]},{"cell_type":"markdown","metadata":{},"source":["## BERT\n","\n","The following is the implementation of BERT model"]},{"cell_type":"markdown","metadata":{},"source":["### Tokenizer"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:27:37.544004Z","iopub.status.busy":"2024-04-02T10:27:37.543168Z","iopub.status.idle":"2024-04-02T10:27:39.972296Z","shell.execute_reply":"2024-04-02T10:27:39.971151Z","shell.execute_reply.started":"2024-04-02T10:27:37.543974Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","import transformers\n","model_checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True) # use AutoTokenizer because it defaults to fast tokenizers where as using the BERT Tokenizer does not\n","assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"]},{"cell_type":"markdown","metadata":{},"source":["#### Quick Example"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:28:05.833795Z","iopub.status.busy":"2024-04-02T10:28:05.833409Z","iopub.status.idle":"2024-04-02T10:28:05.842843Z","shell.execute_reply":"2024-04-02T10:28:05.841857Z","shell.execute_reply.started":"2024-04-02T10:28:05.833767Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['For', 'this', 'purpose', 'the', 'Gothenburg', 'Young', 'Persons', 'Empowerment', 'Scale', '(', 'GYPES', ')', 'was', 'developed', '.']\n","['[CLS]', 'for', 'this', 'purpose', 'the', 'gothenburg', 'young', 'persons', 'empowerment', 'scale', '(', 'g', '##ype', '##s', ')', 'was', 'developed', '.', '[SEP]']\n"]}],"source":["# print an example tokenized text\n","# example = train[0]\n","# tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n","# tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n","# print(example[\"tokens\"])\n","# print(tokens)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:28:08.399235Z","iopub.status.busy":"2024-04-02T10:28:08.398316Z","iopub.status.idle":"2024-04-02T10:28:08.405678Z","shell.execute_reply":"2024-04-02T10:28:08.404544Z","shell.execute_reply.started":"2024-04-02T10:28:08.399204Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 11, 12, 13, 14, None]\n","19 19\n"]}],"source":["# check the length of tokens is the same as in the dataset sample\n","# len(example[f\"{task}_tags\"]), len(tokenized_input[\"input_ids\"])\n","# print(tokenized_input.word_ids())\n","\n","# word_ids = tokenized_input.word_ids()\n","# aligned_labels = [-100 if i is None else example[f\"{task}_tags\"][i] for i in word_ids]\n","# print(len(aligned_labels), len(tokenized_input[\"input_ids\"])) # if it prints the same number twice, then everything is working"]},{"cell_type":"markdown","metadata":{},"source":["I need to map the string tokens to numbers"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:28:11.214854Z","iopub.status.busy":"2024-04-02T10:28:11.214142Z","iopub.status.idle":"2024-04-02T10:28:11.220883Z","shell.execute_reply":"2024-04-02T10:28:11.219794Z","shell.execute_reply.started":"2024-04-02T10:28:11.214816Z"},"trusted":true},"outputs":[],"source":["def encode_tags(tag_sequences, possible_tags):\n","    \"\"\"\n","    Encodes a sequence of string tags into a list of corresponding integer tags.\n","\n","    Args:\n","        tag_sequences: A 2d list of strings representing numerical tags.\n","        possible_tags: A list of strings representing the possible textual labels.\n","\n","    Returns:\n","        A list of strings representing the decoded textual tags.\n","    \"\"\"\n","\n","    encoded_tags = [[possible_tags.index(tag) for tag in row] for row in tag_sequences]\n","    return encoded_tags"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:28:15.437495Z","iopub.status.busy":"2024-04-02T10:28:15.437106Z","iopub.status.idle":"2024-04-02T10:28:15.445771Z","shell.execute_reply":"2024-04-02T10:28:15.444856Z","shell.execute_reply.started":"2024-04-02T10:28:15.437466Z"},"trusted":true},"outputs":[],"source":["label_all_tokens = True\n","def tokenize_and_align_labels(data):\n","    tokenized_inputs = tokenizer(data[\"tokens\"], truncation=True, is_split_into_words=True) ## For some models, you may need to set max_length to approximately 500.\n","\n","    labels = []\n","    converted_tags = encode_tags(data[f\"{task}_tags\"], label_list)\n","    for i, label in enumerate(converted_tags):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:\n","            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n","            # ignored in the loss function.\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            # We set the label for the first token of each word.\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(label[word_idx])\n","            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n","            # the label_all_tokens flag.\n","            else:\n","                label_ids.append(label[word_idx] if label_all_tokens else -100)\n","            previous_word_idx = word_idx\n","\n","        labels.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:28:19.147227Z","iopub.status.busy":"2024-04-02T10:28:19.146365Z","iopub.status.idle":"2024-04-02T10:28:33.275702Z","shell.execute_reply":"2024-04-02T10:28:33.274881Z","shell.execute_reply.started":"2024-04-02T10:28:19.147194Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2b14418958d481dab0da301d41da954","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1072 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29fa44c0b9a5444a86b52d071e39c72e","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/126 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b6b9b0edfa44aebbdef0dd55acde8c1","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/153 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n","tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n","model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:28:55.715611Z","iopub.status.busy":"2024-04-02T10:28:55.714798Z","iopub.status.idle":"2024-04-02T10:28:55.748832Z","shell.execute_reply":"2024-04-02T10:28:55.747981Z","shell.execute_reply.started":"2024-04-02T10:28:55.715573Z"},"trusted":true},"outputs":[],"source":["model_name = model_checkpoint.split(\"/\")[-1]\n","batch_size = 16\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=1000,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=['none'],\n",")"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:28:59.391071Z","iopub.status.busy":"2024-04-02T10:28:59.390382Z","iopub.status.idle":"2024-04-02T10:29:00.436924Z","shell.execute_reply":"2024-04-02T10:29:00.434914Z","shell.execute_reply.started":"2024-04-02T10:28:59.391032Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\harry\\AppData\\Local\\Temp\\ipykernel_89208\\962786391.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(\"seqeval\")\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n","  warnings.warn(\n"]}],"source":["from transformers import DataCollatorForTokenClassification\n","from datasets import load_metric\n","data_collator = DataCollatorForTokenClassification(tokenizer)\n","metric = load_metric(\"seqeval\")\n","# labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n","# metric.compute(predictions=[labels], references=[labels])"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:29:03.553510Z","iopub.status.busy":"2024-04-02T10:29:03.553123Z","iopub.status.idle":"2024-04-02T10:29:03.562736Z","shell.execute_reply":"2024-04-02T10:29:03.561730Z","shell.execute_reply.started":"2024-04-02T10:29:03.553480Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    \n","    overall_results = metric.compute(predictions=true_predictions, references=true_labels)\n","    \n","    true_labels = [item for sublist in true_labels for item in sublist]\n","    true_predictions = [item for sublist in true_predictions for item in sublist]\n","    \n","    \n","    results = classification_report(true_labels, true_predictions, labels = label_list)\n","    print(results)\n","    return {\n","        \"precision\": overall_results[\"overall_precision\"],\n","        \"recall\": overall_results[\"overall_recall\"],\n","        \"f1\": overall_results[\"overall_f1\"],\n","        \"accuracy\": overall_results[\"overall_accuracy\"],\n","    }"]},{"cell_type":"code","execution_count":28,"metadata":{"trusted":true},"outputs":[],"source":["BERTtrainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"markdown","metadata":{},"source":["#### HMM Training"]},{"cell_type":"markdown","metadata":{},"source":["set up the hmm trainer and combine the tokens with their tags"]},{"cell_type":"markdown","metadata":{},"source":["train the model on the data"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["model = trainer.train_supervised(data)"]},{"cell_type":"markdown","metadata":{},"source":["save the model"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model saved as hmm_model.dill\n"]}],"source":["import dill\n","def save_hmm(model, name):\n","    # Open a file for writing in binary mode\n","    with open(name, 'wb') as f:\n","        # Dill can handle more complex objects than pickle\n","        dill.dump(model, f)\n","\n","    print(f\"Model saved as {name}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save_hmm(model, \"hmm_model.dill\")"]},{"cell_type":"markdown","metadata":{},"source":["load the model"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["def load_hmm(name):\n","    # Open the saved model file in binary read mode\n","    with open(name, 'rb') as f:\n","        # Load the model back into a variable using dill.load\n","        model = dill.load(f)\n","        print(\"Model loaded successfully!\")\n","    return model"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model loaded successfully!\n"]}],"source":["model = load_hmm(\"hmm_model.dill\")"]},{"cell_type":"markdown","metadata":{},"source":["#### BERT Training"]},{"cell_type":"markdown","metadata":{},"source":["clear the cuda cache to avoid cuda memory issues"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["train bert"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["BERTtrainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["BERTtrainer.model.save_pretrained(\"/kaggle/working/BERT_save\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["BERTtrainer.model.from_pretrained(\"/kaggle/working/BERT_save\")"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"markdown","metadata":{},"source":["#### HMM evaluation"]},{"cell_type":"code","execution_count":34,"metadata":{"trusted":true},"outputs":[],"source":["def evaluate_hmm(model, test_sentences):\n","    predicted = []\n","    for sentence in test_sentences:\n","        test_result = model.tag(sentence)\n","        out_tags = []\n","        for word, tag in test_result:\n","            out_tags.append(tag)\n","        predicted.append(out_tags)\n","    return predicted"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tag\\hmm.py:336: RuntimeWarning: overflow encountered in cast\n","  O[i, k] = self._output_logprob(si, self._symbols[k])\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tag\\hmm.py:334: RuntimeWarning: overflow encountered in cast\n","  X[i, j] = self._transitions[si].logprob(self._states[j])\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tag\\hmm.py:332: RuntimeWarning: overflow encountered in cast\n","  P[i] = self._priors.logprob(si)\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tag\\hmm.py:364: RuntimeWarning: overflow encountered in cast\n","  O[i, k] = self._output_logprob(si, self._symbols[k])\n"]}],"source":["test_sentences = dataset[\"validation\"][:][\"tokens\"]\n","test_sentences = pre_process_data(test_sentences, dataset[\"validation\"][:][\"pos_tags\"])\n","correct_tags = dataset[\"validation\"][:][\"ner_tags\"]\n","predicted = evaluate_hmm(model, test_sentences)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["number of predictions: 5000\n","number of correct answers: 5000\n","['B-O', 'B-LF', 'I-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF']\n","['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O']\n"]}],"source":["correct_tags = [item for sublist in correct_tags for item in sublist]\n","predicted = [item for sublist in predicted for item in sublist]\n","print(f\"number of predictions: {len(predicted)}\\nnumber of correct answers: {len(correct_tags)}\")\n","\n","print(correct_tags[:100])\n","print(predicted[:100])"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.87      0.99      0.93      4261\n","        B-AC       0.83      0.11      0.19       263\n","        B-LF       0.54      0.10      0.17       149\n","        I-LF       0.72      0.13      0.22       327\n","\n","    accuracy                           0.86      5000\n","   macro avg       0.74      0.33      0.38      5000\n","weighted avg       0.85      0.86      0.82      5000\n","\n"]}],"source":["print(classification_report(correct_tags, predicted, labels = label_list))"]},{"cell_type":"markdown","metadata":{},"source":["#### BERT evaluation"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["model = AutoModelForTokenClassification.from_pretrained(\"model_saves\\\\BERT_save\", num_labels=len(label_list))\n","BERTtrainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":39,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"147579db65744a41bfb3eaba7da67541","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.93      0.95      0.94      5197\n","        B-AC       0.73      0.74      0.74       563\n","        B-LF       0.64      0.25      0.36       290\n","        I-LF       0.60      0.69      0.64       487\n","\n","    accuracy                           0.88      6537\n","   macro avg       0.73      0.66      0.67      6537\n","weighted avg       0.88      0.88      0.87      6537\n","\n","{'eval_loss': 0.3339511752128601, 'eval_precision': 0.887690044139284, 'eval_recall': 0.8970758301668594, 'eval_f1': 0.8923582580115037, 'eval_accuracy': 0.8811381367599816, 'eval_runtime': 2.8251, 'eval_samples_per_second': 44.6, 'eval_steps_per_second': 44.6}\n"]}],"source":["metrics = BERTtrainer.evaluate()\n","print(metrics)"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment 2 (Loss Functions)\n","cross entropy vs MSE\n","This experiment is on BERT as HMM doesnt use loss functions due to its statistical nature rather than being a neural network"]},{"cell_type":"markdown","metadata":{},"source":["## Creating Custom Trainers"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:46:17.503955Z","iopub.status.busy":"2024-04-02T10:46:17.503003Z","iopub.status.idle":"2024-04-02T10:46:17.523520Z","shell.execute_reply":"2024-04-02T10:46:17.522597Z","shell.execute_reply.started":"2024-04-02T10:46:17.503920Z"},"trusted":true},"outputs":[],"source":["from transformers import Trainer\n","import torch.nn as nn\n","\n","# cross entropy\n","class CustomBERTTrainerCrossEntropy(Trainer):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.loss_fn = nn.CrossEntropyLoss()\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            labels = inputs.pop(\"labels\")\n","        else:\n","            labels = None\n","        outputs = model(**inputs)\n","        if self.args.past_index >= 0:\n","            self._past = outputs[self.args.past_index]\n","\n","        if labels is not None:\n","            loss = self.label_smoother(outputs, labels)\n","        else:\n","            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n","            loss = loss*loss\n","            loss = loss.mean()\n","\n","        return (loss, outputs) if return_outputs else loss\n","    \n","    \n","# MLSML\n","class CustomBERTTrainerMLSML(Trainer):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.loss_fn = nn.MultiLabelSoftMarginLoss()\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            labels = inputs.pop(\"labels\")\n","        else:\n","            labels = None\n","        outputs = model(**inputs)\n","        if self.args.past_index >= 0:\n","            self._past = outputs[self.args.past_index]\n","\n","        if labels is not None:\n","            loss = self.label_smoother(outputs, labels)\n","        else:\n","            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n","            loss = loss*loss\n","            loss = loss.mean()\n","\n","        return (loss, outputs) if return_outputs else loss\n","    \n","    \n","# KLDivLoss\n","class CustomBERTTrainerKLDiv(Trainer):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.loss_fn = nn.KLDivLoss()\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            labels = inputs.pop(\"labels\")\n","        else:\n","            labels = None\n","        outputs = model(**inputs)\n","        if self.args.past_index >= 0:\n","            self._past = outputs[self.args.past_index]\n","\n","        if labels is not None:\n","            loss = self.label_smoother(outputs, labels)\n","        else:\n","            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n","            loss = loss*loss\n","            loss = loss.mean()\n","\n","        return (loss, outputs) if return_outputs else loss\n","    \n","\n","# MSE\n","class CustomBERTTrainerMSE(Trainer):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.loss_fn = nn.MSELoss()\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            labels = inputs.pop(\"labels\")\n","        else:\n","            labels = None\n","        outputs = model(**inputs)\n","        if self.args.past_index >= 0:\n","            self._past = outputs[self.args.past_index]\n","\n","        if labels is not None:\n","            loss = self.label_smoother(outputs, labels)\n","        else:\n","            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n","            loss = loss*loss\n","            loss = loss.mean()\n","\n","        return (loss, outputs) if return_outputs else loss\n","  "]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:46:21.012488Z","iopub.status.busy":"2024-04-02T10:46:21.011826Z","iopub.status.idle":"2024-04-02T10:46:22.979708Z","shell.execute_reply":"2024-04-02T10:46:22.978874Z","shell.execute_reply.started":"2024-04-02T10:46:21.012456Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["args_CE = TrainingArguments(\n","    f\"{model_name}-finetuned-CE-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 10,\n","    logging_steps = 10,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=5,\n","    weight_decay=0.001,\n","    save_steps=1000,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=['tensorbaord'],\n",")\n","\n","args_MLSML = TrainingArguments(\n","    f\"{model_name}-finetuned-MLSML-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 10,\n","    logging_steps = 10,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=5,\n","    weight_decay=0.001,\n","    save_steps=1000,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=['tensorbaord'],\n",")\n","\n","args_KLDiv = TrainingArguments(\n","    f\"{model_name}-finetuned-KLDiv-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 10,\n","    logging_steps = 10,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=5,\n","    weight_decay=0.001,\n","    save_steps=1000,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=['tensorbaord'],\n",")\n","\n","args_MSE = TrainingArguments(\n","    f\"{model_name}-finetuned-MSE-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 10,\n","    logging_steps = 10,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=5,\n","    weight_decay=0.001,\n","    save_steps=1000,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=['tensorbaord'],\n",")\n","model_CE = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","model_MLSML = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","model_KLDiv = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","model_MSE = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","\n","BERTtrainer_CE = CustomBERTTrainerCrossEntropy(\n","    model_CE,\n","    args_CE,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_MLSML = CustomBERTTrainerMLSML(\n","    model_MLSML,\n","    args_MLSML,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_KLDiv = CustomBERTTrainerKLDiv(\n","    model_KLDiv,\n","    args_KLDiv,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_MSE = CustomBERTTrainerMSE(\n","    model_MSE,\n","    args_MSE,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:46:30.683075Z","iopub.status.busy":"2024-04-02T10:46:30.682719Z","iopub.status.idle":"2024-04-02T10:46:31.221496Z","shell.execute_reply":"2024-04-02T10:46:31.220530Z","shell.execute_reply.started":"2024-04-02T10:46:30.683050Z"},"trusted":true},"outputs":[{"data":{"text/plain":["37"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:46:37.062030Z","iopub.status.busy":"2024-04-02T10:46:37.061195Z","iopub.status.idle":"2024-04-02T10:52:15.276012Z","shell.execute_reply":"2024-04-02T10:52:15.274980Z","shell.execute_reply.started":"2024-04-02T10:46:37.061995Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='335' max='335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [335/335 01:22, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>No log</td>\n","      <td>0.155046</td>\n","      <td>0.904087</td>\n","      <td>0.906327</td>\n","      <td>0.905206</td>\n","      <td>0.898577</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.231300</td>\n","      <td>0.110292</td>\n","      <td>0.919907</td>\n","      <td>0.910788</td>\n","      <td>0.915325</td>\n","      <td>0.911274</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.231300</td>\n","      <td>0.105445</td>\n","      <td>0.925108</td>\n","      <td>0.920370</td>\n","      <td>0.922733</td>\n","      <td>0.918923</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.060900</td>\n","      <td>0.105013</td>\n","      <td>0.927440</td>\n","      <td>0.918553</td>\n","      <td>0.922975</td>\n","      <td>0.918311</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.060900</td>\n","      <td>0.107633</td>\n","      <td>0.927849</td>\n","      <td>0.924170</td>\n","      <td>0.926006</td>\n","      <td>0.921371</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.034200</td>\n","      <td>0.105874</td>\n","      <td>0.929143</td>\n","      <td>0.920700</td>\n","      <td>0.924902</td>\n","      <td>0.919535</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.95      0.95      0.95      5197\n","        B-AC       0.73      0.84      0.78       563\n","        B-LF       0.80      0.36      0.50       290\n","        I-LF       0.68      0.77      0.72       487\n","\n","    accuracy                           0.90      6537\n","   macro avg       0.79      0.73      0.74      6537\n","weighted avg       0.90      0.90      0.90      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.94      0.95      5197\n","        B-AC       0.78      0.81      0.79       563\n","        B-LF       0.66      0.65      0.65       290\n","        I-LF       0.73      0.84      0.78       487\n","\n","    accuracy                           0.91      6537\n","   macro avg       0.78      0.81      0.80      6537\n","weighted avg       0.91      0.91      0.91      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.95      0.95      5197\n","        B-AC       0.80      0.82      0.81       563\n","        B-LF       0.70      0.73      0.71       290\n","        I-LF       0.76      0.83      0.80       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.80      0.83      0.82      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.94      0.95      5197\n","        B-AC       0.79      0.83      0.81       563\n","        B-LF       0.71      0.73      0.72       290\n","        I-LF       0.74      0.86      0.80       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.80      0.84      0.82      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.94      0.95      5197\n","        B-AC       0.78      0.86      0.82       563\n","        B-LF       0.72      0.76      0.74       290\n","        I-LF       0.78      0.84      0.81       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.81      0.85      0.83      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.94      0.95      5197\n","        B-AC       0.79      0.84      0.81       563\n","        B-LF       0.71      0.77      0.74       290\n","        I-LF       0.76      0.86      0.81       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.80      0.85      0.83      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='335' max='335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [335/335 01:22, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>No log</td>\n","      <td>0.162805</td>\n","      <td>0.897042</td>\n","      <td>0.896745</td>\n","      <td>0.896894</td>\n","      <td>0.884351</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.261100</td>\n","      <td>0.127736</td>\n","      <td>0.916723</td>\n","      <td>0.903849</td>\n","      <td>0.910240</td>\n","      <td>0.906226</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.261100</td>\n","      <td>0.111125</td>\n","      <td>0.924225</td>\n","      <td>0.920866</td>\n","      <td>0.922542</td>\n","      <td>0.918617</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.067400</td>\n","      <td>0.105358</td>\n","      <td>0.927163</td>\n","      <td>0.916901</td>\n","      <td>0.922003</td>\n","      <td>0.917240</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.067400</td>\n","      <td>0.112019</td>\n","      <td>0.926676</td>\n","      <td>0.922848</td>\n","      <td>0.924758</td>\n","      <td>0.919688</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.037100</td>\n","      <td>0.111976</td>\n","      <td>0.928512</td>\n","      <td>0.922683</td>\n","      <td>0.925588</td>\n","      <td>0.920606</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.94      0.94      0.94      5197\n","        B-AC       0.70      0.81      0.75       563\n","        B-LF       0.71      0.20      0.32       290\n","        I-LF       0.60      0.77      0.68       487\n","\n","    accuracy                           0.88      6537\n","   macro avg       0.74      0.68      0.67      6537\n","weighted avg       0.89      0.88      0.88      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.93      0.95      5197\n","        B-AC       0.75      0.81      0.78       563\n","        B-LF       0.68      0.66      0.67       290\n","        I-LF       0.69      0.87      0.77       487\n","\n","    accuracy                           0.91      6537\n","   macro avg       0.77      0.82      0.79      6537\n","weighted avg       0.91      0.91      0.91      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.95      0.95      5197\n","        B-AC       0.79      0.82      0.80       563\n","        B-LF       0.72      0.71      0.71       290\n","        I-LF       0.77      0.83      0.80       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.81      0.83      0.82      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.94      0.95      5197\n","        B-AC       0.80      0.82      0.81       563\n","        B-LF       0.69      0.72      0.71       290\n","        I-LF       0.75      0.88      0.81       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.80      0.84      0.82      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.94      0.95      5197\n","        B-AC       0.79      0.84      0.81       563\n","        B-LF       0.73      0.75      0.74       290\n","        I-LF       0.77      0.85      0.81       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.81      0.84      0.83      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.94      0.95      5197\n","        B-AC       0.78      0.86      0.82       563\n","        B-LF       0.72      0.76      0.74       290\n","        I-LF       0.76      0.86      0.81       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.81      0.85      0.83      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='335' max='335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [335/335 01:22, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>No log</td>\n","      <td>0.166789</td>\n","      <td>0.897635</td>\n","      <td>0.896745</td>\n","      <td>0.897190</td>\n","      <td>0.886798</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.259500</td>\n","      <td>0.118887</td>\n","      <td>0.919460</td>\n","      <td>0.910953</td>\n","      <td>0.915187</td>\n","      <td>0.911121</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.259500</td>\n","      <td>0.103687</td>\n","      <td>0.923852</td>\n","      <td>0.924005</td>\n","      <td>0.923928</td>\n","      <td>0.920606</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.065600</td>\n","      <td>0.102177</td>\n","      <td>0.930271</td>\n","      <td>0.923509</td>\n","      <td>0.926878</td>\n","      <td>0.921371</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.065600</td>\n","      <td>0.101526</td>\n","      <td>0.928820</td>\n","      <td>0.924831</td>\n","      <td>0.926821</td>\n","      <td>0.922136</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.036300</td>\n","      <td>0.104929</td>\n","      <td>0.930090</td>\n","      <td>0.925326</td>\n","      <td>0.927702</td>\n","      <td>0.922900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.93      0.94      5197\n","        B-AC       0.67      0.90      0.77       563\n","        B-LF       0.68      0.24      0.35       290\n","        I-LF       0.62      0.79      0.70       487\n","\n","    accuracy                           0.89      6537\n","   macro avg       0.73      0.71      0.69      6537\n","weighted avg       0.89      0.89      0.88      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.94      0.95      5197\n","        B-AC       0.76      0.85      0.80       563\n","        B-LF       0.68      0.67      0.68       290\n","        I-LF       0.71      0.84      0.77       487\n","\n","    accuracy                           0.91      6537\n","   macro avg       0.78      0.82      0.80      6537\n","weighted avg       0.92      0.91      0.91      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.95      0.96      5197\n","        B-AC       0.78      0.86      0.82       563\n","        B-LF       0.70      0.76      0.73       290\n","        I-LF       0.78      0.81      0.79       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.81      0.84      0.82      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.94      0.95      5197\n","        B-AC       0.77      0.87      0.82       563\n","        B-LF       0.75      0.76      0.75       290\n","        I-LF       0.76      0.85      0.80       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.81      0.85      0.83      6537\n","weighted avg       0.93      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.94      0.96      5197\n","        B-AC       0.78      0.87      0.82       563\n","        B-LF       0.75      0.77      0.76       290\n","        I-LF       0.77      0.84      0.80       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.81      0.85      0.83      6537\n","weighted avg       0.93      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.94      0.96      5197\n","        B-AC       0.77      0.87      0.82       563\n","        B-LF       0.74      0.80      0.77       290\n","        I-LF       0.77      0.84      0.80       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.81      0.86      0.84      6537\n","weighted avg       0.93      0.92      0.92      6537\n","\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='335' max='335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [335/335 01:22, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>No log</td>\n","      <td>0.166362</td>\n","      <td>0.895016</td>\n","      <td>0.892946</td>\n","      <td>0.893979</td>\n","      <td>0.884504</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.266700</td>\n","      <td>0.123942</td>\n","      <td>0.917902</td>\n","      <td>0.910623</td>\n","      <td>0.914248</td>\n","      <td>0.909745</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.266700</td>\n","      <td>0.113495</td>\n","      <td>0.925729</td>\n","      <td>0.922518</td>\n","      <td>0.924121</td>\n","      <td>0.919994</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.067800</td>\n","      <td>0.104700</td>\n","      <td>0.929982</td>\n","      <td>0.925987</td>\n","      <td>0.927980</td>\n","      <td>0.922594</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.067800</td>\n","      <td>0.106556</td>\n","      <td>0.929743</td>\n","      <td>0.926978</td>\n","      <td>0.928359</td>\n","      <td>0.922136</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.038900</td>\n","      <td>0.111790</td>\n","      <td>0.932493</td>\n","      <td>0.928796</td>\n","      <td>0.930641</td>\n","      <td>0.924583</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.95      0.93      0.94      5197\n","        B-AC       0.66      0.88      0.76       563\n","        B-LF       0.68      0.22      0.34       290\n","        I-LF       0.61      0.78      0.68       487\n","\n","    accuracy                           0.88      6537\n","   macro avg       0.73      0.70      0.68      6537\n","weighted avg       0.89      0.88      0.88      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.94      0.95      5197\n","        B-AC       0.77      0.84      0.80       563\n","        B-LF       0.67      0.64      0.66       290\n","        I-LF       0.71      0.84      0.77       487\n","\n","    accuracy                           0.91      6537\n","   macro avg       0.78      0.82      0.79      6537\n","weighted avg       0.91      0.91      0.91      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.95      0.96      5197\n","        B-AC       0.80      0.81      0.80       563\n","        B-LF       0.72      0.72      0.72       290\n","        I-LF       0.77      0.84      0.80       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.81      0.83      0.82      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.95      0.96      5197\n","        B-AC       0.81      0.83      0.82       563\n","        B-LF       0.73      0.72      0.72       290\n","        I-LF       0.78      0.85      0.81       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.82      0.84      0.83      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.79      0.87      0.83       563\n","        B-LF       0.70      0.76      0.73       290\n","        I-LF       0.77      0.83      0.80       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.81      0.85      0.83      6537\n","weighted avg       0.93      0.92      0.92      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.81      0.85      0.83       563\n","        B-LF       0.72      0.76      0.74       290\n","        I-LF       0.77      0.84      0.80       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.82      0.85      0.83      6537\n","weighted avg       0.93      0.92      0.93      6537\n","\n"]},{"data":{"text/plain":["196"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["BERTtrainer_CE.train()\n","BERTtrainer_CE.model.save_pretrained(\"/kaggle/working/BERT_CE_save\")\n","BERTtrainer_CE = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","BERTtrainer_MLSML.train()\n","BERTtrainer_MLSML.model.save_pretrained(\"/kaggle/working/BERT_MLSML_save\")\n","BERTtrainer_MLSML = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","BERTtrainer_KLDiv.train()\n","BERTtrainer_KLDiv.model.save_pretrained(\"/kaggle/working/BERT_KLDiv_save\")\n","BERTtrainer_KLDiv = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","BERTtrainer_MSE.train()\n","BERTtrainer_MSE.model.save_pretrained(\"/kaggle/working/BERT_MSE_save\")\n","BERTtrainer_MSE = None\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T11:07:40.947557Z","iopub.status.busy":"2024-04-02T11:07:40.946549Z","iopub.status.idle":"2024-04-02T11:07:42.243326Z","shell.execute_reply":"2024-04-02T11:07:42.242523Z","shell.execute_reply.started":"2024-04-02T11:07:40.947519Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]}],"source":["model_CE = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_CE_save\", num_labels=len(label_list))\n","model_MLSML = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_MLSML_save\", num_labels=len(label_list))\n","model_KLDiv = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_KLDiv_save\", num_labels=len(label_list))\n","model_MSE = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_MSE_save\", num_labels=len(label_list))\n","\n","BERTtrainer_CE = CustomBERTTrainerCrossEntropy(\n","    model_CE,\n","    args_CE,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_MLSML = CustomBERTTrainerMLSML(\n","    model_MLSML,\n","    args_MLSML,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_KLDiv = CustomBERTTrainerKLDiv(\n","    model_KLDiv,\n","    args_KLDiv,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_MSE = CustomBERTTrainerMSE(\n","    model_MSE,\n","    args_MSE,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T11:07:46.286985Z","iopub.status.busy":"2024-04-02T11:07:46.286056Z","iopub.status.idle":"2024-04-02T11:07:53.378754Z","shell.execute_reply":"2024-04-02T11:07:53.377826Z","shell.execute_reply.started":"2024-04-02T11:07:46.286950Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [126/126 00:01]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.95      0.96      5197\n","        B-AC       0.81      0.83      0.82       563\n","        B-LF       0.72      0.74      0.73       290\n","        I-LF       0.77      0.86      0.81       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.81      0.85      0.83      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","{'eval_loss': 0.10738085955381393, 'eval_precision': 0.9305578684429642, 'eval_recall': 0.9231785891293574, 'eval_f1': 0.9268535412174491, 'eval_accuracy': 0.9215236346948141, 'eval_runtime': 1.7711, 'eval_samples_per_second': 71.143, 'eval_steps_per_second': 71.143}\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [126/126 00:01]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.95      0.95      5197\n","        B-AC       0.79      0.83      0.81       563\n","        B-LF       0.73      0.74      0.74       290\n","        I-LF       0.76      0.86      0.81       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.81      0.84      0.83      6537\n","weighted avg       0.92      0.92      0.92      6537\n","\n","{'eval_loss': 0.11228480190038681, 'eval_precision': 0.9287735064070561, 'eval_recall': 0.9220221377829175, 'eval_f1': 0.9253855082075941, 'eval_accuracy': 0.920299831727092, 'eval_runtime': 1.7679, 'eval_samples_per_second': 71.272, 'eval_steps_per_second': 71.272}\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [126/126 00:01]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.78      0.85      0.81       563\n","        B-LF       0.75      0.78      0.77       290\n","        I-LF       0.77      0.85      0.81       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.82      0.85      0.84      6537\n","weighted avg       0.93      0.92      0.92      6537\n","\n","{'eval_loss': 0.10572151839733124, 'eval_precision': 0.9312178102674863, 'eval_recall': 0.9259871138278539, 'eval_f1': 0.928595096090126, 'eval_accuracy': 0.9235123145173627, 'eval_runtime': 1.7687, 'eval_samples_per_second': 71.237, 'eval_steps_per_second': 71.237}\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [126/126 00:01]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.81      0.86      0.83       563\n","        B-LF       0.73      0.76      0.74       290\n","        I-LF       0.76      0.85      0.80       487\n","\n","    accuracy                           0.92      6537\n","   macro avg       0.82      0.85      0.83      6537\n","weighted avg       0.93      0.92      0.93      6537\n","\n","{'eval_loss': 0.1143692210316658, 'eval_precision': 0.9332558525651669, 'eval_recall': 0.9286304311911449, 'eval_f1': 0.9309373964889035, 'eval_accuracy': 0.9248890928560501, 'eval_runtime': 1.769, 'eval_samples_per_second': 71.227, 'eval_steps_per_second': 71.227}\n"]}],"source":["CE_eval = BERTtrainer_CE.evaluate()\n","print(CE_eval)\n","MLSML_eval = BERTtrainer_MLSML.evaluate()\n","print(MLSML_eval)\n","KLDiv_eval = BERTtrainer_KLDiv.evaluate()\n","print(KLDiv_eval)\n","MSE_eval = BERTtrainer_MSE.evaluate()\n","print(MSE_eval)"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment 3 (Additional Training Samples from Optional Dataset)"]},{"cell_type":"markdown","metadata":{},"source":["lemmatization with Bag of Words VS Word2Vec"]},{"cell_type":"markdown","metadata":{},"source":["## Collecting Dataset"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:29:17.560450Z","iopub.status.busy":"2024-04-02T10:29:17.560051Z","iopub.status.idle":"2024-04-02T10:30:36.069731Z","shell.execute_reply":"2024-04-02T10:30:36.068693Z","shell.execute_reply.started":"2024-04-02T10:29:17.560418Z"},"trusted":true},"outputs":[],"source":["filtered_dataset = load_dataset(\"surrey-nlp/PLOD-filtered\")"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:31:13.633325Z","iopub.status.busy":"2024-04-02T10:31:13.632447Z","iopub.status.idle":"2024-04-02T10:31:13.638806Z","shell.execute_reply":"2024-04-02T10:31:13.637854Z","shell.execute_reply.started":"2024-04-02T10:31:13.633292Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["train size: 112652\n","val size: 24140\n","test size: 24140\n"]}],"source":["filtered_train = filtered_dataset[\"train\"]\n","print(f\"train size: {len(filtered_train)}\")\n","filtered_val = filtered_dataset[\"validation\"]\n","print(f\"val size: {len(filtered_val)}\")\n","filtered_test = filtered_dataset[\"test\"]\n","print(f\"test size: {len(filtered_test)}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Extracting Data to Use\n","I will have three tests, using three sizes of data acquired from the filtered dataset.\n","\n","small: 1072 extra samples to double the dataset size  \n","medium: 10000 extra samples  \n","large: 50000 extra samples"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:31:17.295936Z","iopub.status.busy":"2024-04-02T10:31:17.294800Z","iopub.status.idle":"2024-04-02T10:31:17.311971Z","shell.execute_reply":"2024-04-02T10:31:17.309642Z","shell.execute_reply.started":"2024-04-02T10:31:17.295895Z"},"trusted":true},"outputs":[],"source":["def decode_tags(tag_sequences, possible_tags):\n","    \"\"\"\n","    Decodes a sequence of numerical tags into a list of corresponding textual labels.\n","\n","    Args:\n","        tag_sequence: A list of integers representing numerical tags.\n","        possible_tags: A list of strings representing the possible textual labels.\n","\n","    Returns:\n","        A list of strings representing the decoded textual tags.\n","    \"\"\"\n","\n","    decoded_tags = [[possible_tags[tag] for tag in row] for row in tag_sequences]\n","    return decoded_tags\n","\n","\n","def build_dataset(filtered_set, cw_set, num_of_samples):\n","    \"\"\"\n","    Merges a specified number of rows from a larger list to a smaller list, ensuring no duplicates.\n","\n","    Args:\n","        filtered_set: a split of the filtered dataset\n","        cw_set: a split of the cw dataset\n","        num_of_samples: The number of rows to add from the filtered set.\n","\n","    Returns:\n","        new tokens, pos_tags and ner_tags lists\n","    \"\"\"\n","    # set up the initial lists\n","    tokens = cw_set[\"tokens\"]\n","    pos_tags = cw_set[\"pos_tags\"]\n","    ner_tags = cw_set[\"ner_tags\"]\n","     \n","    # set up the filtered lists\n","    # tokens\n","    filtered_tokens = filtered_set[\"tokens\"]\n","    # pos_tags\n","    filtered_label_list = filtered_set.features[f\"pos_tags\"].feature.names\n","    filtered_pos_tags = decode_tags(filtered_set[\"pos_tags\"], filtered_label_list)\n","    # ner_tags\n","    filtered_label_list = filtered_set.features[f\"ner_tags\"].feature.names\n","    filtered_ner_tags = decode_tags(filtered_set[\"ner_tags\"], filtered_label_list)\n","\n","    # convert the tokens list to sets for efficient duplicate checking\n","    tokens_set = set(tuple(row) for row in tokens)\n","    filtered_tokens_set = set(tuple(row) for row in filtered_tokens)\n","\n","    # find rows to add\n","    rows_to_add = []\n","    for index, row in enumerate(filtered_tokens_set):\n","        if tuple(row) not in tokens_set and len(rows_to_add) < num_of_samples:\n","            rows_to_add.append(index)\n","\n","    # Merge and return the lists\n","    tokens = tokens + [filtered_tokens[i] for i in rows_to_add]\n","    pos_tags = pos_tags + [filtered_pos_tags[i] for i in rows_to_add]\n","    ner_tags = ner_tags + [filtered_ner_tags[i] for i in rows_to_add]\n","\n","    return tokens, pos_tags, ner_tags\n","\n","\n"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:31:20.545781Z","iopub.status.busy":"2024-04-02T10:31:20.544695Z","iopub.status.idle":"2024-04-02T10:32:27.878498Z","shell.execute_reply":"2024-04-02T10:32:27.877481Z","shell.execute_reply.started":"2024-04-02T10:31:20.545746Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["num of small samples: 2144\n","num of medium samples: 11072\n","num of large samples: 51072\n"]}],"source":["small = 1072\n","medium = 10000\n","large = 50000\n","tokens_small, pos_tags_small, ner_tags_small = build_dataset(filtered_train, train, small)\n","tokens_small = pre_process_data(tokens_small, pos_tags_small)\n","tokens_medium, pos_tags_medium, ner_tags_medium = build_dataset(filtered_train, train, medium)\n","tokens_medium = pre_process_data(tokens_medium, pos_tags_medium)\n","tokens_large, pos_tags_large, ner_tags_large = build_dataset(filtered_train, train, large)\n","tokens_large = pre_process_data(tokens_large, pos_tags_large)\n","print(f\"num of small samples: {len(tokens_small)}\\nnum of medium samples: {len(tokens_medium)}\\nnum of large samples: {len(tokens_large)}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"markdown","metadata":{},"source":["### HMM Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# create the character sets\n","char_set_small = get_char_set(tokens_small)\n","char_set_medium = get_char_set(tokens_medium)\n","char_set_large = get_char_set(tokens_large)\n","\n","# create trainers\n","#small\n","trainer_small = nltk.tag.hmm.HiddenMarkovModelTrainer(states=label_list, symbols=char_set_small)\n","data_small = combine_lists_elementwise(tokens_small.copy(), ner_tags_small.copy())\n","# medium\n","trainer_medium = nltk.tag.hmm.HiddenMarkovModelTrainer(states=label_list, symbols=char_set_medium)\n","data_medium = combine_lists_elementwise(tokens_medium.copy(), ner_tags_medium.copy())\n","# large\n","trainer_large = nltk.tag.hmm.HiddenMarkovModelTrainer(states=label_list, symbols=char_set_large)\n","data_large = combine_lists_elementwise(tokens_large.copy(), ner_tags_large.copy())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_small = trainer_small.train_supervised(data_small)\n","model_medium = trainer_medium.train_supervised(data_medium)\n","model_large = trainer_large.train_supervised(data_large)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["save_hmm(model_small, \"hmm_model_small.dill\")\n","save_hmm(model_medium, \"hmm_model_medium.dill\")\n","save_hmm(model_large, \"hmm_model_large.dill\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_small = load_hmm(\"hmm_model_small.dill\")\n","model_medium = load_hmm(\"hmm_model_medium.dill\")\n","model_large = load_hmm(\"hmm_model_large.dill\")"]},{"cell_type":"markdown","metadata":{},"source":["### BERT Training"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:32:39.192692Z","iopub.status.busy":"2024-04-02T10:32:39.191788Z","iopub.status.idle":"2024-04-02T10:32:40.338396Z","shell.execute_reply":"2024-04-02T10:32:40.337366Z","shell.execute_reply.started":"2024-04-02T10:32:39.192655Z"},"trusted":true},"outputs":[],"source":["# create 3 datasets\n","from datasets import DatasetDict, Dataset\n","\n","small_datasets_dict = {\n","    \"train\": Dataset.from_dict({\"tokens\": tokens_small, \"pos_tags\": pos_tags_small, \"ner_tags\": ner_tags_small})\n","}\n","medium_datasets_dict = {\n","    \"train\": Dataset.from_dict({\"tokens\": tokens_medium, \"pos_tags\": pos_tags_medium, \"ner_tags\": ner_tags_medium})\n","}\n","large_datasets_dict = {\n","    \"train\": Dataset.from_dict({\"tokens\": tokens_large, \"pos_tags\": pos_tags_large, \"ner_tags\": ner_tags_large})\n","}\n","\n","small_dataset = DatasetDict(small_datasets_dict)\n","medium_dataset = DatasetDict(medium_datasets_dict)\n","large_dataset = DatasetDict(large_datasets_dict)"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:33:53.738907Z","iopub.status.busy":"2024-04-02T10:33:53.738138Z","iopub.status.idle":"2024-04-02T10:34:24.089199Z","shell.execute_reply":"2024-04-02T10:34:24.088096Z","shell.execute_reply.started":"2024-04-02T10:33:53.738875Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7783a4e5113d4c84aaef2c853885360d","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2144 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b4827fbbd5849889ccd9a16006efb7b","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/11072 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07fb9c4ce923498f8cfc615d5ed7bfa3","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/51072 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["small_tokenized_dataset = small_dataset.map(tokenize_and_align_labels, batched=True)\n","medium_tokenized_dataset = medium_dataset.map(tokenize_and_align_labels, batched=True)\n","large_tokenized_dataset = large_dataset.map(tokenize_and_align_labels, batched=True)"]},{"cell_type":"code","execution_count":52,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["import tensorboard\n","args_small = TrainingArguments(\n","    f\"{model_name}-finetuned-small-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 20,\n","    logging_steps = 20,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","args_medium = TrainingArguments(\n","    f\"{model_name}-finetuned-medium-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 50,\n","    logging_steps = 50,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","args_large = TrainingArguments(\n","    f\"{model_name}-finetuned-large-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 300,\n","    logging_steps = 300,\n","    save_total_limit = 1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","model_small = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","model_medium = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","model_large = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","\n","\n","BERTtrainer_small = Trainer(\n","    model_small,\n","    args_small,\n","    train_dataset=small_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_medium = Trainer(\n","    model_medium,\n","    args_medium,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_large = Trainer(\n","    model_large,\n","    args_large,\n","    train_dataset=large_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# BERTtrainer_small = None\n","# BERTtrainer_medium = None\n","# BERTtrainer_large = None\n","# BERTtrainer = None\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["BERTtrainer_small.train()\n","BERTtrainer_small.model.save_pretrained(\"/kaggle/working/BERT_small_save\")\n","BERTtrainer_small = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","BERTtrainer_medium.train()\n","BERTtrainer_medium.model.save_pretrained(\"/kaggle/working/BERT_medium_save\")\n","BERTtrainer_medium = None\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","BERTtrainer_large.train()\n","BERTtrainer_large.model.save_pretrained(\"/kaggle/working/BERT_large_save\")\n","BERTtrainer_large = None\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## evaluation"]},{"cell_type":"markdown","metadata":{},"source":["### HMM Evaluation"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model loaded successfully!\n","Model loaded successfully!\n","Model loaded successfully!\n"]}],"source":["model_small = load_hmm(\"hmm_model_small.dill\")\n","model_medium = load_hmm(\"hmm_model_medium.dill\")\n","model_large = load_hmm(\"hmm_model_large.dill\")"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tag\\hmm.py:336: RuntimeWarning: overflow encountered in cast\n","  O[i, k] = self._output_logprob(si, self._symbols[k])\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tag\\hmm.py:334: RuntimeWarning: overflow encountered in cast\n","  X[i, j] = self._transitions[si].logprob(self._states[j])\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tag\\hmm.py:332: RuntimeWarning: overflow encountered in cast\n","  P[i] = self._priors.logprob(si)\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\tag\\hmm.py:364: RuntimeWarning: overflow encountered in cast\n","  O[i, k] = self._output_logprob(si, self._symbols[k])\n"]}],"source":["predicted_small = evaluate_hmm(model_small, test_sentences)\n","predicted_medium = evaluate_hmm(model_medium, test_sentences)\n","predicted_large = evaluate_hmm(model_large, test_sentences)"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["5000\n","5000\n","5000\n","5000\n"]}],"source":["predicted_small = [item for sublist in predicted_small for item in sublist]\n","predicted_medium = [item for sublist in predicted_medium for item in sublist]\n","predicted_large = [item for sublist in predicted_large for item in sublist]\n","\n","print(len(correct_tags))\n","print(len(predicted_small))\n","print(len(predicted_medium))\n","print(len(predicted_large))\n"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.87      0.98      0.92      4261\n","        B-AC       0.84      0.18      0.29       263\n","        B-LF       0.51      0.15      0.24       149\n","        I-LF       0.58      0.16      0.25       327\n","\n","    accuracy                           0.86      5000\n","   macro avg       0.70      0.37      0.43      5000\n","weighted avg       0.84      0.86      0.83      5000\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.90      0.97      0.94      4261\n","        B-AC       0.86      0.37      0.51       263\n","        B-LF       0.57      0.32      0.41       149\n","        I-LF       0.65      0.39      0.49       327\n","\n","    accuracy                           0.88      5000\n","   macro avg       0.74      0.51      0.58      5000\n","weighted avg       0.87      0.88      0.87      5000\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.94      0.97      0.96      4261\n","        B-AC       0.86      0.69      0.77       263\n","        B-LF       0.66      0.54      0.59       149\n","        I-LF       0.74      0.65      0.69       327\n","\n","    accuracy                           0.92      5000\n","   macro avg       0.80      0.71      0.75      5000\n","weighted avg       0.92      0.92      0.92      5000\n","\n"]}],"source":["print(classification_report(correct_tags, predicted_small, labels = label_list))\n","print(classification_report(correct_tags, predicted_medium, labels = label_list))\n","print(classification_report(correct_tags, predicted_large, labels = label_list))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predicted = evaluate_hmm(model, test_sentences)\n","predicted = [item for sublist in predicted for item in sublist]\n","print(classification_report(correct_tags, predicted, labels = label_list))"]},{"cell_type":"markdown","metadata":{},"source":["### BERT Evaluation"]},{"cell_type":"code","execution_count":56,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54c7de7d4dad4c68a55548e78165ca79","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.96      0.94      0.95      5197\n","        B-AC       0.74      0.82      0.78       563\n","        B-LF       0.66      0.66      0.66       290\n","        I-LF       0.72      0.83      0.77       487\n","\n","    accuracy                           0.91      6537\n","   macro avg       0.77      0.81      0.79      6537\n","weighted avg       0.91      0.91      0.91      6537\n","\n","{'eval_loss': 0.26455628871917725, 'eval_precision': 0.9140572951365756, 'eval_recall': 0.906657855608789, 'eval_f1': 0.9103425396035498, 'eval_accuracy': 0.9062260975982867, 'eval_runtime': 2.2217, 'eval_samples_per_second': 56.715, 'eval_steps_per_second': 56.715}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de2b8239028f4a7284f1e845286473b0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.82      0.87      0.84       563\n","        B-LF       0.76      0.83      0.79       290\n","        I-LF       0.78      0.89      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.88      0.86      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","{'eval_loss': 0.1811598688364029, 'eval_precision': 0.9424904150691782, 'eval_recall': 0.9340822732529325, 'eval_f1': 0.9382675074676402, 'eval_accuracy': 0.933149762888175, 'eval_runtime': 2.1692, 'eval_samples_per_second': 58.087, 'eval_steps_per_second': 58.087}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc2960251326479f8541bf3391814b05","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.96      0.97      5197\n","        B-AC       0.88      0.90      0.89       563\n","        B-LF       0.78      0.87      0.82       290\n","        I-LF       0.80      0.92      0.86       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.91      0.88      6537\n","weighted avg       0.95      0.94      0.95      6537\n","\n","{'eval_loss': 0.1521802544593811, 'eval_precision': 0.9544924154025671, 'eval_recall': 0.9459772013877417, 'eval_f1': 0.9502157318287422, 'eval_accuracy': 0.9444699403396053, 'eval_runtime': 2.2088, 'eval_samples_per_second': 57.044, 'eval_steps_per_second': 57.044}\n"]}],"source":["model_small = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_small_save\", num_labels=len(label_list))\n","model_medium = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_medium_save\", num_labels=len(label_list))\n","model_large = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_large_save\", num_labels=len(label_list))\n","\n","\n","BERTtrainer_small = Trainer(\n","    model_small,\n","    args_small,\n","    train_dataset=small_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_medium = Trainer(\n","    model_medium,\n","    args_medium,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_large = Trainer(\n","    model_large,\n","    args_large,\n","    train_dataset=large_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# BERTtrainer_small.model.from_pretrained(\"model_saves/BERT_small_save\")\n","# BERTtrainer_medium.model.from_pretrained(\"model_saves/BERT_medium_save\")\n","# BERTtrainer_large.model.from_pretrained(\"model_saves/BERT_large_save\")\n","\n","small_metrics = BERTtrainer_small.evaluate()\n","print(small_metrics)\n","medium_metrics = BERTtrainer_medium.evaluate()\n","print(medium_metrics)\n","large_metrics = BERTtrainer_large.evaluate()\n","print(large_metrics)"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment 4 (Hyperparameters)"]},{"cell_type":"markdown","metadata":{},"source":["## Arguments For Each Test"]},{"cell_type":"code","execution_count":59,"metadata":{"trusted":true},"outputs":[],"source":["args_one = TrainingArguments(\n","    f\"{model_name}-finetuned-one-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.01,\n","    adam_beta1=0.9,\n","    adam_beta2=0.999,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","args_two = TrainingArguments(\n","    f\"{model_name}-finetuned-two-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.001,\n","    adam_beta1=0.9,\n","    adam_beta2=0.999,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","args_three = TrainingArguments(\n","    f\"{model_name}-finetuned-three-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.0001,\n","    adam_beta1=0.9,\n","    adam_beta2=0.999,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","args_four = TrainingArguments(\n","    f\"{model_name}-finetuned-four-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.00001,\n","    adam_beta1=0.9,\n","    adam_beta2=0.999,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","\n","args_five = TrainingArguments(\n","    f\"{model_name}-finetuned-five-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.000001,\n","    adam_beta1=0.9,\n","    adam_beta2=0.999,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","args_six = TrainingArguments(\n","    f\"{model_name}-finetuned-six-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.00001,\n","    adam_beta1=0.9,\n","    adam_beta2=0.999,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=5,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","args_seven = TrainingArguments(\n","    f\"{model_name}-finetuned-seven-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.00001,\n","    adam_beta1=0.5,\n","    adam_beta2=0.5,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=3,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","args_eight = TrainingArguments(\n","    f\"{model_name}-finetuned-eight-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.00001,\n","    adam_beta1=0.2,\n","    adam_beta2=0.2,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=3,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","args_nine = TrainingArguments(\n","    f\"{model_name}-finetuned-nine-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.00001,\n","    adam_beta1=0.1,\n","    adam_beta2=0.9,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=3,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","\n","args_ten = TrainingArguments(\n","    f\"{model_name}-finetuned-ten-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 100,\n","    logging_steps = 100,\n","    save_total_limit = 1,\n","    learning_rate=0.00001,\n","    adam_beta1=0.9,\n","    adam_beta2=0.1,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=5,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import gc\n","BERTtrainer_ex_4 = None\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# BERTtrainer_ex_4 = Trainer(\n","#     model_ex_4,\n","#     args_one,\n","#     train_dataset=medium_tokenized_dataset[\"train\"],\n","#     eval_dataset=tokenized_datasets[\"validation\"],\n","#     data_collator=data_collator,\n","#     tokenizer=tokenizer,\n","#     compute_metrics=compute_metrics,\n","# )\n","\n","# BERTtrainer_ex_4.train()\n","# BERTtrainer_ex_4.model.save_pretrained(\"/kaggle/working/BERT_ex4_one_save\")\n","# BERTtrainer_ex_4 = None\n","# torch.cuda.empty_cache()\n","# gc.collect()\n","\n","# model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","# BERTtrainer_ex_4 = Trainer(\n","#     model_ex_4,\n","#     args_two,\n","#     train_dataset=medium_tokenized_dataset[\"train\"],\n","#     eval_dataset=tokenized_datasets[\"validation\"],\n","#     data_collator=data_collator,\n","#     tokenizer=tokenizer,\n","#     compute_metrics=compute_metrics,\n","# )\n","\n","# BERTtrainer_ex_4.train()\n","# BERTtrainer_ex_4.model.save_pretrained(\"/kaggle/working/BERT_ex4_two_save\")\n","# BERTtrainer_ex_4 = None\n","# torch.cuda.empty_cache()\n","# gc.collect()\n","\n","# model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","# BERTtrainer_ex_4 = Trainer(\n","#     model_ex_4,\n","#     args_four,\n","#     train_dataset=medium_tokenized_dataset[\"train\"],\n","#     eval_dataset=tokenized_datasets[\"validation\"],\n","#     data_collator=data_collator,\n","#     tokenizer=tokenizer,\n","#     compute_metrics=compute_metrics,\n","# )\n","\n","# BERTtrainer_ex_4.train()\n","# BERTtrainer_ex_4.model.save_pretrained(\"/kaggle/working/BERT_ex4_four_save\")\n","# BERTtrainer_ex_4 = None\n","# torch.cuda.empty_cache()\n","# gc.collect()\n","\n","\n","# model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","# BERTtrainer_ex_4 = Trainer(\n","#     model_ex_4,\n","#     args_five,\n","#     train_dataset=medium_tokenized_dataset[\"train\"],\n","#     eval_dataset=tokenized_datasets[\"validation\"],\n","#     data_collator=data_collator,\n","#     tokenizer=tokenizer,\n","#     compute_metrics=compute_metrics,\n","# )\n","\n","# BERTtrainer_ex_4.train()\n","# BERTtrainer_ex_4.model.save_pretrained(\"/kaggle/working/BERT_ex4_five_save\")\n","# BERTtrainer_ex_4 = None\n","# torch.cuda.empty_cache()\n","# gc.collect()\n","\n","# model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","# BERTtrainer_ex_4 = Trainer(\n","#     model_ex_4,\n","#     args_seven,\n","#     train_dataset=medium_tokenized_dataset[\"train\"],\n","#     eval_dataset=tokenized_datasets[\"validation\"],\n","#     data_collator=data_collator,\n","#     tokenizer=tokenizer,\n","#     compute_metrics=compute_metrics,\n","# )\n","\n","# BERTtrainer_ex_4.train()\n","# BERTtrainer_ex_4.model.save_pretrained(\"/kaggle/working/BERT_ex4_seven_save\")\n","# BERTtrainer_ex_4 = None\n","# torch.cuda.empty_cache()\n","# gc.collect()\n","\n","# model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","# BERTtrainer_ex_4 = Trainer(\n","#     model_ex_4,\n","#     args_eight,\n","#     train_dataset=medium_tokenized_dataset[\"train\"],\n","#     eval_dataset=tokenized_datasets[\"validation\"],\n","#     data_collator=data_collator,\n","#     tokenizer=tokenizer,\n","#     compute_metrics=compute_metrics,\n","# )\n","\n","# BERTtrainer_ex_4.train()\n","# BERTtrainer_ex_4.model.save_pretrained(\"/kaggle/working/BERT_ex4_eight_save\")\n","# BERTtrainer_ex_4 = None\n","# torch.cuda.empty_cache()\n","# gc.collect()\n","\n","# model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","# BERTtrainer_ex_4 = Trainer(\n","#     model_ex_4,\n","#     args_nine,\n","#     train_dataset=medium_tokenized_dataset[\"train\"],\n","#     eval_dataset=tokenized_datasets[\"validation\"],\n","#     data_collator=data_collator,\n","#     tokenizer=tokenizer,\n","#     compute_metrics=compute_metrics,\n","# )\n","\n","# BERTtrainer_ex_4.train()\n","# BERTtrainer_ex_4.model.save_pretrained(\"/kaggle/working/BERT_ex4_nine_save\")\n","# BERTtrainer_ex_4 = None\n","# torch.cuda.empty_cache()\n","# gc.collect()\n","\n","model_ex_4 = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","BERTtrainer_ex_4 = Trainer(\n","    model_ex_4,\n","    args_ten,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4.train()\n","BERTtrainer_ex_4.model.save_pretrained(\"/kaggle/working/BERT_ex4_ten_save\")\n","BERTtrainer_ex_4 = None\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"code","execution_count":60,"metadata":{"trusted":true},"outputs":[],"source":["model_ex_4_one = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_one_save\", num_labels=len(label_list))\n","model_ex_4_two = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_two_save\", num_labels=len(label_list))\n","model_ex_4_three = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_three_save\", num_labels=len(label_list))\n","model_ex_4_four = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_four_save\", num_labels=len(label_list))\n","model_ex_4_five = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_five_save\", num_labels=len(label_list))\n","model_ex_4_six = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_six_save\", num_labels=len(label_list))\n","model_ex_4_seven = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_seven_save\", num_labels=len(label_list))\n","model_ex_4_eight = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_eight_save\", num_labels=len(label_list))\n","model_ex_4_nine = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_nine_save\", num_labels=len(label_list))\n","model_ex_4_ten = AutoModelForTokenClassification.from_pretrained(\"model_saves/BERT_ex4_ten_save\", num_labels=len(label_list))\n","\n","BERTtrainer_ex_4_one = Trainer(\n","    model_ex_4_one,\n","    args_one,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_two = Trainer(\n","    model_ex_4_two,\n","    args_two,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_three = Trainer(\n","    model_ex_4_three,\n","    args_three,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_four = Trainer(\n","    model_ex_4_four,\n","    args_four,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_five = Trainer(\n","    model_ex_4_five,\n","    args_five,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_six = Trainer(\n","    model_ex_4_six,\n","    args_six,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_seven = Trainer(\n","    model_ex_4_seven,\n","    args_seven,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_eight = Trainer(\n","    model_ex_4_eight,\n","    args_eight,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_nine = Trainer(\n","    model_ex_4_nine,\n","    args_nine,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","BERTtrainer_ex_4_ten = Trainer(\n","    model_ex_4_ten,\n","    args_ten,\n","    train_dataset=medium_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n"]},{"cell_type":"code","execution_count":61,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d2f78a90816e4161a0ffc2c427298601","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.80      1.00      0.89      5197\n","        B-AC       0.00      0.00      0.00       563\n","        B-LF       0.00      0.00      0.00       290\n","        I-LF       0.00      0.00      0.00       487\n","\n","    accuracy                           0.80      6537\n","   macro avg       0.20      0.25      0.22      6537\n","weighted avg       0.63      0.80      0.70      6537\n","\n","{'eval_loss': 0.7786154747009277, 'eval_precision': 0.7950130029065321, 'eval_recall': 0.8585825210639353, 'eval_f1': 0.8255758538522637, 'eval_accuracy': 0.7950130029065321, 'eval_runtime': 2.815, 'eval_samples_per_second': 44.76, 'eval_steps_per_second': 44.76}\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}],"source":["metrics = BERTtrainer_ex_4_one.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":62,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94f42bf897ad46f99282cbc960095017","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.80      1.00      0.89      5197\n","        B-AC       0.00      0.00      0.00       563\n","        B-LF       0.00      0.00      0.00       290\n","        I-LF       0.00      0.00      0.00       487\n","\n","    accuracy                           0.80      6537\n","   macro avg       0.20      0.25      0.22      6537\n","weighted avg       0.63      0.80      0.70      6537\n","\n","{'eval_loss': 0.7777854204177856, 'eval_precision': 0.7950130029065321, 'eval_recall': 0.8585825210639353, 'eval_f1': 0.8255758538522637, 'eval_accuracy': 0.7950130029065321, 'eval_runtime': 2.4648, 'eval_samples_per_second': 51.12, 'eval_steps_per_second': 51.12}\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","C:\\Users\\harry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}],"source":["metrics = BERTtrainer_ex_4_two.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":63,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5be89fafc70943de8e45d4b23e600012","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.96      5197\n","        B-AC       0.88      0.87      0.87       563\n","        B-LF       0.76      0.82      0.79       290\n","        I-LF       0.79      0.91      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.85      0.89      0.87      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","{'eval_loss': 0.16653192043304443, 'eval_precision': 0.9499666444296198, 'eval_recall': 0.9410209813315711, 'eval_f1': 0.9454726533322267, 'eval_accuracy': 0.9394217530977512, 'eval_runtime': 4.015, 'eval_samples_per_second': 31.382, 'eval_steps_per_second': 31.382}\n"]}],"source":["metrics = BERTtrainer_ex_4_three.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":64,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c1b8a9e1b2c4e148054187785cd1018","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.95      0.96      5197\n","        B-AC       0.82      0.87      0.84       563\n","        B-LF       0.73      0.80      0.76       290\n","        I-LF       0.78      0.90      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.83      0.88      0.85      6537\n","weighted avg       0.93      0.93      0.93      6537\n","\n","{'eval_loss': 0.18823082745075226, 'eval_precision': 0.9395, 'eval_recall': 0.9312737485544358, 'eval_f1': 0.9353687878536463, 'eval_accuracy': 0.9308551323236959, 'eval_runtime': 8.9061, 'eval_samples_per_second': 14.148, 'eval_steps_per_second': 14.148}\n"]}],"source":["metrics = BERTtrainer_ex_4_four.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = BERTtrainer_ex_4_five.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = BERTtrainer_ex_4_six.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = BERTtrainer_ex_4_seven.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = BERTtrainer_ex_4_eight.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = BERTtrainer_ex_4_nine.evaluate()\n","print(metrics)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metrics = BERTtrainer_ex_4_ten.evaluate()\n","print(metrics)"]},{"cell_type":"markdown","metadata":{},"source":["# Final implementation"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T13:15:59.563968Z","iopub.status.busy":"2024-04-02T13:15:59.563009Z","iopub.status.idle":"2024-04-02T13:15:59.913055Z","shell.execute_reply":"2024-04-02T13:15:59.912056Z","shell.execute_reply.started":"2024-04-02T13:15:59.563931Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# KLDivLoss\n","class CustomBERTTrainerKLDiv(Trainer):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.loss_fn = nn.KLDivLoss()\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            labels = inputs.pop(\"labels\")\n","        else:\n","            labels = None\n","        outputs = model(**inputs)\n","        if self.args.past_index >= 0:\n","            self._past = outputs[self.args.past_index]\n","\n","        if labels is not None:\n","            loss = self.label_smoother(outputs, labels)\n","        else:\n","            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n","            loss = loss*loss\n","            loss = loss.mean()\n","\n","        return (loss, outputs) if return_outputs else loss\n","\n","args_final = TrainingArguments(\n","    f\"{model_name}-finetuned-final-{task}\",\n","    evaluation_strategy ='steps',\n","    eval_steps = 1000,\n","    logging_steps = 1000,\n","    save_total_limit = 1,\n","    learning_rate=0.00001,\n","    adam_beta1=0.9,\n","    adam_beta2=0.999,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=1,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    save_steps=0,\n","    metric_for_best_model = 'f1',\n","    load_best_model_at_end=True,\n","    report_to=[\"tensorboard\"],\n",")\n","\n","model_final = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n","\n","BERTtrainer_final = CustomBERTTrainerKLDiv(\n","    model_final,\n","    args_final,\n","    train_dataset=large_tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T13:15:53.754304Z","iopub.status.busy":"2024-04-02T13:15:53.753918Z","iopub.status.idle":"2024-04-02T13:15:54.552043Z","shell.execute_reply":"2024-04-02T13:15:54.551023Z","shell.execute_reply.started":"2024-04-02T13:15:53.754276Z"},"trusted":true},"outputs":[{"data":{"text/plain":["45"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","BERTtrainer_final = None\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T13:16:03.349570Z","iopub.status.busy":"2024-04-02T13:16:03.349170Z","iopub.status.idle":"2024-04-02T13:30:39.964144Z","shell.execute_reply":"2024-04-02T13:30:39.963193Z","shell.execute_reply.started":"2024-04-02T13:16:03.349535Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6384' max='6384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6384/6384 14:35, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.103000</td>\n","      <td>0.070246</td>\n","      <td>0.939409</td>\n","      <td>0.929787</td>\n","      <td>0.934573</td>\n","      <td>0.930702</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.052800</td>\n","      <td>0.054938</td>\n","      <td>0.944638</td>\n","      <td>0.941517</td>\n","      <td>0.943075</td>\n","      <td>0.938504</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.048600</td>\n","      <td>0.065105</td>\n","      <td>0.940351</td>\n","      <td>0.929787</td>\n","      <td>0.935039</td>\n","      <td>0.931161</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.043000</td>\n","      <td>0.058449</td>\n","      <td>0.952127</td>\n","      <td>0.943003</td>\n","      <td>0.947543</td>\n","      <td>0.942787</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.039000</td>\n","      <td>0.051703</td>\n","      <td>0.950708</td>\n","      <td>0.943169</td>\n","      <td>0.946923</td>\n","      <td>0.942022</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.038700</td>\n","      <td>0.052330</td>\n","      <td>0.951147</td>\n","      <td>0.945647</td>\n","      <td>0.948389</td>\n","      <td>0.942175</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.95      0.96      5197\n","        B-AC       0.83      0.87      0.85       563\n","        B-LF       0.70      0.83      0.76       290\n","        I-LF       0.77      0.90      0.83       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.82      0.89      0.85      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.89      0.83      0.86       563\n","        B-LF       0.78      0.81      0.80       290\n","        I-LF       0.80      0.87      0.83       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.87      0.86      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.94      0.96      5197\n","        B-AC       0.77      0.94      0.85       563\n","        B-LF       0.72      0.82      0.77       290\n","        I-LF       0.78      0.92      0.85       487\n","\n","    accuracy                           0.93      6537\n","   macro avg       0.82      0.91      0.86      6537\n","weighted avg       0.94      0.93      0.93      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.96      0.97      5197\n","        B-AC       0.89      0.87      0.88       563\n","        B-LF       0.77      0.87      0.82       290\n","        I-LF       0.79      0.92      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.90      0.88      6537\n","weighted avg       0.95      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.87      0.88       563\n","        B-LF       0.78      0.86      0.82       290\n","        I-LF       0.80      0.92      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.90      0.88      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n","              precision    recall  f1-score   support\n","\n","         B-O       0.97      0.96      0.97      5197\n","        B-AC       0.88      0.89      0.88       563\n","        B-LF       0.77      0.86      0.82       290\n","        I-LF       0.80      0.89      0.84       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.90      0.88      6537\n","weighted avg       0.94      0.94      0.94      6537\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=6384, training_loss=0.0533039795426199, metrics={'train_runtime': 876.0279, 'train_samples_per_second': 58.3, 'train_steps_per_second': 7.287, 'total_flos': 3593319985147392.0, 'train_loss': 0.0533039795426199, 'epoch': 1.0})"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["BERTtrainer_final.train()\n","#BERTtrainer_final.model.save_pretrained(\"/kaggle/working/BERT_final_save\")"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T13:33:10.637872Z","iopub.status.busy":"2024-04-02T13:33:10.636944Z","iopub.status.idle":"2024-04-02T13:33:11.559216Z","shell.execute_reply":"2024-04-02T13:33:11.558171Z","shell.execute_reply.started":"2024-04-02T13:33:10.637835Z"},"trusted":true},"outputs":[],"source":["BERTtrainer_final.model.save_pretrained(\"/kaggle/working/BERT_final_save\")"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T13:31:47.028607Z","iopub.status.busy":"2024-04-02T13:31:47.027968Z","iopub.status.idle":"2024-04-02T13:31:48.927142Z","shell.execute_reply":"2024-04-02T13:31:48.926057Z","shell.execute_reply.started":"2024-04-02T13:31:47.028574Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [126/126 00:01]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         B-O       0.98      0.96      0.97      5197\n","        B-AC       0.88      0.90      0.89       563\n","        B-LF       0.77      0.87      0.82       290\n","        I-LF       0.80      0.90      0.85       487\n","\n","    accuracy                           0.94      6537\n","   macro avg       0.86      0.91      0.88      6537\n","weighted avg       0.95      0.94      0.94      6537\n","\n"]},{"data":{"text/plain":["{'eval_loss': 0.053110260516405106,\n"," 'eval_precision': 0.9521276595744681,\n"," 'eval_recall': 0.946307616058153,\n"," 'eval_f1': 0.9492087165465242,\n"," 'eval_accuracy': 0.9432461373718831,\n"," 'eval_runtime': 1.886,\n"," 'eval_samples_per_second': 66.808,\n"," 'eval_steps_per_second': 66.808,\n"," 'epoch': 1.0}"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["BERTtrainer_final.evaluate()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
