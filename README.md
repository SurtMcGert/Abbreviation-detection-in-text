# NLP Coursework

This is a repo for the NLP Coursework at the University of Surrey.

## Dataset

This work was done using the PLOD dataset found at https://github.com/surrey-nlp/PLOD-AbbreviationDetection

## Branches

Each member of the group has their own branch to work on and experiment on. Each member's experiments are detailed below.

### Harry
1. HMM vs BERT
2. Loss function comparison
3. Added data samples
4. Hyperparameter tuning

### Chris
1. Stop word removal, stemming, converting to lower case, lemmatization, oversampling and undersampling
2. tf-idf vs Word2Vec vs GloVe
3. Hinge loss vs Squared Hinge loss vs Perceptron loss vs Modified Huber loss
4. Hyperparameter tuning

### Adam
1. BERT vs RoBERTa vs XLNet
2. No pre-processing vs lemmatization vs stemming
3. Pre-trained model vs training from scratch vs LoRA fine-tuning
4. Hyperparameter tuning

### Paula
1. CRF vs BERT
2. Focal Loss vs Weighted Cross Entropy Loss
3. Bag of Words vs Word2Vec
4.  Hyperparameter tuning 

## Usage
1. Run the first cell to install requirements
2. Run the second sell to import dependencies
3. Scroll down to the server cell, and run it to deploy the Streamlit server

